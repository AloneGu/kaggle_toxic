{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jac/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/cnn_glove_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_glove_2_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_gru_glove_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_gru_glove_1_trainable_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_gru_muse_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_muse_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_muse_adj_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_muse_adj_1_feat_de_fr.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/cnn_muse_adj_2_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/gru_glove_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/gru_muse_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_2_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_glove_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_glove_1_trainable_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_adj2_add_de_fr_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_adj3_add_de_fr_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_adj_1_add_de_fr_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_adj_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_adj_2_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/lstm_muse_adj_bn_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(95851, 16) (226998, 16)\n",
      "file path ../features/simple_rnn_muse_1_feat.pkl\n",
      "(95851, 6) (226998, 6)\n",
      "file path ../features/tfidf_feat1.pkl\n",
      "(95851, 30) (226998, 30)\n",
      "file path ../features/tfidf_feat2.pkl\n",
      "(95851, 30) (226998, 30)\n",
      "(95851, 238)\n",
      "(95851, 238)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "#     if 'tfidf' in feat or 'lr' in feat or 'mnb' in feat:\n",
    "#         continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_55 (Dense)             (None, 256)               61184     \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 94,854\n",
      "Trainable params: 94,854\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "input_len = train_x.shape[1]\n",
    "num_classes = 6\n",
    "\n",
    "def get_nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation='relu', input_shape=(input_len,)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "tmp_m = get_nn_model()\n",
    "tmp_m.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import ModelCheckpoint,LearningRateScheduler,ReduceLROnPlateau,EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def lr_s(e):\n",
    "    if e < 5:\n",
    "        return 0.001\n",
    "    elif e < 10:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return 0.00001\n",
    "    \n",
    "\n",
    "def nn_eval(y_true,y_pred):\n",
    "    res = []\n",
    "    for i in range(6):\n",
    "        a = y_true[:,i]\n",
    "        b = y_pred[:,i]\n",
    "        res.append(log_loss(a,b))\n",
    "    return res\n",
    "\n",
    "def simple_ens(model_name,k=3,rnd=233):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((226998,6))\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    \n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "\n",
    "        model = get_nn_model()\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        file_path = 'nn_ens.h5'\n",
    "        chk = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                patience=3,\n",
    "                                                verbose=1,\n",
    "                                                factor=0.5,\n",
    "                                                min_lr=0.000001)\n",
    "        \n",
    "        early = EarlyStopping(monitor='val_loss', patience=10)\n",
    "        \n",
    "        model.fit(curr_x,curr_y,\n",
    "                  batch_size=128,\n",
    "                  epochs=100,\n",
    "                  validation_data=(hold_out_x,hold_out_y),\n",
    "                  callbacks=[chk,learning_rate_reduction,early]\n",
    "                 )\n",
    "        model = load_model(file_path)\n",
    "        \n",
    "        curr_tr_pred = model.predict(curr_x)\n",
    "        curr_val_pred = model.predict(hold_out_x)\n",
    "        curr_test_pred = model.predict(test_x)\n",
    "        test_pred += curr_test_pred\n",
    "        \n",
    "        tr_res = nn_eval(curr_y,curr_tr_pred)\n",
    "        val_res = nn_eval(hold_out_y,curr_val_pred)\n",
    "        train_loss_l = np.mean(tr_res)\n",
    "        val_loss_l = np.mean(val_res)\n",
    "        print(tr_res,train_loss_l)\n",
    "        print(val_res,val_loss_l)\n",
    "        all_train_loss_l += train_loss_l/k\n",
    "        all_val_loss_l += val_loss_l/k\n",
    "\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all train avg',all_train_loss_l,'all val avg',all_val_loss_l)\n",
    "    return test_pred\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76680 samples, validate on 19171 samples\n",
      "Epoch 1/100\n",
      "75264/76680 [============================>.] - ETA: 0s - loss: 0.1611 - acc: 0.9763Epoch 00001: val_loss improved from inf to 0.12885, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 3s 39us/step - loss: 0.1601 - acc: 0.9763 - val_loss: 0.1288 - val_acc: 0.9786\n",
      "Epoch 2/100\n",
      "76416/76680 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9808Epoch 00002: val_loss improved from 0.12885 to 0.06384, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0906 - acc: 0.9808 - val_loss: 0.0638 - val_acc: 0.9825\n",
      "Epoch 3/100\n",
      "75264/76680 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9824Epoch 00003: val_loss improved from 0.06384 to 0.05879, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 26us/step - loss: 0.0606 - acc: 0.9824 - val_loss: 0.0588 - val_acc: 0.9804\n",
      "Epoch 4/100\n",
      "74624/76680 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9833Epoch 00004: val_loss improved from 0.05879 to 0.05358, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 26us/step - loss: 0.0523 - acc: 0.9833 - val_loss: 0.0536 - val_acc: 0.9837\n",
      "Epoch 5/100\n",
      "74752/76680 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9835Epoch 00005: val_loss improved from 0.05358 to 0.04829, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0489 - acc: 0.9835 - val_loss: 0.0483 - val_acc: 0.9842\n",
      "Epoch 6/100\n",
      "75520/76680 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9836Epoch 00006: val_loss improved from 0.04829 to 0.04565, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0464 - acc: 0.9837 - val_loss: 0.0457 - val_acc: 0.9842\n",
      "Epoch 7/100\n",
      "74624/76680 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9838Epoch 00007: val_loss improved from 0.04565 to 0.04223, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0450 - acc: 0.9839 - val_loss: 0.0422 - val_acc: 0.9847\n",
      "Epoch 8/100\n",
      "76032/76680 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9839Epoch 00008: val_loss improved from 0.04223 to 0.04178, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0440 - acc: 0.9839 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "Epoch 9/100\n",
      "75392/76680 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9840Epoch 00009: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0430 - acc: 0.9840 - val_loss: 0.0430 - val_acc: 0.9846\n",
      "Epoch 10/100\n",
      "75392/76680 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9838Epoch 00010: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0433 - acc: 0.9838 - val_loss: 0.0420 - val_acc: 0.9841\n",
      "Epoch 11/100\n",
      "74880/76680 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9840Epoch 00011: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0426 - acc: 0.9840 - val_loss: 0.0436 - val_acc: 0.9836\n",
      "Epoch 12/100\n",
      "76288/76680 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9841Epoch 00012: val_loss improved from 0.04178 to 0.04088, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0418 - acc: 0.9841 - val_loss: 0.0409 - val_acc: 0.9847\n",
      "Epoch 13/100\n",
      "75648/76680 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9840Epoch 00013: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0417 - acc: 0.9840 - val_loss: 0.0417 - val_acc: 0.9846\n",
      "Epoch 14/100\n",
      "76288/76680 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9842Epoch 00014: val_loss improved from 0.04088 to 0.04033, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0418 - acc: 0.9842 - val_loss: 0.0403 - val_acc: 0.9845\n",
      "Epoch 15/100\n",
      "74880/76680 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9841Epoch 00015: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0413 - acc: 0.9841 - val_loss: 0.0412 - val_acc: 0.9842\n",
      "Epoch 16/100\n",
      "76544/76680 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9843Epoch 00016: val_loss improved from 0.04033 to 0.03982, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0408 - acc: 0.9843 - val_loss: 0.0398 - val_acc: 0.9848\n",
      "Epoch 17/100\n",
      "76160/76680 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9843Epoch 00017: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0409 - acc: 0.9843 - val_loss: 0.0410 - val_acc: 0.9846\n",
      "Epoch 18/100\n",
      "75648/76680 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9843Epoch 00018: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0413 - acc: 0.9843 - val_loss: 0.0414 - val_acc: 0.9844\n",
      "Epoch 19/100\n",
      "76032/76680 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9844Epoch 00019: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0405 - acc: 0.9844 - val_loss: 0.0410 - val_acc: 0.9847\n",
      "Epoch 20/100\n",
      "75392/76680 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9845Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.0005000000237487257.\n",
      "76680/76680 [==============================] - 2s 32us/step - loss: 0.0403 - acc: 0.9845 - val_loss: 0.0420 - val_acc: 0.9843\n",
      "Epoch 21/100\n",
      "75520/76680 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9847Epoch 00021: val_loss improved from 0.03982 to 0.03969, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0397 - val_acc: 0.9850\n",
      "Epoch 22/100\n",
      "75264/76680 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9846Epoch 00022: val_loss improved from 0.03969 to 0.03946, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0390 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9850\n",
      "Epoch 23/100\n",
      "75776/76680 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9846Epoch 00023: val_loss improved from 0.03946 to 0.03925, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0393 - val_acc: 0.9847\n",
      "Epoch 24/100\n",
      "75136/76680 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9848Epoch 00024: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0394 - val_acc: 0.9848\n",
      "Epoch 25/100\n",
      "75264/76680 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9848Epoch 00025: val_loss improved from 0.03925 to 0.03918, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0392 - val_acc: 0.9850\n",
      "Epoch 26/100\n",
      "75264/76680 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9848Epoch 00026: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0396 - val_acc: 0.9846\n",
      "Epoch 27/100\n",
      "75008/76680 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 0.0002500000118743628.\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0395 - val_acc: 0.9848\n",
      "Epoch 28/100\n",
      "75008/76680 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849Epoch 00028: val_loss improved from 0.03918 to 0.03843, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 29/100\n",
      "76288/76680 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9850Epoch 00029: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0385 - val_acc: 0.9848\n",
      "Epoch 30/100\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9850Epoch 00030: val_loss improved from 0.03843 to 0.03842, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0384 - val_acc: 0.9850\n",
      "Epoch 31/100\n",
      "75904/76680 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00031: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0385 - val_acc: 0.9847\n",
      "Epoch 32/100\n",
      "76032/76680 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9850Epoch 00032: val_loss did not improve\n",
      "\n",
      "Epoch 00032: reducing learning rate to 0.0001250000059371814.\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0375 - acc: 0.9850 - val_loss: 0.0386 - val_acc: 0.9849\n",
      "Epoch 33/100\n",
      "75520/76680 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9851Epoch 00033: val_loss improved from 0.03842 to 0.03830, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0372 - acc: 0.9851 - val_loss: 0.0383 - val_acc: 0.9849\n",
      "Epoch 34/100\n",
      "75008/76680 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00034: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 27us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0384 - val_acc: 0.9850\n",
      "Epoch 35/100\n",
      "76288/76680 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00035: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0386 - val_acc: 0.9849\n",
      "Epoch 36/100\n",
      "75520/76680 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00036: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0388 - val_acc: 0.9849\n",
      "Epoch 37/100\n",
      "76160/76680 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00037: val_loss did not improve\n",
      "\n",
      "Epoch 00037: reducing learning rate to 6.25000029685907e-05.\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0385 - val_acc: 0.9848\n",
      "Epoch 38/100\n",
      "75136/76680 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00038: val_loss improved from 0.03830 to 0.03819, saving model to nn_ens.h5\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0382 - val_acc: 0.9849\n",
      "Epoch 39/100\n",
      "75392/76680 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9852Epoch 00039: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0383 - val_acc: 0.9848\n",
      "Epoch 40/100\n",
      "75776/76680 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00040: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0382 - val_acc: 0.9849\n",
      "Epoch 41/100\n",
      "76288/76680 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00041: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0383 - val_acc: 0.9848\n",
      "Epoch 42/100\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: reducing learning rate to 3.125000148429535e-05.\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0383 - val_acc: 0.9848\n",
      "Epoch 43/100\n",
      "75008/76680 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9853Epoch 00043: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0365 - acc: 0.9853 - val_loss: 0.0382 - val_acc: 0.9848\n",
      "Epoch 44/100\n",
      "76416/76680 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9853Epoch 00044: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0365 - acc: 0.9853 - val_loss: 0.0382 - val_acc: 0.9847\n",
      "Epoch 45/100\n",
      "75648/76680 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9853Epoch 00045: val_loss did not improve\n",
      "\n",
      "Epoch 00045: reducing learning rate to 1.5625000742147677e-05.\n",
      "76680/76680 [==============================] - 2s 29us/step - loss: 0.0365 - acc: 0.9853 - val_loss: 0.0382 - val_acc: 0.9848\n",
      "Epoch 46/100\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9854Epoch 00046: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0364 - acc: 0.9854 - val_loss: 0.0382 - val_acc: 0.9848\n",
      "Epoch 47/100\n",
      "76416/76680 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9854Epoch 00047: val_loss did not improve\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0364 - acc: 0.9854 - val_loss: 0.0382 - val_acc: 0.9848\n",
      "Epoch 48/100\n",
      "75520/76680 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9854Epoch 00048: val_loss did not improve\n",
      "\n",
      "Epoch 00048: reducing learning rate to 7.812500371073838e-06.\n",
      "76680/76680 [==============================] - 2s 28us/step - loss: 0.0364 - acc: 0.9853 - val_loss: 0.0382 - val_acc: 0.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jac/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n",
      "/home/jac/.local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1694: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = -(transformed_labels * np.log(y_pred)).sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan, 0.019479271746140725, nan, 0.0069934080193730947, 0.054630047234466232, 0.017970996869438958] nan\n",
      "[nan, 0.024324997959981706, nan, 0.0091981501719756928, 0.055430236062574835, 0.0208025882635689] nan\n",
      "========================\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9769Epoch 00001: val_loss improved from inf to 0.08758, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 3s 43us/step - loss: 0.1468 - acc: 0.9770 - val_loss: 0.0876 - val_acc: 0.9809\n",
      "Epoch 2/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0776 - acc: 0.9817Epoch 00002: val_loss improved from 0.08758 to 0.08395, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 26us/step - loss: 0.0774 - acc: 0.9817 - val_loss: 0.0839 - val_acc: 0.9767\n",
      "Epoch 3/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9827Epoch 00003: val_loss improved from 0.08395 to 0.05661, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 26us/step - loss: 0.0605 - acc: 0.9827 - val_loss: 0.0566 - val_acc: 0.9818\n",
      "Epoch 4/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9830Epoch 00004: val_loss improved from 0.05661 to 0.04866, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 25us/step - loss: 0.0549 - acc: 0.9830 - val_loss: 0.0487 - val_acc: 0.9834\n",
      "Epoch 5/100\n",
      "76416/76681 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9836Epoch 00005: val_loss improved from 0.04866 to 0.04696, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 26us/step - loss: 0.0496 - acc: 0.9836 - val_loss: 0.0470 - val_acc: 0.9837\n",
      "Epoch 6/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9837Epoch 00006: val_loss improved from 0.04696 to 0.04468, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0470 - acc: 0.9837 - val_loss: 0.0447 - val_acc: 0.9837\n",
      "Epoch 7/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9839Epoch 00007: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 26us/step - loss: 0.0449 - acc: 0.9839 - val_loss: 0.0464 - val_acc: 0.9836\n",
      "Epoch 8/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9841Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 26us/step - loss: 0.0441 - acc: 0.9841 - val_loss: 0.0447 - val_acc: 0.9836\n",
      "Epoch 9/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9840Epoch 00009: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0433 - acc: 0.9841 - val_loss: 0.0466 - val_acc: 0.9837\n",
      "Epoch 10/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9842Epoch 00010: val_loss improved from 0.04468 to 0.04369, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0421 - acc: 0.9842 - val_loss: 0.0437 - val_acc: 0.9837\n",
      "Epoch 11/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9842Epoch 00011: val_loss improved from 0.04369 to 0.04199, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0425 - acc: 0.9841 - val_loss: 0.0420 - val_acc: 0.9842\n",
      "Epoch 12/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9843Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0417 - acc: 0.9843 - val_loss: 0.0420 - val_acc: 0.9839\n",
      "Epoch 13/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9842Epoch 00013: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0414 - acc: 0.9842 - val_loss: 0.0426 - val_acc: 0.9839\n",
      "Epoch 14/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9844Epoch 00014: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.0454 - val_acc: 0.9838\n",
      "Epoch 15/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9842Epoch 00015: val_loss improved from 0.04199 to 0.04126, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0410 - acc: 0.9842 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 16/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9843Epoch 00016: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 17/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9844Epoch 00017: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0406 - acc: 0.9844 - val_loss: 0.0427 - val_acc: 0.9841\n",
      "Epoch 18/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9843Epoch 00018: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.0451 - val_acc: 0.9836\n",
      "Epoch 19/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9844Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00019: reducing learning rate to 0.0005000000237487257.\n",
      "76681/76681 [==============================] - 2s 32us/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "Epoch 20/100\n",
      "74752/76681 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9847Epoch 00020: val_loss improved from 0.04126 to 0.04016, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0402 - val_acc: 0.9843\n",
      "Epoch 21/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9848Epoch 00021: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 22/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9848Epoch 00022: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0404 - val_acc: 0.9843\n",
      "Epoch 23/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9849Epoch 00023: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0405 - val_acc: 0.9841\n",
      "Epoch 24/100\n",
      "74752/76681 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9850Epoch 00024: val_loss did not improve\n",
      "\n",
      "Epoch 00024: reducing learning rate to 0.0002500000118743628.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0406 - val_acc: 0.9842\n",
      "Epoch 25/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00025: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0378 - acc: 0.9850 - val_loss: 0.0406 - val_acc: 0.9841\n",
      "Epoch 26/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9849Epoch 00026: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 27/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9850Epoch 00027: val_loss did not improve\n",
      "\n",
      "Epoch 00027: reducing learning rate to 0.0001250000059371814.\n",
      "76681/76681 [==============================] - 2s 27us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 28/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00028: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0404 - val_acc: 0.9843\n",
      "Epoch 29/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00029: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0405 - val_acc: 0.9844\n",
      "Epoch 30/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 6.25000029685907e-05.\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0403 - val_acc: 0.9844\n",
      "[nan, 0.021745649571959215, nan, 0.0081255361518935629, 0.056864404160075391, 0.019767038236930479] nan\n",
      "[nan, 0.0214060993517571, 0.0453847615866299, 0.0084128235095112039, 0.058640227751549874, 0.019939158470846416] nan\n",
      "========================\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9762Epoch 00001: val_loss improved from inf to 0.10105, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 3s 43us/step - loss: 0.1482 - acc: 0.9762 - val_loss: 0.1011 - val_acc: 0.9808\n",
      "Epoch 2/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0813 - acc: 0.9815Epoch 00002: val_loss improved from 0.10105 to 0.07715, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0810 - acc: 0.9815 - val_loss: 0.0771 - val_acc: 0.9819\n",
      "Epoch 3/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0655 - acc: 0.9823Epoch 00003: val_loss improved from 0.07715 to 0.06011, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0657 - acc: 0.9822 - val_loss: 0.0601 - val_acc: 0.9832\n",
      "Epoch 4/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9830Epoch 00004: val_loss improved from 0.06011 to 0.04998, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0597 - acc: 0.9830 - val_loss: 0.0500 - val_acc: 0.9835\n",
      "Epoch 5/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9835Epoch 00005: val_loss improved from 0.04998 to 0.04661, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0506 - acc: 0.9835 - val_loss: 0.0466 - val_acc: 0.9840\n",
      "Epoch 6/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9834Epoch 00006: val_loss improved from 0.04661 to 0.04479, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0502 - acc: 0.9834 - val_loss: 0.0448 - val_acc: 0.9838\n",
      "Epoch 7/100\n",
      "74752/76681 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9839Epoch 00007: val_loss improved from 0.04479 to 0.04457, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0456 - acc: 0.9840 - val_loss: 0.0446 - val_acc: 0.9836\n",
      "Epoch 8/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9839Epoch 00008: val_loss improved from 0.04457 to 0.04208, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0447 - acc: 0.9839 - val_loss: 0.0421 - val_acc: 0.9842\n",
      "Epoch 9/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9839Epoch 00009: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0438 - acc: 0.9839 - val_loss: 0.0434 - val_acc: 0.9839\n",
      "Epoch 10/100\n",
      "76416/76681 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9840Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0430 - acc: 0.9840 - val_loss: 0.0450 - val_acc: 0.9839\n",
      "Epoch 11/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9842Epoch 00011: val_loss improved from 0.04208 to 0.04135, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0424 - acc: 0.9842 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "Epoch 12/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9841Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0424 - acc: 0.9841 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 13/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9843Epoch 00013: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0421 - acc: 0.9843 - val_loss: 0.0424 - val_acc: 0.9840\n",
      "Epoch 14/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9844Epoch 00014: val_loss improved from 0.04135 to 0.04065, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0413 - acc: 0.9844 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 15/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9843Epoch 00015: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0412 - acc: 0.9844 - val_loss: 0.0416 - val_acc: 0.9840\n",
      "Epoch 16/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9841Epoch 00016: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0410 - acc: 0.9841 - val_loss: 0.0425 - val_acc: 0.9839\n",
      "Epoch 17/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9844Epoch 00017: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0413 - acc: 0.9844 - val_loss: 0.0408 - val_acc: 0.9842\n",
      "Epoch 18/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9842Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00018: reducing learning rate to 0.0005000000237487257.\n",
      "76681/76681 [==============================] - 3s 33us/step - loss: 0.0408 - acc: 0.9842 - val_loss: 0.0416 - val_acc: 0.9836\n",
      "Epoch 19/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9846Epoch 00019: val_loss improved from 0.04065 to 0.04008, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 20/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9848Epoch 00020: val_loss improved from 0.04008 to 0.03999, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0391 - acc: 0.9848 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 21/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9846Epoch 00021: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 22/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9846Epoch 00022: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0406 - val_acc: 0.9842\n",
      "Epoch 23/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00023: reducing learning rate to 0.0002500000118743628.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 24/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9850Epoch 00024: val_loss improved from 0.03999 to 0.03985, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0382 - acc: 0.9850 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 25/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9849Epoch 00025: val_loss improved from 0.03985 to 0.03956, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0381 - acc: 0.9849 - val_loss: 0.0396 - val_acc: 0.9842\n",
      "Epoch 26/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849Epoch 00026: val_loss improved from 0.03956 to 0.03933, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0381 - acc: 0.9849 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 27/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00027: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0394 - val_acc: 0.9843\n",
      "Epoch 28/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9849Epoch 00028: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0378 - acc: 0.9849 - val_loss: 0.0395 - val_acc: 0.9844\n",
      "Epoch 29/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00029: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 30/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00030: val_loss did not improve\n",
      "\n",
      "Epoch 00030: reducing learning rate to 0.0001250000059371814.\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 31/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852Epoch 00031: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 32/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9852Epoch 00032: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0395 - val_acc: 0.9845\n",
      "Epoch 33/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852Epoch 00033: val_loss did not improve\n",
      "\n",
      "Epoch 00033: reducing learning rate to 6.25000029685907e-05.\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0394 - val_acc: 0.9843\n",
      "Epoch 34/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00034: val_loss improved from 0.03933 to 0.03927, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 35/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00035: val_loss improved from 0.03927 to 0.03923, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0392 - val_acc: 0.9843\n",
      "Epoch 36/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9853Epoch 00036: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0370 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 37/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9852Epoch 00037: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0369 - acc: 0.9852 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 38/100\n",
      "76416/76681 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9852Epoch 00038: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0369 - acc: 0.9852 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 39/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00039: val_loss did not improve\n",
      "\n",
      "Epoch 00039: reducing learning rate to 3.125000148429535e-05.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0394 - val_acc: 0.9846\n",
      "Epoch 40/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00040: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 41/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00041: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 42/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: reducing learning rate to 1.5625000742147677e-05.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 43/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00043: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 44/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00044: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 45/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00045: val_loss improved from 0.03923 to 0.03923, saving model to nn_ens.h5\n",
      "\n",
      "Epoch 00045: reducing learning rate to 7.812500371073838e-06.\n",
      "76681/76681 [==============================] - 2s 31us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 46/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00046: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 47/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00047: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 32us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 48/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00048: val_loss did not improve\n",
      "\n",
      "Epoch 00048: reducing learning rate to 3.906250185536919e-06.\n",
      "76681/76681 [==============================] - 2s 32us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 49/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00049: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 50/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00050: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 51/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00051: val_loss did not improve\n",
      "\n",
      "Epoch 00051: reducing learning rate to 1.9531250927684596e-06.\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 52/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00052: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 53/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00053: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 54/100\n",
      "76416/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00054: val_loss did not improve\n",
      "\n",
      "Epoch 00054: reducing learning rate to 1e-06.\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 55/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9854Epoch 00055: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "[nan, 0.02003607604708077, nan, 0.0072004889836885791, nan, 0.018285315406830514] nan\n",
      "[nan, 0.021667470523754308, 0.042698510074582939, 0.0094360395319851333, 0.058121445754582578, 0.020740224662494425] nan\n",
      "========================\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.1648 - acc: 0.9757Epoch 00001: val_loss improved from inf to 0.12442, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 4s 47us/step - loss: 0.1648 - acc: 0.9757 - val_loss: 0.1244 - val_acc: 0.9766\n",
      "Epoch 2/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0739 - acc: 0.9819Epoch 00002: val_loss improved from 0.12442 to 0.06542, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 3s 35us/step - loss: 0.0737 - acc: 0.9819 - val_loss: 0.0654 - val_acc: 0.9817\n",
      "Epoch 3/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0585 - acc: 0.9825Epoch 00003: val_loss improved from 0.06542 to 0.05952, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 31us/step - loss: 0.0583 - acc: 0.9825 - val_loss: 0.0595 - val_acc: 0.9831\n",
      "Epoch 4/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9832Epoch 00004: val_loss improved from 0.05952 to 0.05202, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 31us/step - loss: 0.0531 - acc: 0.9832 - val_loss: 0.0520 - val_acc: 0.9831\n",
      "Epoch 5/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0502 - acc: 0.9834Epoch 00005: val_loss improved from 0.05202 to 0.04892, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 31us/step - loss: 0.0501 - acc: 0.9835 - val_loss: 0.0489 - val_acc: 0.9830\n",
      "Epoch 6/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9839Epoch 00006: val_loss improved from 0.04892 to 0.04447, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0457 - acc: 0.9839 - val_loss: 0.0445 - val_acc: 0.9838\n",
      "Epoch 7/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9841Epoch 00007: val_loss improved from 0.04447 to 0.04411, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0439 - acc: 0.9840 - val_loss: 0.0441 - val_acc: 0.9836\n",
      "Epoch 8/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9839Epoch 00008: val_loss improved from 0.04411 to 0.04387, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0440 - acc: 0.9839 - val_loss: 0.0439 - val_acc: 0.9840\n",
      "Epoch 9/100\n",
      "74752/76681 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9841Epoch 00009: val_loss improved from 0.04387 to 0.04295, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0427 - acc: 0.9842 - val_loss: 0.0430 - val_acc: 0.9840\n",
      "Epoch 10/100\n",
      "74752/76681 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9842Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 31us/step - loss: 0.0423 - acc: 0.9841 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "Epoch 11/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9842Epoch 00011: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0417 - acc: 0.9842 - val_loss: 0.0473 - val_acc: 0.9837\n",
      "Epoch 12/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9842Epoch 00012: val_loss improved from 0.04295 to 0.04266, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0416 - acc: 0.9842 - val_loss: 0.0427 - val_acc: 0.9841\n",
      "Epoch 13/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9843Epoch 00013: val_loss improved from 0.04266 to 0.04216, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0415 - acc: 0.9843 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "Epoch 14/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9844Epoch 00014: val_loss improved from 0.04216 to 0.04165, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0407 - acc: 0.9844 - val_loss: 0.0417 - val_acc: 0.9839\n",
      "Epoch 15/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9842Epoch 00015: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0411 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9840\n",
      "Epoch 16/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9843Epoch 00016: val_loss improved from 0.04165 to 0.04113, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0408 - acc: 0.9844 - val_loss: 0.0411 - val_acc: 0.9840\n",
      "Epoch 17/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9846Epoch 00017: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0405 - acc: 0.9846 - val_loss: 0.0434 - val_acc: 0.9837\n",
      "Epoch 18/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9845Epoch 00018: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0402 - acc: 0.9845 - val_loss: 0.0420 - val_acc: 0.9842\n",
      "Epoch 19/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9846Epoch 00019: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0404 - acc: 0.9846 - val_loss: 0.0423 - val_acc: 0.9842\n",
      "Epoch 20/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9845Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.0005000000237487257.\n",
      "76681/76681 [==============================] - 3s 33us/step - loss: 0.0400 - acc: 0.9845 - val_loss: 0.0420 - val_acc: 0.9842\n",
      "Epoch 21/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9848Epoch 00021: val_loss improved from 0.04113 to 0.04003, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0390 - acc: 0.9848 - val_loss: 0.0400 - val_acc: 0.9845\n",
      "Epoch 22/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9848Epoch 00022: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0409 - val_acc: 0.9839\n",
      "Epoch 23/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849Epoch 00023: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0409 - val_acc: 0.9843\n",
      "Epoch 24/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9849Epoch 00024: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0385 - acc: 0.9849 - val_loss: 0.0401 - val_acc: 0.9843\n",
      "Epoch 25/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9849Epoch 00025: val_loss did not improve\n",
      "\n",
      "Epoch 00025: reducing learning rate to 0.0002500000118743628.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0412 - val_acc: 0.9842\n",
      "Epoch 26/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9851Epoch 00026: val_loss improved from 0.04003 to 0.03995, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0399 - val_acc: 0.9843\n",
      "Epoch 27/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9851Epoch 00027: val_loss improved from 0.03995 to 0.03993, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 28/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00028: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0401 - val_acc: 0.9844\n",
      "Epoch 29/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9851Epoch 00029: val_loss improved from 0.03993 to 0.03990, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0374 - acc: 0.9851 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 30/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9852Epoch 00030: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 31/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9852Epoch 00031: val_loss did not improve\n",
      "\n",
      "Epoch 00031: reducing learning rate to 0.0001250000059371814.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0374 - acc: 0.9852 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 32/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854Epoch 00032: val_loss improved from 0.03990 to 0.03976, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0370 - acc: 0.9853 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 33/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9853Epoch 00033: val_loss improved from 0.03976 to 0.03975, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0397 - val_acc: 0.9844\n",
      "Epoch 34/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9853Epoch 00034: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 35/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9853Epoch 00035: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 36/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00036: val_loss did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 6.25000029685907e-05.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0399 - val_acc: 0.9843\n",
      "Epoch 37/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00037: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 38/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9854Epoch 00038: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 39/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9855Epoch 00039: val_loss did not improve\n",
      "\n",
      "Epoch 00039: reducing learning rate to 3.125000148429535e-05.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 40/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9854Epoch 00040: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0364 - acc: 0.9854 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 41/100\n",
      "76416/76681 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00041: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 42/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: reducing learning rate to 1.5625000742147677e-05.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 43/100\n",
      "74880/76681 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9854Epoch 00043: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "[nan, 0.019827204444351241, 0.041030720763910504, 0.0072717206121256147, 0.054694462978110883, 0.01869666217925289] nan\n",
      "[nan, 0.022382516082252432, 0.047266478490603281, 0.010109991967631625, 0.056538296968869792, 0.018757229516017552] nan\n",
      "========================\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/100\n",
      "74752/76681 [============================>.] - ETA: 0s - loss: 0.1339 - acc: 0.9773Epoch 00001: val_loss improved from inf to 0.09030, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 3s 45us/step - loss: 0.1326 - acc: 0.9775 - val_loss: 0.0903 - val_acc: 0.9817\n",
      "Epoch 2/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0783 - acc: 0.9813Epoch 00002: val_loss improved from 0.09030 to 0.07776, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0783 - acc: 0.9813 - val_loss: 0.0778 - val_acc: 0.9819\n",
      "Epoch 3/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9823Epoch 00003: val_loss improved from 0.07776 to 0.05325, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0624 - acc: 0.9823 - val_loss: 0.0532 - val_acc: 0.9837\n",
      "Epoch 4/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9833Epoch 00004: val_loss improved from 0.05325 to 0.05020, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0528 - acc: 0.9833 - val_loss: 0.0502 - val_acc: 0.9837\n",
      "Epoch 5/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9831Epoch 00005: val_loss improved from 0.05020 to 0.04980, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0522 - acc: 0.9832 - val_loss: 0.0498 - val_acc: 0.9834\n",
      "Epoch 6/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9839Epoch 00006: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0461 - acc: 0.9839 - val_loss: 0.0511 - val_acc: 0.9832\n",
      "Epoch 7/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9839Epoch 00007: val_loss improved from 0.04980 to 0.04540, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0452 - acc: 0.9839 - val_loss: 0.0454 - val_acc: 0.9838\n",
      "Epoch 8/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9841Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0430 - acc: 0.9841 - val_loss: 0.0474 - val_acc: 0.9834\n",
      "Epoch 9/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9841Epoch 00009: val_loss improved from 0.04540 to 0.04331, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0426 - acc: 0.9841 - val_loss: 0.0433 - val_acc: 0.9838\n",
      "Epoch 10/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9841Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0418 - acc: 0.9841 - val_loss: 0.0441 - val_acc: 0.9832\n",
      "Epoch 11/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9842Epoch 00011: val_loss improved from 0.04331 to 0.04289, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0415 - acc: 0.9841 - val_loss: 0.0429 - val_acc: 0.9837\n",
      "Epoch 12/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9842Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0416 - acc: 0.9842 - val_loss: 0.0443 - val_acc: 0.9839\n",
      "Epoch 13/100\n",
      "76416/76681 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9844Epoch 00013: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0413 - acc: 0.9844 - val_loss: 0.0452 - val_acc: 0.9835\n",
      "Epoch 14/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9843Epoch 00014: val_loss improved from 0.04289 to 0.04283, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0414 - acc: 0.9843 - val_loss: 0.0428 - val_acc: 0.9839\n",
      "Epoch 15/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9844Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00015: reducing learning rate to 0.0005000000237487257.\n",
      "76681/76681 [==============================] - 3s 34us/step - loss: 0.0411 - acc: 0.9844 - val_loss: 0.0485 - val_acc: 0.9824\n",
      "Epoch 16/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9847Epoch 00016: val_loss improved from 0.04283 to 0.04158, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0397 - acc: 0.9847 - val_loss: 0.0416 - val_acc: 0.9840\n",
      "Epoch 17/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9845Epoch 00017: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0398 - acc: 0.9845 - val_loss: 0.0421 - val_acc: 0.9841\n",
      "Epoch 18/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9847Epoch 00018: val_loss did not improve\n",
      "76681/76681 [==============================] - 3s 33us/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0419 - val_acc: 0.9841\n",
      "Epoch 19/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00019: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 32us/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0419 - val_acc: 0.9842\n",
      "Epoch 20/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9847Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00020: reducing learning rate to 0.0002500000118743628.\n",
      "76681/76681 [==============================] - 2s 33us/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 21/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9849Epoch 00021: val_loss improved from 0.04158 to 0.04074, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0385 - acc: 0.9849 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 22/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9849Epoch 00022: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0409 - val_acc: 0.9842\n",
      "Epoch 23/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9850Epoch 00023: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0382 - acc: 0.9850 - val_loss: 0.0412 - val_acc: 0.9843\n",
      "Epoch 24/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00024: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0412 - val_acc: 0.9840\n",
      "Epoch 25/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849Epoch 00025: val_loss improved from 0.04074 to 0.04064, saving model to nn_ens.h5\n",
      "\n",
      "Epoch 00025: reducing learning rate to 0.0001250000059371814.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0406 - val_acc: 0.9845\n",
      "Epoch 26/100\n",
      "75904/76681 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9851Epoch 00026: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0375 - acc: 0.9851 - val_loss: 0.0407 - val_acc: 0.9844\n",
      "Epoch 27/100\n",
      "75008/76681 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9851Epoch 00027: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0375 - acc: 0.9851 - val_loss: 0.0408 - val_acc: 0.9844\n",
      "Epoch 28/100\n",
      "75264/76681 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9851Epoch 00028: val_loss did not improve\n",
      "\n",
      "Epoch 00028: reducing learning rate to 6.25000029685907e-05.\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0374 - acc: 0.9851 - val_loss: 0.0409 - val_acc: 0.9844\n",
      "Epoch 29/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9852Epoch 00029: val_loss improved from 0.04064 to 0.04051, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0405 - val_acc: 0.9843\n",
      "Epoch 30/100\n",
      "76544/76681 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00030: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 31/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9851Epoch 00031: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0371 - acc: 0.9851 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 32/100\n",
      "75520/76681 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00032: val_loss improved from 0.04051 to 0.04048, saving model to nn_ens.h5\n",
      "76681/76681 [==============================] - 2s 30us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0405 - val_acc: 0.9843\n",
      "Epoch 33/100\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00033: val_loss improved from 0.04048 to 0.04044, saving model to nn_ens.h5\n",
      "\n",
      "Epoch 00033: reducing learning rate to 3.125000148429535e-05.\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0404 - val_acc: 0.9844\n",
      "Epoch 34/100\n",
      "76160/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00034: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0405 - val_acc: 0.9843\n",
      "Epoch 35/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9852Epoch 00035: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0369 - acc: 0.9852 - val_loss: 0.0405 - val_acc: 0.9843\n",
      "Epoch 36/100\n",
      "74624/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9852Epoch 00036: val_loss did not improve\n",
      "\n",
      "Epoch 00036: reducing learning rate to 1.5625000742147677e-05.\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0369 - acc: 0.9852 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 37/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9852Epoch 00037: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 29us/step - loss: 0.0368 - acc: 0.9852 - val_loss: 0.0405 - val_acc: 0.9843\n",
      "Epoch 38/100\n",
      "75648/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00038: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 39/100\n",
      "75392/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00039: val_loss did not improve\n",
      "\n",
      "Epoch 00039: reducing learning rate to 7.812500371073838e-06.\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 40/100\n",
      "75136/76681 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9852Epoch 00040: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 41/100\n",
      "75776/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00041: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 42/100\n",
      "76032/76681 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00042: val_loss did not improve\n",
      "\n",
      "Epoch 00042: reducing learning rate to 3.906250185536919e-06.\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 43/100\n",
      "76288/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00043: val_loss did not improve\n",
      "76681/76681 [==============================] - 2s 28us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "[nan, 0.020457225091429259, 0.042111718967600117, 0.0075094486326464736, 0.055645796970382633, 0.018299289755354723] nan\n",
      "[0.085306088917920991, 0.021684543479762213, 0.044624960395232061, 0.0092487275594605543, 0.059861970291040935, 0.021913892213168706] 0.0404400304761\n",
      "========================\n",
      "all train avg nan all val avg nan\n",
      "         id     toxic  severe_toxic   obscene        threat    insult  \\\n",
      "0   6044863  0.000739  1.853709e-07  0.000305  2.299599e-06  0.000264   \n",
      "1   6102620  0.002357  9.711452e-08  0.000184  1.652316e-05  0.000163   \n",
      "2  14563293  0.001260  5.766202e-08  0.000108  1.391012e-05  0.000094   \n",
      "3  21086297  0.000485  2.576329e-08  0.000115  2.628050e-06  0.000081   \n",
      "4  22982444  0.000165  1.129319e-08  0.000048  2.010278e-07  0.000026   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.000090  \n",
      "1       0.000257  \n",
      "2       0.000135  \n",
      "3       0.000016  \n",
      "4       0.000003  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "xgb_res = simple_ens('xgb',k=5)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = xgb_res\n",
    "sample_submission.to_csv(\"../results/nn_ens_new_csv_fold5.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
