{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping\n",
      "sleep done =======================\n",
      "file path ../features/glove_cnn2d_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cudnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_gru_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_lstm_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_glove_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn2d_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cudnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_gru_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_lstm_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn2d_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cudnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_gru_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_lstm_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 16) (153164, 16)\n",
      "file path ../features/pool_gru_fasttext_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj1_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_glove_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tfidf_feat1.pkl\n",
      "(159571, 30) (153164, 30)\n",
      "file path ../features/tfidf_feat2.pkl\n",
      "(159571, 30) (153164, 30)\n",
      "(159571, 262)\n",
      "(159571, 262)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(1800)\n",
    "# print('sleep done =======================')\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat:\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,c_bytree=0.9,s_sample=0.9):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    cache_test_pred = np.zeros((153164,6))\n",
    "    single_best = 100\n",
    "    single_best_pred = None\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    \n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        d_test = xgb.DMatrix(test_x)\n",
    "        \n",
    "        # share params\n",
    "        params = {\n",
    "                'subsample': s_sample,\n",
    "                'eta': lr,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'logloss',\n",
    "                #'eval_metric':'auc',\n",
    "                'objective':'binary:logistic',\n",
    "                #'scale_pos_weight':0.9,\n",
    "                'colsample_bytree':c_bytree\n",
    "            \n",
    "                }\n",
    "        \n",
    "        # train for each class\n",
    "        for i in range(6):\n",
    "            d_train = xgb.DMatrix(curr_x, curr_y[:,i])\n",
    "            d_valid = xgb.DMatrix(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "            model = xgb.train(params, d_train, 1000, watchlist,\n",
    "                              early_stopping_rounds=50,\n",
    "                              verbose_eval=None)\n",
    "            print(i)\n",
    "            try:\n",
    "                curr_train_loss = log_loss(curr_y[:,i],model.predict(d_train))\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],model.predict(d_valid))\n",
    "                print(curr_train_loss,curr_val_loss)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            \n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            cache_test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg 6 class\n",
    "        train_loss_l = train_loss_l/6\n",
    "        val_loss_l = val_loss_l/6\n",
    "        print('this fold avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        \n",
    "        # save best one fold result\n",
    "        if val_loss_l < single_best:\n",
    "            single_best = val_loss_l\n",
    "            single_best_pred = cache_test_pred\n",
    "            print('new single best')\n",
    "        \n",
    "        cache_test_pred = np.zeros((153164,6))\n",
    "        \n",
    "        # avg k fold\n",
    "        all_train_loss_l += train_loss_l/k\n",
    "        all_val_loss_l += val_loss_l/k\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all train avg',all_train_loss_l,'all val avg',all_val_loss_l)\n",
    "    return test_pred, single_best_pred\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.062973794631 0.0763881019974\n",
      "1\n",
      "0.0164602255816 0.0198319759818\n",
      "2\n",
      "0.0333548193393 0.038028671643\n",
      "3\n",
      "0.00348009773798 0.00725259354787\n",
      "4\n",
      "0.0491060259655 0.0529090547533\n",
      "5\n",
      "0.0137876713406 0.0181313460997\n",
      "this fold avg train 0.0298604390993 avg val 0.0354236240038\n",
      "new single best\n",
      "========================\n",
      "0\n",
      "0.0668482266346 0.0774560444889\n",
      "1\n",
      "0.0154588847763 0.019301161928\n",
      "2\n",
      "0.0326719610736 0.0403419955222\n",
      "3\n",
      "0.00401298030387 0.00742316441814\n",
      "4\n",
      "0.0439969112501 0.0534643304489\n",
      "5\n",
      "0.0116686688798 0.0170925358694\n",
      "this fold avg train 0.0291096054864 avg val 0.0358465387793\n",
      "========================\n",
      "0\n",
      "0.0658870975437 0.0779617476273\n",
      "1\n",
      "0.0161184485773 0.0206028744958\n",
      "2\n",
      "0.0336842506716 0.0403185396165\n",
      "3\n",
      "0.00368287148504 0.0069344766827\n",
      "4\n",
      "0.0480677566007 0.0558578583305\n",
      "5\n",
      "0.0140471776233 0.0182216986402\n",
      "this fold avg train 0.0302479337503 avg val 0.0366495325655\n",
      "========================\n",
      "0\n",
      "0.0688607267041 0.0767909899429\n",
      "1\n",
      "0.0164882482404 0.0209338428573\n",
      "2\n",
      "0.0338151915151 0.0387201229943\n",
      "3\n",
      "0.00414992100495 0.00598185018421\n",
      "4\n",
      "0.0477316484978 0.0538552105019\n",
      "5\n",
      "0.0139730474661 0.0181004604811\n",
      "this fold avg train 0.0308364639048 avg val 0.035730412827\n",
      "========================\n",
      "0\n",
      "0.0650055020118 0.0762249997115\n",
      "1\n",
      "0.0160962138959 0.0217047107393\n",
      "2\n",
      "0.0330524440538 0.0406595681946\n",
      "3\n",
      "0.00370826966528 0.00814448409407\n",
      "4\n",
      "0.0478418424985 0.0553869071264\n",
      "5\n",
      "0.0139131254714 0.0188762767002\n",
      "this fold avg train 0.0299362329328 avg val 0.0368328244277\n",
      "========================\n",
      "all train avg 0.0299981350347 all val avg 0.0360965865206\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998877      0.310820  0.977031  0.249392  0.924009   \n",
      "1  0000247867823ef7  0.000181      0.000033  0.000066  0.000029  0.000043   \n",
      "2  00013b17ad220c46  0.000479      0.000042  0.000294  0.000031  0.000199   \n",
      "3  00017563c3f7919a  0.000146      0.000032  0.000099  0.000031  0.000102   \n",
      "4  00017695ad8997eb  0.002099      0.000042  0.000239  0.000041  0.000306   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.515539  \n",
      "1       0.000040  \n",
      "2       0.000121  \n",
      "3       0.000060  \n",
      "4       0.000070  \n",
      "save done\n",
      "CPU times: user 6h 23min 24s, sys: 38.2 s, total: 6h 24min 2s\n",
      "Wall time: 48min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# adj lr, colsample_bytree, sample\n",
    "for lr in [0.05]:\n",
    "    for c1 in [0.8]:\n",
    "        for c2 in [0.7]:\n",
    "            xgb_res,b = simple_ens('xgb',5,233,lr,c1,c2)\n",
    "            sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "            sample_submission[list_classes] = xgb_res\n",
    "            fname = \"../results/xgb_adj_fold5_{}_{}_{}.gz\".format(lr,c1,c2)\n",
    "            sample_submission.to_csv(fname, index=False, compression='gzip')\n",
    "            print(sample_submission.head())\n",
    "            print('save done')\n",
    "\n",
    "# no rm, 0.0699781296574 0.0780951434931, 0.0161680844181 0.0199438522849,\n",
    "# final, all train avg 0.031309680463 all val avg 0.0368863155994\n",
    "# rm muse and pretrain, not good\n",
    "# rm gru_v1, 0.0687157498068 0.0780051849506, 0.0152173938614 0.0200296896045\n",
    "# rm tfidf, 0.072673329496 0.0787288243817, 0.0169421114344 0.0199760695638, not good\n",
    "# only rm no pretrain, not good,\n",
    "# test rm gru_v1, lstm_v1, all train avg 0.0313024384367 all val avg 0.0369036640753\n",
    "# rm cnn2d, 0.0704654561461 0.0780916497355, 0.0170009305396 0.0200476295259\n",
    "# 1st fold, this fold avg train 0.0318658486615 avg val 0.0362271742281\n",
    "\n",
    "# adj params\n",
    "# col sample by tree: 0.9 all train avg 0.031309680463 all val avg 0.0368863155994\n",
    "\n",
    "\n",
    "# adj lr, colsample_bytree, sample\n",
    "#   0.05  0.7  0.7 all train avg 0.031149993004 all val avg 0.0368516540068\n",
    "#   0.05  0.7  0.8 all train avg 0.0311077763624 all val avg 0.036855567286\n",
    "#   0.05  0.7  0.9 all train avg 0.0312078560147 all val avg 0.0368798432732\n",
    "#   0.05  0.8  0.7 all train avg 0.0309175391583 all val avg 0.0368485662838\n",
    "#   0.05  0.8  0.8 all train avg 0.0314143017087 all val avg 0.0368859121266\n",
    "#   0.05  0.8  0.9 all train avg 0.0312955837465 all val avg 0.0368866903553\n",
    "#   0.05  0.9  0.7 all train avg 0.0311044998336 all val avg 0.0368667306586\n",
    "#   0.05  0.9  0.8 all train avg 0.0311835661273 all val avg 0.0368992582647\n",
    "#   0.05  0.9  0.9 all train avg 0.0313585027516 all val avg 0.0368996796111\n",
    "#    0.1  0.7  0.7 all train avg 0.0306404949681 all val avg 0.0370501103027\n",
    "#    0.1  0.7  0.8 all train avg 0.0307017678518 all val avg 0.037048645753\n",
    "#    0.1  0.7  0.9 all train avg 0.030880668966 all val avg 0.0370266615654\n",
    "#    0.1  0.8  0.7 all train avg 0.0307153292658 all val avg 0.0370698994366\n",
    "#    0.1  0.8  0.8 all train avg 0.0308616897847 all val avg 0.0370446456042\n",
    "# large lr is worse\n",
    "\n",
    "# adj lr, colsample_bytree, sample\n",
    "#   0.03  0.8  0.7 all train avg 0.0320478053971 all val avg 0.036823783635\n",
    "#   0.05  0.8  0.7 all train avg 0.0309175391583 all val avg 0.0368485662838\n",
    "\n",
    "# fix lr, mnb bug, rm scale_pos_weight\n",
    "# all train avg 0.0299882438015 all val avg 0.0360842600203\n",
    "# 0\n",
    "# 0.0635105397429 0.0763360044684\n",
    "# 1\n",
    "# 0.0162476813928 0.0198345568551\n",
    "# 2\n",
    "# 0.0353173073883 0.0380803762678\n",
    "# 3\n",
    "# 0.00364481098573 0.00719515731424\n",
    "# 4\n",
    "# 0.0494110049729 0.0531263392106\n",
    "# 5\n",
    "# 0.0140568534717 0.0182191483172\n",
    "# 1st fold avg train 0.0303646996591 avg val 0.0354652637389\n",
    "\n",
    "# improve lr mnb feat (changed word, char tfidf params)\n",
    "# all train avg 0.0306645437303 all val avg 0.0362385982181, worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.064225964926 0.0752389609982\n",
      "1\n",
      "0.0163442076623 0.020291969048\n",
      "2\n",
      "0.032701065606 0.0404785466739\n",
      "3\n",
      "0.0030014567237 0.00700397938608\n",
      "4\n",
      "0.0452456469501 0.0563354562136\n",
      "5\n",
      "0.0145176485387 0.0195239674137\n",
      "this fold avg train 0.0293393317345 avg val 0.0364788132889\n",
      "new single best\n",
      "========================\n",
      "0\n",
      "0.0671655596121 0.0776210156925\n",
      "1\n",
      "0.0157110708057 0.0194758977584\n",
      "2\n",
      "0.033854999747 0.0356640663667\n",
      "3\n",
      "0.00404870635245 0.00754606957788\n",
      "4\n",
      "0.0494867988696 0.0491471647236\n",
      "5\n",
      "0.0133418779027 0.0165853688225\n",
      "this fold avg train 0.0306015022149 avg val 0.0343399304903\n",
      "new single best\n",
      "========================\n",
      "0\n",
      "0.0681625542802 0.0735352122237\n",
      "1\n",
      "0.0169461055581 0.018691569028\n",
      "2\n",
      "0.0349497298881 0.0388836021198\n",
      "3\n",
      "0.00428690174795 0.00789294308112\n",
      "4\n",
      "0.0442312521105 0.0533452298975\n",
      "5\n",
      "0.0138451564061 0.0188944754214\n",
      "this fold avg train 0.0304036166651 avg val 0.0352071719619\n",
      "========================\n",
      "0\n",
      "0.0656717158877 0.0811805567765\n",
      "1\n",
      "0.0161968047693 0.0198438616186\n",
      "2\n",
      "0.028740333525 0.0414993777156\n",
      "3\n",
      "0.00421337493084 0.00677035841879\n",
      "4\n",
      "0.045580546793 0.0537540304114\n",
      "5\n",
      "0.0146232677472 0.0156273054794\n",
      "this fold avg train 0.0291710072755 avg val 0.03644591507\n",
      "========================\n",
      "0\n",
      "0.0700843217017 0.0781996292892\n",
      "1\n",
      "0.0163168235064 0.0211785880654\n",
      "2\n",
      "0.0348676983871 0.0372629608131\n",
      "3\n",
      "0.0037304009814 0.00732295726437\n",
      "4\n",
      "0.0489002508041 0.0542893047992\n",
      "5\n",
      "0.014660646918 0.0187751965509\n",
      "this fold avg train 0.0314266903831 avg val 0.0361714394637\n",
      "========================\n",
      "0\n",
      "0.0688760789534 0.0780485496703\n",
      "1\n",
      "0.0159961349265 0.0203100325298\n",
      "2\n",
      "0.033928691828 0.0432489168535\n",
      "3\n",
      "0.00387146690455 0.00649796067506\n",
      "4\n",
      "0.0495880700823 0.0570448998402\n",
      "5\n",
      "0.0143638693997 0.0170754662234\n",
      "this fold avg train 0.0311040520157 avg val 0.0370376376321\n",
      "========================\n",
      "0\n",
      "0.0664915937269 0.0779514122004\n",
      "1\n",
      "0.0167919870946 0.0220368096832\n",
      "2\n",
      "0.0313792320258 0.0354674510533\n",
      "3\n",
      "0.00348295359063 0.00535666110611\n",
      "4\n",
      "0.0487913608715 0.051270929986\n",
      "5\n",
      "0.0140679126579 0.0188936024909\n",
      "this fold avg train 0.0301675066612 avg val 0.0351628110867\n",
      "========================\n",
      "0\n",
      "0.0662173522007 0.0751620614103\n",
      "1\n",
      "0.0169445146479 0.019733172548\n",
      "2\n",
      "0.0343963583714 0.041498157225\n",
      "3\n",
      "0.00423898547789 0.00651326407305\n",
      "4\n",
      "0.0489678164024 0.0569879435269\n",
      "5\n",
      "0.0141962605671 0.017049018242\n",
      "this fold avg train 0.0308268812779 avg val 0.0361572695042\n",
      "========================\n",
      "0\n",
      "0.0680887356426 0.0780377806638\n",
      "1\n",
      "0.0167114938248 0.021123236763\n",
      "2\n",
      "0.0352783020038 0.0426485061224\n",
      "3\n",
      "0.00413660482429 0.009141028874\n",
      "4\n",
      "0.0473371229251 0.0535125263032\n",
      "5\n",
      "0.0145723904671 0.0190400539226\n",
      "this fold avg train 0.0310207749479 avg val 0.0372505221082\n",
      "========================\n",
      "0\n",
      "0.0665354851879 0.0740913257487\n",
      "1\n",
      "0.0167179438336 0.0222574626267\n",
      "2\n",
      "0.0344088442198 0.0385991652122\n",
      "3\n",
      "0.00369546556851 0.00690466701377\n",
      "4\n",
      "0.0490343235577 0.0574042462774\n",
      "5\n",
      "0.0138531639932 0.0186167426061\n",
      "this fold avg train 0.0307075377268 avg val 0.0363122682475\n",
      "========================\n",
      "all train avg 0.0304768900903 all val avg 0.0360563778853\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998712      0.297594  0.977986  0.231897  0.922360   \n",
      "1  0000247867823ef7  0.000224      0.000030  0.000066  0.000024  0.000039   \n",
      "2  00013b17ad220c46  0.000435      0.000035  0.000268  0.000027  0.000205   \n",
      "3  00017563c3f7919a  0.000178      0.000039  0.000085  0.000029  0.000073   \n",
      "4  00017695ad8997eb  0.002629      0.000044  0.000242  0.000037  0.000310   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.549263  \n",
      "1       0.000042  \n",
      "2       0.000128  \n",
      "3       0.000058  \n",
      "4       0.000085  \n",
      "save done\n",
      "CPU times: user 14h 36min 18s, sys: 1min 24s, total: 14h 37min 42s\n",
      "Wall time: 1h 50min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_res,b = simple_ens('xgb',10,233,0.05,0.8,0.7)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = xgb_res\n",
    "sample_submission.to_csv(\"../results/xgb_adj_fold10.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "# all train avg 0.0321542340346 all val avg 0.0367885979049, PUB 9862, rnd 42, lr 0.03\n",
    "# all train avg 0.0318508428487 all val avg 0.0368012450966, rnd 233, lr 0.05, PUB unknown\n",
    "\n",
    "# fix lr, mnb bug, rm scale_pos_weight\n",
    "# all train avg 0.0304768900903 all val avg 0.0360563778853"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
