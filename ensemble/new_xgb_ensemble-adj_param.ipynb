{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/glove_cnn2d_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cudnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_gru_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_lstm_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_glove_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn2d_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cudnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_gru_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_lstm_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn2d_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cudnn_gru_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_gru_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_lstm_v1_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 16) (153164, 16)\n",
      "file path ../features/pool_gru_fasttext_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj1_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_glove_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tfidf_feat1.pkl\n",
      "(159571, 30) (153164, 30)\n",
      "file path ../features/tfidf_feat2.pkl\n",
      "(159571, 30) (153164, 30)\n",
      "(159571, 262)\n",
      "(159571, 262)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(7200)\n",
    "# print('sleep done =======================')\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat:\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,c_bytree=0.9,s_sample=0.9):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    cache_test_pred = np.zeros((153164,6))\n",
    "    single_best = 100\n",
    "    single_best_pred = None\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    \n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        d_test = xgb.DMatrix(test_x)\n",
    "        \n",
    "        # share params\n",
    "        params = {\n",
    "                'subsample': s_sample,\n",
    "                'eta': lr,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'logloss',\n",
    "                #'eval_metric':'auc',\n",
    "                'objective':'binary:logistic',\n",
    "                'scale_pos_weight':0.9,\n",
    "                'colsample_bytree':c_bytree\n",
    "            \n",
    "                }\n",
    "        \n",
    "        # train for each class\n",
    "        for i in range(6):\n",
    "            d_train = xgb.DMatrix(curr_x, curr_y[:,i])\n",
    "            d_valid = xgb.DMatrix(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "            model = xgb.train(params, d_train, 1000, watchlist,\n",
    "                              early_stopping_rounds=50,\n",
    "                              verbose_eval=None)\n",
    "            print(i)\n",
    "            try:\n",
    "                curr_train_loss = log_loss(curr_y[:,i],model.predict(d_train))\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],model.predict(d_valid))\n",
    "                print(curr_train_loss,curr_val_loss)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            \n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            cache_test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg 6 class\n",
    "        train_loss_l = train_loss_l/6\n",
    "        val_loss_l = val_loss_l/6\n",
    "        print('this fold avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        \n",
    "        # save best one fold result\n",
    "        if val_loss_l < single_best:\n",
    "            single_best = val_loss_l\n",
    "            single_best_pred = cache_test_pred\n",
    "            print('new single best')\n",
    "        \n",
    "        cache_test_pred = np.zeros((153164,6))\n",
    "        \n",
    "        # avg k fold\n",
    "        all_train_loss_l += train_loss_l/k\n",
    "        all_val_loss_l += val_loss_l/k\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all train avg',all_train_loss_l,'all val avg',all_val_loss_l)\n",
    "    return test_pred, single_best_pred\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# adj lr, colsample_bytree, sample\n",
    "for lr in [0.05]:\n",
    "    for c1 in [0.8]:\n",
    "        for c2 in [0.7]:\n",
    "            xgb_res,b = simple_ens('xgb',5,233,lr,c1,c2)\n",
    "            sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "            sample_submission[list_classes] = xgb_res\n",
    "            fname = \"../results/xgb_some_csv_fold5_{}_{}_{}.gz\".format(lr,c1,c2)\n",
    "            sample_submission.to_csv(fname, index=False, compression='gzip')\n",
    "            print(sample_submission.head())\n",
    "            print('save done')\n",
    "\n",
    "# no rm, 0.0699781296574 0.0780951434931, 0.0161680844181 0.0199438522849,\n",
    "# final, all train avg 0.031309680463 all val avg 0.0368863155994\n",
    "# rm muse and pretrain, not good\n",
    "# rm gru_v1, 0.0687157498068 0.0780051849506, 0.0152173938614 0.0200296896045\n",
    "# rm tfidf, 0.072673329496 0.0787288243817, 0.0169421114344 0.0199760695638, not good\n",
    "# only rm no pretrain, not good,\n",
    "# test rm gru_v1, lstm_v1, all train avg 0.0313024384367 all val avg 0.0369036640753\n",
    "# rm cnn2d, 0.0704654561461 0.0780916497355, 0.0170009305396 0.0200476295259\n",
    "# 1st fold, this fold avg train 0.0318658486615 avg val 0.0362271742281\n",
    "\n",
    "# adj params\n",
    "# col sample by tree: 0.9 all train avg 0.031309680463 all val avg 0.0368863155994\n",
    "\n",
    "\n",
    "# adj lr, colsample_bytree, sample\n",
    "#   0.05  0.7  0.7 all train avg 0.031149993004 all val avg 0.0368516540068\n",
    "#   0.05  0.7  0.8 all train avg 0.0311077763624 all val avg 0.036855567286\n",
    "#   0.05  0.7  0.9 all train avg 0.0312078560147 all val avg 0.0368798432732\n",
    "#   0.05  0.8  0.7 all train avg 0.0309175391583 all val avg 0.0368485662838\n",
    "#   0.05  0.8  0.8 all train avg 0.0314143017087 all val avg 0.0368859121266\n",
    "#   0.05  0.8  0.9 all train avg 0.0312955837465 all val avg 0.0368866903553\n",
    "#   0.05  0.9  0.7 all train avg 0.0311044998336 all val avg 0.0368667306586\n",
    "#   0.05  0.9  0.8 all train avg 0.0311835661273 all val avg 0.0368992582647\n",
    "#   0.05  0.9  0.9 all train avg 0.0313585027516 all val avg 0.0368996796111\n",
    "#    0.1  0.7  0.7 all train avg 0.0306404949681 all val avg 0.0370501103027\n",
    "#    0.1  0.7  0.8 all train avg 0.0307017678518 all val avg 0.037048645753\n",
    "#    0.1  0.7  0.9 all train avg 0.030880668966 all val avg 0.0370266615654\n",
    "#    0.1  0.8  0.7 all train avg 0.0307153292658 all val avg 0.0370698994366\n",
    "#    0.1  0.8  0.8 all train avg 0.0308616897847 all val avg 0.0370446456042\n",
    "# large lr is worse\n",
    "\n",
    "# adj lr, colsample_bytree, sample\n",
    "#   0.03  0.8  0.7 all train avg 0.0320478053971 all val avg 0.036823783635\n",
    "#   0.05  0.8  0.7 all train avg 0.0309175391583 all val avg 0.0368485662838\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.0715445621255 0.0777686806239\n",
      "1\n",
      "0.0167661675801 0.0203887910049\n",
      "2\n",
      "0.0355492689091 0.0419493081363\n",
      "3\n",
      "0.00376773833307 0.00710639406701\n",
      "4\n",
      "0.0497243095625 0.058259026495\n",
      "5\n",
      "0.0143989866965 0.0197137042688\n",
      "this fold avg train 0.0319585055345 avg val 0.0375309840993\n",
      "new single best\n",
      "========================\n",
      "0\n",
      "0.0695445140217 0.0780855780057\n",
      "1\n",
      "0.0167715961432 0.0195617982575\n",
      "2\n",
      "0.0356496258971 0.0368136593064\n",
      "3\n",
      "0.00363275365437 0.0075979260006\n",
      "4\n",
      "0.0507245961677 0.0493637442885\n",
      "5\n",
      "0.0146891432019 0.0168799144076\n",
      "this fold avg train 0.0318353715143 avg val 0.0347171033777\n",
      "new single best\n",
      "========================\n",
      "0\n",
      "0.0676621959332 0.0748877792594\n",
      "1\n",
      "0.0172348182651 0.0184432435904\n",
      "2\n",
      "0.0362571997654 0.0405191095317\n",
      "3\n",
      "0.00426062992333 0.00784190302653\n",
      "4\n",
      "0.0512273372962 0.054807708031\n",
      "5\n",
      "0.0129719426644 0.01903442907\n",
      "this fold avg train 0.0316023539746 avg val 0.0359223620848\n",
      "========================\n",
      "0\n",
      "0.0706209282431 0.0832473963746\n",
      "1\n",
      "0.0160319344678 0.0201861570977\n",
      "2\n",
      "0.0358450478093 0.0425667601179\n",
      "3\n",
      "0.00438345928996 0.00658656461427\n",
      "4\n",
      "0.0496700902488 0.0545739360555\n",
      "5\n",
      "0.0146496502642 0.0158497018846\n",
      "this fold avg train 0.0318668517205 avg val 0.0371684193574\n",
      "========================\n",
      "0\n",
      "0.0721883689081 0.0798210977162\n",
      "1\n",
      "0.0165210012916 0.0213396639618\n",
      "2\n",
      "0.036156991378 0.0386316713194\n",
      "3\n",
      "0.0039331924384 0.00739666607516\n",
      "4\n",
      "0.0506015757022 0.0549570546201\n",
      "5\n",
      "0.0149393253893 0.0191219249206\n",
      "this fold avg train 0.0323900758513 avg val 0.0368780131022\n",
      "========================\n",
      "0\n",
      "0.0703231011491 0.0803617553994\n",
      "1\n",
      "0.0175207736347 0.0211118075615\n",
      "2\n",
      "0.0353568174468 0.0441581069498\n",
      "3\n",
      "0.00399618413445 0.00656259162919\n",
      "4\n",
      "0.0507063532858 0.058018693394\n",
      "5\n",
      "0.0147347634014 0.0174394378383\n",
      "this fold avg train 0.0321063321754 avg val 0.037942065462\n",
      "========================\n",
      "0\n",
      "0.0736602413826 0.0800447535363\n",
      "1\n",
      "0.0168016064282 0.0226570393562\n",
      "2\n",
      "0.0362606495441 0.036711871705\n",
      "3\n",
      "0.00363254357087 0.00531963921366\n",
      "4\n",
      "0.0472318162118 0.052026054397\n",
      "5\n",
      "0.0139206700241 0.0193155349378\n",
      "this fold avg train 0.0319179211936 avg val 0.036012482191\n",
      "========================\n",
      "0\n",
      "0.0701848523301 0.0768547575936\n",
      "1\n",
      "0.0156620065452 0.0197704223391\n",
      "2\n",
      "0.0366967980196 0.0424911232755\n",
      "3\n",
      "0.00434612068294 0.00647263747413\n",
      "4\n",
      "0.0457900539237 0.057500046264\n",
      "5\n",
      "0.0134105113129 0.0172675350677\n",
      "this fold avg train 0.0310150571357 avg val 0.0367260870023\n",
      "========================\n",
      "0\n",
      "0.0670495107161 0.0794475205215\n",
      "1\n",
      "0.0173868195999 0.0215153181806\n",
      "2\n",
      "0.036207261039 0.0440401153324\n",
      "3\n",
      "0.00414499162566 0.00929534643595\n",
      "4\n",
      "0.051173683129 0.0545579501704\n",
      "5\n",
      "0.0148479208879 0.0194275415236\n",
      "this fold avg train 0.0318016978329 avg val 0.0380472986941\n",
      "========================\n",
      "0\n",
      "0.0705881678247 0.0764720341633\n",
      "1\n",
      "0.0163565892255 0.022333099976\n",
      "2\n",
      "0.0368416032801 0.0399580057411\n",
      "3\n",
      "0.00392637038464 0.00703686735759\n",
      "4\n",
      "0.049763881987 0.0578567605533\n",
      "5\n",
      "0.0146089566209 0.0187490457782\n",
      "this fold avg train 0.0320142615538 avg val 0.0370676355949\n",
      "========================\n",
      "all train avg 0.0318508428487 all val avg 0.0368012450966\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998284      0.273279  0.975984  0.195797  0.901169   \n",
      "1  0000247867823ef7  0.000146      0.000027  0.000086  0.000024  0.000048   \n",
      "2  00013b17ad220c46  0.000445      0.000039  0.000274  0.000029  0.000293   \n",
      "3  00017563c3f7919a  0.000246      0.000035  0.000111  0.000032  0.000085   \n",
      "4  00017695ad8997eb  0.003828      0.000057  0.000443  0.000042  0.000471   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.527949  \n",
      "1       0.000039  \n",
      "2       0.000142  \n",
      "3       0.000054  \n",
      "4       0.000084  \n",
      "save done\n",
      "CPU times: user 13h 11min 38s, sys: 1min 27s, total: 13h 13min 6s\n",
      "Wall time: 1h 40min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "xgb_res,b = simple_ens('xgb',10,233,0.05,0.8,0.7)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = xgb_res\n",
    "sample_submission.to_csv(\"../results/xgb_some_csv_fold10.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "# all train avg 0.0321542340346 all val avg 0.0367885979049, PUB 9862, rnd 42, lr 0.03\n",
    "# all train avg 0.0318508428487 all val avg 0.0368012450966, rnd 233, lr 0.05, PUB"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
