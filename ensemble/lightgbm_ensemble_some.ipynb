{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../features/fasttext_cnn2d_5_feat.pkl',\n",
      "'../features/fasttext_cnn_gru_5_feat.pkl',\n",
      "'../features/fasttext_cnn_v1_5_feat.pkl',\n",
      "'../features/fasttext_cnn_v2_5_feat.pkl',\n",
      "'../features/fasttext_cudnn_gru_5_feat.pkl',\n",
      "'../features/fasttext_gru_v1_5_feat.pkl',\n",
      "'../features/fasttext_lstm_v1_5_feat.pkl',\n",
      "'../features/glove_cnn2d_5_feat.pkl',\n",
      "'../features/glove_cnn_gru_5_feat.pkl',\n",
      "'../features/glove_cnn_v1_5_feat.pkl',\n",
      "'../features/glove_cnn_v2_5_feat.pkl',\n",
      "'../features/glove_cudnn_gru_5_feat.pkl',\n",
      "'../features/glove_gru_v1_5_feat.pkl',\n",
      "'../features/glove_lstm_v1_5_feat.pkl',\n",
      "'../features/lgb1_feat.pkl',\n",
      "'../features/lr_feat1.pkl',\n",
      "'../features/lr_feat2.pkl',\n",
      "'../features/lstm_attention_fasttext_10_feat.pkl',\n",
      "'../features/lstm_attention_fasttext_4_feat.pkl',\n",
      "'../features/lstm_attention_fasttext_adj2_4_feat.pkl',\n",
      "'../features/lstm_attention_glove_5_feat.pkl',\n",
      "'../features/mnb_feat1.pkl',\n",
      "'../features/mnb_feat2.pkl',\n",
      "'../features/muse_cnn2d_5_feat.pkl',\n",
      "'../features/muse_cnn_gru_5_feat.pkl',\n",
      "'../features/muse_cnn_v1_5_feat.pkl',\n",
      "'../features/muse_cnn_v2_5_feat.pkl',\n",
      "'../features/muse_cudnn_gru_5_feat.pkl',\n",
      "'../features/muse_gru_v1_5_feat.pkl',\n",
      "'../features/muse_lstm_v1_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn2d_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn_gru_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn_v1_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn_v2_5_feat.pkl',\n",
      "'../features/no_pretrained_cudnn_gru_5_feat.pkl',\n",
      "'../features/no_pretrained_gru_v1_5_feat.pkl',\n",
      "'../features/no_pretrained_lstm_v1_5_feat.pkl',\n",
      "'../features/other_feat.pkl',\n",
      "'../features/pool_gru_fasttext_5_feat.pkl',\n",
      "'../features/pool_gru_fasttext_adj1_10_feat.pkl',\n",
      "'../features/pool_gru_fasttext_adj2_10_feat.pkl',\n",
      "'../features/pool_gru_fasttext_adj2_5_feat.pkl',\n",
      "'../features/pool_gru_glove_5_feat.pkl',\n",
      "'../features/ridge_feat1.pkl',\n",
      "'../features/ridge_feat2.pkl',\n",
      "'../features/tfidf_feat1.pkl',\n",
      "'../features/tfidf_feat2.pkl',\n",
      "'../features/tilli_lr_feat.pkl',\n",
      "'../features/wordbatch_feat.pkl',\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(7200)\n",
    "# print('sleep done =======================')\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat:\n",
    "        continue\n",
    "    print(\"'{}',\".format(feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/fasttext_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_adj2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/pool_gru_fasttext_adj1_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 253)\n",
      "(159571, 253)\n",
      "[ 3.20521576e-05  3.98428599e-12  1.90362641e-07  4.36426484e-15\n",
      "  3.14909670e-07  2.49261084e-10  1.03984428e-04  1.01192825e-06\n",
      "  7.71425694e-05  5.70658983e-07  2.76282262e-05  4.82240830e-06\n",
      "  1.07179466e-03  4.30504834e-13  4.95859013e-06  3.81039023e-09\n",
      "  2.17764386e-06  4.04727665e-08  2.31894222e-03  1.04079954e-08\n",
      "  7.93577201e-05  3.33727321e-08  2.54135884e-05  6.86043165e-07\n",
      "  3.92362796e-04  8.51056670e-10  7.99019381e-06  8.22520896e-09\n",
      "  3.10496898e-06  5.43770611e-08  2.64098839e-04  6.69312954e-07\n",
      "  2.83593632e-04  1.90346142e-07  3.36135199e-05  5.04876652e-06\n",
      "  7.83401448e-03  3.27365773e-10  1.42471661e-04  1.99955039e-08\n",
      "  3.13217024e-05  5.06735319e-07  3.46535770e-03  9.90751323e-06\n",
      "  2.47916975e-03  1.44417436e-06  4.54552210e-04  6.33683303e-05\n",
      "  1.04952266e-03  2.65866940e-10  5.29929355e-04  1.17285182e-09\n",
      "  5.67557345e-06  7.35631644e-08  1.29255708e-02  5.42654144e-10\n",
      "  1.30079075e-04  4.38657892e-08  9.30503593e-04  1.10900987e-06\n",
      "  4.20544326e-04  1.73265295e-08  1.17154714e-05  5.90198169e-07\n",
      "  7.86788678e-06  3.19371793e-06  4.97183589e-03  2.19470500e-04\n",
      "  1.91532348e-03  1.12080601e-04  6.59428638e-04  2.94189463e-04\n",
      "  6.48717407e-03  1.43058692e-03  3.50386039e-03  7.49363315e-04\n",
      "  2.99510552e-03  1.17916128e-03  7.35962419e-03  6.12739154e-04\n",
      "  2.56212582e-03  3.21850807e-04  2.84586342e-03  5.57503823e-04\n",
      "  4.52273962e-05  2.84812951e-09  4.51102369e-06  1.93301888e-10\n",
      "  6.55921781e-07  2.39560087e-08  4.12431080e-04  2.52912855e-06\n",
      "  1.49930165e-05  3.27736672e-07  1.48465215e-05  4.94528285e-06\n",
      "  1.47045578e-02  2.72551819e-04  4.38906331e-03  6.81379250e-05\n",
      "  4.40233785e-03  2.41395784e-04  3.02297447e-04  5.99867599e-09\n",
      "  2.28718188e-05  1.14941809e-08  1.50047469e-05  1.12412318e-07\n",
      "  3.99922207e-03  1.32866560e-07  1.80772971e-04  2.05082347e-06\n",
      "  1.43799378e-04  7.99837471e-06  8.23467271e-04  6.46239982e-07\n",
      "  1.50306194e-04  1.36767039e-06  3.72287868e-05  5.96239579e-06\n",
      "  2.18682364e-03  1.86483384e-09  4.22727244e-05  4.17673673e-09\n",
      "  5.22176488e-05  3.63439661e-08  9.44107189e-04  3.49101157e-07\n",
      "  8.80941734e-05  1.40849508e-07  4.77972389e-05  5.13671102e-06\n",
      "  1.22240034e-03  3.26898453e-10  1.57116501e-05  7.14215531e-08\n",
      "  9.55454470e-06  1.14095758e-06  1.20788801e-03  1.45915319e-05\n",
      "  2.57672102e-04  4.77434805e-05  1.85553276e-04  1.01938291e-04\n",
      "  7.75199878e-05  9.16150294e-14  1.35723269e-06  6.78946455e-09\n",
      "  2.34089612e-06  1.25148290e-07  2.77680490e-04  5.02962560e-10\n",
      "  5.57820113e-06  7.88782657e-08  1.62170618e-05  7.07555216e-07\n",
      "  2.08675076e-04  3.51234530e-09  3.66928653e-05  1.65822357e-07\n",
      "  1.23678383e-05  6.41339355e-08  2.32254402e-04  2.02516279e-08\n",
      "  6.71203861e-06  5.43154272e-07  2.23355823e-06  1.34485454e-05\n",
      "  5.75403101e-04  1.23468681e-07  3.39683756e-05  1.35936205e-07\n",
      "  1.52858502e-05  2.41758426e-06  5.00000000e+01  4.60000000e+01\n",
      "  2.64000000e+02  0.00000000e+00  2.00000000e+01  0.00000000e+00\n",
      "  3.00000000e+00  1.20000000e+01  4.24000000e+00  0.00000000e+00\n",
      "  6.43939394e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.51515152e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.60000000e+01\n",
      "  3.40000000e+02  2.50000000e+01  2.50000000e-01  9.20000000e-01\n",
      "  5.00000000e+01  1.00000000e+00  4.00000000e-01  3.00000000e+01\n",
      "  6.00000000e-01  6.00000000e-02  2.40000000e-01  3.18382401e-03\n",
      "  6.99680186e-06  1.21792611e-04  8.39482891e-06  8.26432224e-05\n",
      "  1.92302832e-05  1.28676184e-03  7.59803743e-06  1.64519093e-04\n",
      "  1.50343601e-06  2.52827129e-04  9.02887132e-06  6.34323619e-03\n",
      "  4.35701077e-05  1.06552534e-03  1.79293893e-05  2.69570126e-04\n",
      "  8.49992557e-06 -2.61802127e-02 -3.55490310e-03 -2.24073743e-02\n",
      " -2.24863418e-03 -3.27414011e-02 -9.82143021e-03 -8.71819733e-03\n",
      " -1.09741811e-02 -1.36728622e-02 -4.26841221e-03 -2.65620055e-02\n",
      " -1.09669200e-02  4.09644022e-03  6.09518792e-03  1.30472148e-02\n",
      "  2.28091293e-03  1.25153108e-02  6.42610836e-03  4.78512980e-01\n",
      "  4.94473060e-01  4.93796720e-01  4.86044940e-01  4.89667930e-01\n",
      "  4.97940090e-01]\n"
     ]
    }
   ],
   "source": [
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "\n",
    "feats_files = [\n",
    "'../features/fasttext_cnn2d_5_feat.pkl',\n",
    "'../features/fasttext_cnn_gru_5_feat.pkl',\n",
    "'../features/fasttext_cnn_v1_5_feat.pkl',\n",
    "'../features/fasttext_cnn_v2_5_feat.pkl',\n",
    "'../features/fasttext_cudnn_gru_5_feat.pkl',\n",
    "#'../features/fasttext_gru_v1_5_feat.pkl',\n",
    "'../features/fasttext_lstm_v1_5_feat.pkl',\n",
    "'../features/glove_cnn2d_5_feat.pkl',\n",
    "'../features/glove_cnn_gru_5_feat.pkl',\n",
    "'../features/glove_cnn_v1_5_feat.pkl',\n",
    "'../features/glove_cnn_v2_5_feat.pkl',\n",
    "'../features/glove_cudnn_gru_5_feat.pkl',\n",
    "#'../features/glove_gru_v1_5_feat.pkl',\n",
    "#'../features/glove_lstm_v1_5_feat.pkl',\n",
    "'../features/lgb1_feat.pkl',\n",
    "'../features/lr_feat1.pkl',\n",
    "'../features/lr_feat2.pkl',\n",
    "# '../features/lstm_attention_fasttext_10_feat.pkl',\n",
    "# '../features/lstm_attention_fasttext_4_feat.pkl',\n",
    "'../features/lstm_attention_fasttext_adj2_4_feat.pkl',\n",
    "'../features/lstm_attention_glove_5_feat.pkl',\n",
    "'../features/mnb_feat1.pkl',\n",
    "'../features/mnb_feat2.pkl',\n",
    "'../features/muse_cnn2d_5_feat.pkl',\n",
    "'../features/muse_cnn_gru_5_feat.pkl',\n",
    "#'../features/muse_cnn_v1_5_feat.pkl',\n",
    "'../features/muse_cnn_v2_5_feat.pkl',\n",
    "'../features/muse_cudnn_gru_5_feat.pkl',\n",
    "#'../features/muse_gru_v1_5_feat.pkl',\n",
    "#'../features/muse_lstm_v1_5_feat.pkl',\n",
    "'../features/no_pretrained_cnn2d_5_feat.pkl',\n",
    "'../features/no_pretrained_cnn_gru_5_feat.pkl',\n",
    "'../features/no_pretrained_cnn_v1_5_feat.pkl',\n",
    "'../features/no_pretrained_cnn_v2_5_feat.pkl',\n",
    "'../features/no_pretrained_cudnn_gru_5_feat.pkl',\n",
    "'../features/no_pretrained_gru_v1_5_feat.pkl',\n",
    "'../features/no_pretrained_lstm_v1_5_feat.pkl',\n",
    "'../features/other_feat.pkl',\n",
    "#'../features/pool_gru_fasttext_5_feat.pkl',\n",
    "'../features/pool_gru_fasttext_adj1_10_feat.pkl',\n",
    "'../features/pool_gru_fasttext_adj2_10_feat.pkl',\n",
    "#'../features/pool_gru_fasttext_adj2_5_feat.pkl',\n",
    "'../features/pool_gru_glove_5_feat.pkl',\n",
    "'../features/ridge_feat1.pkl',\n",
    "'../features/ridge_feat2.pkl',\n",
    "# '../features/tfidf_feat1.pkl',\n",
    "# '../features/tfidf_feat2.pkl',\n",
    "'../features/tilli_lr_feat.pkl',\n",
    "'../features/wordbatch_feat.pkl',\n",
    "]\n",
    "\n",
    "for feat in feats_files:\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.20521454e-05 4.89555083e-12 1.90365273e-07 4.64348831e-15\n",
      " 3.15957909e-07 2.59789556e-10 9.16498370e-05 1.13355713e-06\n",
      " 7.24869416e-05 5.81399818e-07 2.43423178e-05 4.81439416e-06\n",
      " 1.07179477e-03 4.88805527e-13 4.95880057e-06 3.81615681e-09\n",
      " 2.18490891e-06 4.19219603e-08 2.31894219e-03 1.29394237e-08\n",
      " 7.93581931e-05 3.53740753e-08 2.54700022e-05 7.03738971e-07\n",
      " 3.88531271e-04 9.67030148e-10 7.92067129e-06 8.69702087e-09\n",
      " 3.12861708e-06 6.02284241e-08 2.64098439e-04 6.85468495e-07\n",
      " 2.83630892e-04 2.00738290e-07 3.37293069e-05 5.24582129e-06\n",
      " 7.83401448e-03 3.65120710e-10 1.42477453e-04 2.12124282e-08\n",
      " 3.13741853e-05 5.19897572e-07 3.44083674e-03 1.21799667e-05\n",
      " 2.48687941e-03 1.46833189e-06 4.65022698e-04 7.12203760e-05\n",
      " 1.04952266e-03 3.07052981e-10 5.30055256e-04 1.19123942e-09\n",
      " 5.69250008e-06 7.48166902e-08 1.29255704e-02 5.85622927e-10\n",
      " 1.30088318e-04 4.54351489e-08 9.31599431e-04 1.13912413e-06\n",
      " 4.09279057e-04 1.84011858e-08 1.15490292e-05 6.51481030e-07\n",
      " 7.78886272e-06 3.72168763e-06 4.83638733e-03 2.17394968e-04\n",
      " 1.79814374e-03 1.11942435e-04 5.97145528e-04 2.77291187e-04\n",
      " 6.41563293e-03 1.38321079e-03 3.37450246e-03 7.06767259e-04\n",
      " 2.88629059e-03 1.12457600e-03 7.31546044e-03 5.68872108e-04\n",
      " 2.40054267e-03 3.05619484e-04 2.77494446e-03 5.34113253e-04\n",
      " 4.52034600e-05 2.85217074e-09 4.50998349e-06 1.93720188e-10\n",
      " 6.56847814e-07 2.41428381e-08 4.12416571e-04 2.54449957e-06\n",
      " 1.49925625e-05 3.30424448e-07 1.48866399e-05 5.00354000e-06\n",
      " 1.47043943e-02 2.72562789e-04 4.38900502e-03 6.98790411e-05\n",
      " 4.40231664e-03 2.41533516e-04 3.02297447e-04 5.99867599e-09\n",
      " 2.28718188e-05 1.14941809e-08 1.50047469e-05 1.12412318e-07\n",
      " 3.99921861e-03 1.64247526e-07 1.80797227e-04 2.21692957e-06\n",
      " 1.44103345e-04 8.71664935e-06 8.19987165e-04 6.96833344e-07\n",
      " 1.50009016e-04 1.48846635e-06 3.73956026e-05 6.05270214e-06\n",
      " 2.18682108e-03 2.19080180e-09 4.22735480e-05 4.34542572e-09\n",
      " 5.23643968e-05 3.88956318e-08 9.37810586e-04 4.14866428e-07\n",
      " 8.78742570e-05 1.50998994e-07 4.79842662e-05 5.56476375e-06\n",
      " 1.22239100e-03 3.91583492e-10 1.57119573e-05 1.14808753e-07\n",
      " 9.57191793e-06 1.23354037e-06 1.15937835e-03 1.81251585e-05\n",
      " 2.46056087e-04 5.28473355e-05 1.89417723e-04 1.26749527e-04\n",
      " 7.75199326e-05 1.22815319e-13 1.35723627e-06 7.37844817e-09\n",
      " 2.34522654e-06 1.29129470e-07 2.77679820e-04 5.69020910e-10\n",
      " 5.57820170e-06 8.45466049e-08 1.62772003e-05 1.02402727e-06\n",
      " 1.87407466e-04 3.60640157e-09 3.49349905e-05 4.13546203e-07\n",
      " 1.20644434e-05 1.17703795e-07 2.30023038e-04 2.09456224e-08\n",
      " 6.67540175e-06 5.62926036e-07 2.21729842e-06 1.42399571e-05\n",
      " 5.73234385e-04 1.24701001e-07 3.39330246e-05 1.75836711e-07\n",
      " 1.53857133e-05 2.53468859e-06 3.49500713e-02 9.33609959e-02\n",
      " 5.16619944e-02 0.00000000e+00 2.09863589e-02 0.00000000e+00\n",
      " 2.21893491e-03 1.20000000e-02 2.61501211e-03 0.00000000e+00\n",
      " 6.45107832e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.19159566e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.20970043e-02\n",
      " 4.39760604e-02 8.88888889e-01 2.64705882e-01 9.19935949e-01\n",
      " 3.49500713e-02 0.00000000e+00 4.00000000e-01 2.40000000e-02\n",
      " 6.00000000e-01 6.00000000e-02 2.40000000e-01 3.17884450e-03\n",
      " 6.68346570e-06 1.17619317e-04 8.37344689e-06 7.94814591e-05\n",
      " 1.92838266e-05 1.27871513e-03 7.52405873e-06 1.60147943e-04\n",
      " 1.50287065e-06 2.51967662e-04 8.99418141e-06 6.34003305e-03\n",
      " 4.33187639e-05 1.06320117e-03 1.79374214e-05 2.69580290e-04\n",
      " 8.47601614e-06 6.67202266e-02 4.87881259e-02 2.28364572e-02\n",
      " 3.87381735e-02 3.56464940e-02 2.62395352e-02 6.39271377e-02\n",
      " 4.78549963e-02 2.95811533e-02 4.17371945e-02 3.53564038e-02\n",
      " 3.71946016e-02 4.06211702e-03 5.70668569e-03 1.26388567e-02\n",
      " 2.22039166e-03 1.22442901e-02 6.32540379e-03 6.12307004e-02\n",
      " 9.00218375e-02 5.42136083e-02 5.61924912e-02 4.89451054e-02\n",
      " 8.05734814e-02]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,feature_fraction=0.9,bagging_fraction=0.9,\n",
    "               bag_frec=3,met='binary_logloss',max_d=3):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    cls_auc_res = [0,0,0,0,0,0]\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "\n",
    "            # share params\n",
    "            params = {\n",
    "                    'application': 'binary',\n",
    "                    #'num_leaves': 8,\n",
    "                    #'lambda_l1': 1,\n",
    "                    'lambda_l2': 1.0,\n",
    "                    'max_depth': max_d,\n",
    "                    'metric': met, # or auc\n",
    "                    'data_random_seed': 2,\n",
    "                    'learning_rate':lr,\n",
    "                    # 'bagging_fraction': bagging_fraction,\n",
    "                    # 'bagging_freq':bag_frec,\n",
    "                    'feature_fraction': feature_fraction,\n",
    "\n",
    "                    }\n",
    "            if met == 'auc':\n",
    "                s_round = 60\n",
    "            else:\n",
    "                s_round = 30\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i], lr, feature_fraction, max_d)\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        cls_auc_res[i] = val_auc_l\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred, cls_auc_res\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.07006243401594699 0.07815096070482307 auc 0.9905916359258318 0.9871330890733523\n",
      "1 fold: ls 0.06927252834677794 0.07087472482786562 auc 0.9908541818460636 0.9899988856091371\n",
      "2 fold: ls 0.07041427383633003 0.07557685660013301 auc 0.9904400414270562 0.9884979460779784\n",
      "3 fold: ls 0.06617862164930581 0.07324331272570839 auc 0.9918566864676601 0.9890381991443653\n",
      "4 fold: ls 0.07016092121569122 0.07747709516959411 auc 0.9905020567115067 0.9879844039177511\n",
      "5 fold: ls 0.06744077438596482 0.07512222473078832 auc 0.9914720671511664 0.988201353628391\n",
      "6 fold: ls 0.06977119458540804 0.07744602455120202 auc 0.9906751052370067 0.9879179953665417\n",
      "7 fold: ls 0.06778442608177622 0.06974776246043846 auc 0.9913472906862133 0.9905525134704237\n",
      "8 fold: ls 0.06637329323330114 0.07370617080865154 auc 0.9917788730773065 0.9895266229028914\n",
      "9 fold: ls 0.06846744787032376 0.07655402601863402 auc 0.9910880600586808 0.988065986840766\n",
      "toxic 0.05 0.6 3\n",
      "this class avg train 0.06859259152208261 avg val 0.07478991585978385\n",
      "this class auc train 0.9910605998588492 auc val 0.9886916996031598\n",
      "========================\n",
      "0 fold: ls 0.01793184011674037 0.020959816397639972 auc 0.9942311995127275 0.9914292948474492\n",
      "1 fold: ls 0.017501609523949885 0.02050630495588108 auc 0.9945957619106599 0.9917505380427902\n",
      "2 fold: ls 0.01826017607500235 0.020915591705348884 auc 0.9939927782531814 0.9912291112799086\n",
      "3 fold: ls 0.01808872072842618 0.01974674023681787 auc 0.9940953806591657 0.9926691669831624\n",
      "4 fold: ls 0.018112638771978287 0.021034413077624322 auc 0.9941039114584882 0.9913442366122294\n",
      "5 fold: ls 0.017052483418009797 0.01963262877962757 auc 0.9949902337103347 0.9924845195753622\n",
      "6 fold: ls 0.018130658062133626 0.01925076663274689 auc 0.9940843088827904 0.9928284289310565\n",
      "7 fold: ls 0.0180509911794714 0.020297915380967646 auc 0.9941468183590065 0.9920464955729592\n",
      "8 fold: ls 0.018085356150815992 0.020759435660442752 auc 0.9941305793644211 0.9914755727442875\n",
      "9 fold: ls 0.01827114578537939 0.01893889339797577 auc 0.9939590793205655 0.9931943132264187\n",
      "severe_toxic 0.05 0.6 3\n",
      "this class avg train 0.01794856198119073 avg val 0.02020425062250728\n",
      "this class auc train 0.9942330051431341 auc val 0.9920451677815623\n",
      "========================\n",
      "0 fold: ls 0.035296464369824455 0.03716023964942702 auc 0.996226548952463 0.9956698590539044\n",
      "1 fold: ls 0.03531793454150602 0.03652791846888005 auc 0.9962259987753619 0.9957484778377642\n",
      "2 fold: ls 0.033358891290952604 0.039059641709552895 auc 0.9966520306452388 0.9953408240169653\n",
      "3 fold: ls 0.0359331542755082 0.03375206820216877 auc 0.9960807726177884 0.9964576135270846\n",
      "4 fold: ls 0.03518285416233806 0.0423371782907954 auc 0.996260147405382 0.9944519970805754\n",
      "5 fold: ls 0.03526587814912859 0.037393419208098244 auc 0.9962263750951876 0.9955692564551546\n",
      "6 fold: ls 0.036087189726983486 0.037610737559666894 auc 0.9960678430675157 0.9954869518639524\n",
      "7 fold: ls 0.03526864143688625 0.03811296259584772 auc 0.9962258810030408 0.9955780272584036\n",
      "8 fold: ls 0.03480474123792741 0.038792342175326185 auc 0.9963510724468267 0.9952310323548667\n",
      "9 fold: ls 0.03475084499902316 0.03879800290497671 auc 0.9963484996293105 0.9953030014125179\n",
      "obscene 0.05 0.6 3\n",
      "this class avg train 0.035126659419007826 avg val 0.03795445107647399\n",
      "this class auc train 0.9962665169638114 auc val 0.995483704086119\n",
      "========================\n",
      "0 fold: ls 0.005141925028842278 0.006712409645701466 auc 0.9984048548052589 0.9968219673161534\n",
      "1 fold: ls 0.004452890072550788 0.006326316342106732 auc 0.9990302132463758 0.9964749633354284\n",
      "2 fold: ls 0.004172839192824926 0.007064486462249492 auc 0.9991919269355901 0.9948172009218521\n",
      "3 fold: ls 0.0044110541980268685 0.0065610701451675035 auc 0.999014432884537 0.9923340038133551\n",
      "4 fold: ls 0.0038864591558148086 0.0066809695932112355 auc 0.999342405738461 0.9965087895740357\n",
      "5 fold: ls 0.004914441308028989 0.006982027352580618 auc 0.9986387575459906 0.9957309279862552\n",
      "6 fold: ls 0.005081721524465908 0.006190816469917386 auc 0.9984534536144093 0.9952974731284179\n",
      "7 fold: ls 0.005055859621448329 0.0075248411462815815 auc 0.9985531789312565 0.9956981897039412\n",
      "8 fold: ls 0.004775172403861319 0.007021269331828094 auc 0.9987737858563829 0.9951318870758289\n",
      "9 fold: ls 0.0051105529400156035 0.007499920650519067 auc 0.9984875544429518 0.9924731484787817\n",
      "threat 0.05 0.6 3\n",
      "this class avg train 0.004700291544587981 avg val 0.006856412713956317\n",
      "this class auc train 0.9987890564001214 auc val 0.995128855133405\n",
      "========================\n",
      "0 fold: ls 0.04969944767389946 0.049770287029909675 auc 0.9917950680999286 0.9914760464314754\n",
      "1 fold: ls 0.0496787724881096 0.053186405997015726 auc 0.9918021365730345 0.990078517913729\n",
      "2 fold: ls 0.05124772735190867 0.05493946007411836 auc 0.9912354007409299 0.989136654296986\n",
      "3 fold: ls 0.049128819429208305 0.051419279034258226 auc 0.9919982998756803 0.9906847605312381\n",
      "4 fold: ls 0.05091600351795523 0.05540362443383996 auc 0.9913265236879294 0.9890792167970142\n",
      "5 fold: ls 0.04784440242680664 0.052488911078310004 auc 0.9924904894421978 0.9905440162661426\n",
      "6 fold: ls 0.05032309177311113 0.055942365347609144 auc 0.9915277417326347 0.9892955610443822\n",
      "7 fold: ls 0.049395044152658815 0.05506388356295962 auc 0.9918859423495079 0.989674403667012\n",
      "8 fold: ls 0.04989692335219668 0.05205162099664059 auc 0.9917150870294787 0.9905047770552579\n",
      "9 fold: ls 0.049247733865349225 0.05440877395698474 auc 0.9919676268366872 0.9895009240657756\n",
      "insult 0.05 0.6 3\n",
      "this class avg train 0.049737796603120374 avg val 0.05346746115116461\n",
      "this class auc train 0.9917744316368008 auc val 0.9899974878069013\n",
      "========================\n",
      "0 fold: ls 0.014722910341148993 0.016809681624080346 auc 0.9955541830646039 0.993166074566507\n",
      "1 fold: ls 0.014577758794018026 0.017610776913277882 auc 0.9956714125498829 0.9917155300630394\n",
      "2 fold: ls 0.015201201997700009 0.01732236333448912 auc 0.9951294130595947 0.9928078102517401\n",
      "3 fold: ls 0.01523149722343596 0.018229098452741687 auc 0.9951728968453447 0.9889772069462922\n",
      "4 fold: ls 0.014310719835475017 0.01849217365139717 auc 0.9958698128801108 0.990025096437669\n",
      "5 fold: ls 0.015304132368282455 0.016411696415994492 auc 0.9950447901287647 0.9936431416468718\n",
      "6 fold: ls 0.013964141344453868 0.016313440098120593 auc 0.9961460132085505 0.9926123636100874\n",
      "7 fold: ls 0.015437665724074444 0.018485713929193258 auc 0.9949418582211911 0.987248446419539\n",
      "8 fold: ls 0.012445680625275136 0.016738021696967448 auc 0.9972345788021786 0.9930815991039815\n",
      "9 fold: ls 0.015362980772546966 0.01645168829895414 auc 0.9950563674959148 0.9925780403208324\n",
      "identity_hate 0.05 0.6 3\n",
      "this class avg train 0.014655868902641086 avg val 0.017286465441521615\n",
      "this class auc train 0.9955821326256136 auc val 0.9915855309366559\n",
      "========================\n",
      "all loss avg 0.031793628328771764 0.03509315947756795\n",
      "all auc avg 0.9946176237713884 0.9921554075579672\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999132      0.369871  0.982821  0.157875  0.921243   \n",
      "1  0000247867823ef7  0.000193      0.000036  0.000084  0.000023  0.000072   \n",
      "2  00013b17ad220c46  0.000365      0.000039  0.000166  0.000025  0.000177   \n",
      "3  00017563c3f7919a  0.000049      0.000036  0.000063  0.000043  0.000061   \n",
      "4  00017695ad8997eb  0.001067      0.000041  0.000183  0.000079  0.000201   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.460115  \n",
      "1       0.000034  \n",
      "2       0.000054  \n",
      "3       0.000035  \n",
      "4       0.000042  \n",
      "save done\n",
      "CPU times: user 2h 43min 13s, sys: 15.4 s, total: 2h 43min 29s\n",
      "Wall time: 24min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lgb_res,_ = simple_ens('lgb',10,666,0.05,0.6)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_some_fold10.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# select feats\n",
    "# all loss avg 0.0323893274343 0.0361490549659\n",
    "# all auc avg 0.994298774643 0.991023792329\n",
    "\n",
    "# 1 fold\n",
    "# rf tfidf select feats, feat_fraction 0.6 this fold avg train 0.0324570163191 avg val 0.0365732819959\n",
    "# rf tfidf select feats, feat_fraction 0.75 this fold avg train 0.0325535548896 avg val 0.0366403197325\n",
    "# rf tfidf select feats, feat_fraction 0.5 this fold avg train 0.0328270646453 avg val 0.0366173320613\n",
    "\n",
    "# rm muse, rm ft, glove gru lstm , feat frac 0.6, min max scalar\n",
    "# all loss avg 0.03185315549604115 0.03507562597568142 all auc avg 0.9945693363386173 0.9922054290233556\n",
    "\n",
    "# select rm based on analyse_corr, feat frac 0.6, min max scalar\n",
    "# all loss avg 0.031793628328771764 0.03509315947756795 all auc avg 0.9946176237713884 0.9921554075579672"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
