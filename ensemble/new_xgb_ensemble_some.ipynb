{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'../features/fasttext_cnn2d_5_feat.pkl',\n",
      "'../features/fasttext_cnn_gru_5_feat.pkl',\n",
      "'../features/fasttext_cnn_v1_5_feat.pkl',\n",
      "'../features/fasttext_cnn_v2_5_feat.pkl',\n",
      "'../features/fasttext_cudnn_gru_5_feat.pkl',\n",
      "'../features/fasttext_gru_v1_5_feat.pkl',\n",
      "'../features/fasttext_lstm_v1_5_feat.pkl',\n",
      "'../features/glove_cnn2d_5_feat.pkl',\n",
      "'../features/glove_cnn_gru_5_feat.pkl',\n",
      "'../features/glove_cnn_v1_5_feat.pkl',\n",
      "'../features/glove_cnn_v2_5_feat.pkl',\n",
      "'../features/glove_cudnn_gru_5_feat.pkl',\n",
      "'../features/glove_gru_v1_5_feat.pkl',\n",
      "'../features/glove_lstm_v1_5_feat.pkl',\n",
      "'../features/lgb1_feat.pkl',\n",
      "'../features/lr_feat1.pkl',\n",
      "'../features/lr_feat2.pkl',\n",
      "'../features/lstm_attention_fasttext_10_feat.pkl',\n",
      "'../features/lstm_attention_fasttext_4_feat.pkl',\n",
      "'../features/lstm_attention_fasttext_adj2_4_feat.pkl',\n",
      "'../features/lstm_attention_glove_5_feat.pkl',\n",
      "'../features/mnb_feat1.pkl',\n",
      "'../features/mnb_feat2.pkl',\n",
      "'../features/muse_cnn2d_5_feat.pkl',\n",
      "'../features/muse_cnn_gru_5_feat.pkl',\n",
      "'../features/muse_cnn_v1_5_feat.pkl',\n",
      "'../features/muse_cnn_v2_5_feat.pkl',\n",
      "'../features/muse_cudnn_gru_5_feat.pkl',\n",
      "'../features/muse_gru_v1_5_feat.pkl',\n",
      "'../features/muse_lstm_v1_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn2d_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn_gru_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn_v1_5_feat.pkl',\n",
      "'../features/no_pretrained_cnn_v2_5_feat.pkl',\n",
      "'../features/no_pretrained_cudnn_gru_5_feat.pkl',\n",
      "'../features/no_pretrained_gru_v1_5_feat.pkl',\n",
      "'../features/no_pretrained_lstm_v1_5_feat.pkl',\n",
      "'../features/other_feat.pkl',\n",
      "'../features/pool_gru_fasttext_4_feat.pkl',\n",
      "'../features/pool_gru_fasttext_adj1_10_feat.pkl',\n",
      "'../features/pool_gru_fasttext_adj2_4_feat.pkl',\n",
      "'../features/pool_gru_glove_4_feat.pkl',\n",
      "'../features/tfidf_feat1.pkl',\n",
      "'../features/tfidf_feat2.pkl',\n",
      "'../features/tilli_lr_feat.pkl',\n",
      "'../features/wordbatch_feat.pkl',\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import xgboost as xgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(7200)\n",
    "# print('sleep done =======================')\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat:\n",
    "        continue\n",
    "    print(\"'{}',\".format(feat))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 36) (153164, 36)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 144)\n",
      "(159571, 144)\n"
     ]
    }
   ],
   "source": [
    "feats_files = [\n",
    "#'../features/fasttext_cnn2d_4_feat.pkl',\n",
    "# '../features/fasttext_cnn_gru_4_feat.pkl',\n",
    "'../features/fasttext_cnn_v1_4_feat.pkl',\n",
    "'../features/fasttext_cnn_v2_4_feat.pkl',\n",
    "# '../features/fasttext_cudnn_gru_4_feat.pkl',\n",
    "# '../features/fasttext_gru_v1_4_feat.pkl',\n",
    "# '../features/fasttext_lstm_v1_4_feat.pkl',\n",
    "# '../features/glove_cnn2d_4_feat.pkl',\n",
    "# '../features/glove_cnn_gru_4_feat.pkl',\n",
    "# '../features/glove_cnn_v1_4_feat.pkl',\n",
    "'../features/glove_cnn_v2_4_feat.pkl',\n",
    "# '../features/glove_cudnn_gru_4_feat.pkl',\n",
    "# '../features/glove_gru_v1_4_feat.pkl',\n",
    "# '../features/glove_lstm_v1_4_feat.pkl',\n",
    "'../features/lr_feat1.pkl',\n",
    "'../features/lr_feat2.pkl',\n",
    "'../features/lstm_attention_fasttext_10_feat.pkl',\n",
    "# '../features/lstm_attention_fasttext_4_feat.pkl',\n",
    "# '../features/lstm_attention_fasttext_adj2_4_feat.pkl',\n",
    "# '../features/lstm_attention_glove_4_feat.pkl',\n",
    "'../features/mnb_feat1.pkl',\n",
    "'../features/mnb_feat2.pkl',\n",
    "'../features/muse_cnn2d_4_feat.pkl',\n",
    "# '../features/muse_cnn_gru_4_feat.pkl',\n",
    "# '../features/muse_cnn_v1_4_feat.pkl',\n",
    "# '../features/muse_cnn_v2_4_feat.pkl',\n",
    "# '../features/muse_cudnn_gru_4_feat.pkl',\n",
    "# '../features/muse_gru_v1_4_feat.pkl',\n",
    "# '../features/muse_lstm_v1_4_feat.pkl',\n",
    "'../features/no_pretrained_cnn2d_4_feat.pkl',\n",
    "'../features/no_pretrained_cnn_gru_4_feat.pkl',\n",
    "'../features/no_pretrained_cnn_v1_4_feat.pkl',\n",
    "'../features/no_pretrained_cnn_v2_4_feat.pkl',\n",
    "'../features/no_pretrained_cudnn_gru_4_feat.pkl',\n",
    "'../features/no_pretrained_gru_v1_4_feat.pkl',\n",
    "'../features/no_pretrained_lstm_v1_4_feat.pkl',\n",
    "'../features/other_feat.pkl',\n",
    "# '../features/pool_gru_fasttext_4_feat.pkl',\n",
    "'../features/pool_gru_fasttext_adj1_10_feat.pkl',\n",
    "'../features/pool_gru_fasttext_adj2_4_feat.pkl',\n",
    "# '../features/pool_gru_glove_4_feat.pkl',\n",
    "# '../features/tfidf_feat1.pkl',\n",
    "# '../features/tfidf_feat2.pkl',\n",
    "]\n",
    "\n",
    "for feat in feats_files:\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def simple_ens(model_name,k=3,rnd=233):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        d_test = xgb.DMatrix(test_x)\n",
    "        \n",
    "        # share params\n",
    "        params = {\n",
    "                'subsample': 0.9,\n",
    "                'eta': 0.05,\n",
    "                'max_depth': 3,\n",
    "                'eval_metric':'logloss',\n",
    "                #'eval_metric':'auc',\n",
    "                'objective':'binary:logistic',\n",
    "                #'scale_pos_weight':0.9,\n",
    "                'colsample_bylevel':0.9,\n",
    "                'colsample_bytree':0.9,\n",
    "            \n",
    "                }\n",
    "        \n",
    "        # train for each class\n",
    "        for i in range(6):\n",
    "            d_train = xgb.DMatrix(curr_x, curr_y[:,i])\n",
    "            d_valid = xgb.DMatrix(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "\n",
    "            model = xgb.train(params, d_train, 1000, watchlist,\n",
    "                              early_stopping_rounds=50,\n",
    "                              verbose_eval=None)\n",
    "            print(i)\n",
    "            try:\n",
    "                train_pred = model.predict(d_train)\n",
    "                tmp_test_pred = model.predict(d_valid)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "            \n",
    "        # avg 6 class\n",
    "        train_loss_l = train_loss_l/6\n",
    "        val_loss_l = val_loss_l/6\n",
    "        train_auc_l = train_auc_l/6\n",
    "        val_auc_l = val_auc_l/6\n",
    "        print('this fold avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this fold auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        \n",
    "        \n",
    "        # avg k fold\n",
    "        all_train_loss_l += train_loss_l/k\n",
    "        all_val_loss_l += val_loss_l/k\n",
    "        all_train_auc_l += train_auc_l/k\n",
    "        all_val_auc_l += val_auc_l/k\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "ls 0.068227023931 0.0756803462098 auc 0.990956070604 0.988661955352\n",
      "1\n",
      "ls 0.0168721160139 0.0205382626056 auc 0.995130640603 0.992215823781\n",
      "2\n",
      "ls 0.0355603197706 0.0408421511733 auc 0.996117040618 0.993968340413\n",
      "3\n",
      "ls 0.00423023857281 0.00708573402938 auc 0.999035327784 0.997482343966\n",
      "4\n",
      "ls 0.0499585491875 0.0572201400685 auc 0.991664013202 0.987197869516\n",
      "5\n",
      "ls 0.0146531519532 0.0197015295552 auc 0.995327626385 0.988940443067\n",
      "this fold avg train 0.0315835665715 avg val 0.0368446939403\n",
      "this fold auc train 0.994705119866 auc val 0.991411129349\n",
      "========================\n",
      "0\n",
      "ls 0.0661946149474 0.0778033138322 auc 0.991723521131 0.986549293055\n",
      "1\n",
      "ls 0.016849369443 0.0193734259501 auc 0.995122339654 0.993433272783\n",
      "2\n",
      "ls 0.0351190899722 0.0356029823941 auc 0.996240371647 0.99583696848\n",
      "3\n",
      "ls 0.00360188875017 0.00726070374693 auc 0.99942741501 0.992413292734\n",
      "4\n",
      "ls 0.0494784717463 0.0491870992297 auc 0.991861987261 0.991485631165\n",
      "5\n",
      "ls 0.0149520654785 0.0165350506493 auc 0.995254074157 0.985675948728\n",
      "this fold avg train 0.0310325833896 avg val 0.0342937626337\n",
      "this fold auc train 0.99493828481 auc val 0.990899067824\n",
      "========================\n",
      "0\n",
      "ls 0.0674777975193 0.0730756580634 auc 0.991358671027 0.988372826869\n",
      "1\n",
      "ls 0.0169459550286 0.0186664083147 auc 0.995187591726 0.992098465485\n",
      "2\n",
      "ls 0.0355103920404 0.0394750815074 auc 0.99610787646 0.995527838294\n",
      "3\n",
      "ls 0.00398402369492 0.00796932762826 auc 0.999252263346 0.983996982461\n",
      "4\n",
      "ls 0.0499530056162 0.0540275316899 auc 0.991682437371 0.989232003716\n",
      "5\n",
      "ls 0.0133993965945 0.0188430785311 auc 0.996312413022 0.989950028161\n",
      "this fold avg train 0.031211761749 avg val 0.0353428476225\n",
      "this fold auc train 0.994983542159 auc val 0.989863024164\n",
      "========================\n",
      "0\n",
      "ls 0.0712823060154 0.0819293325214 auc 0.990006534196 0.986183071464\n",
      "1\n",
      "ls 0.016614548863 0.0200909270261 auc 0.995337767967 0.992606495749\n",
      "2\n",
      "ls 0.0346770477864 0.0422707809256 auc 0.996319296799 0.99389931924\n",
      "3\n",
      "ls 0.00399845270381 0.00658194438381 auc 0.999248826684 0.993675839651\n",
      "4\n",
      "ls 0.0494619529522 0.0543034838242 auc 0.991877258955 0.98892808624\n",
      "5\n",
      "ls 0.0145505265149 0.0156440331885 auc 0.995622464891 0.99206973122\n",
      "this fold avg train 0.0317641391393 avg val 0.0368034169782\n",
      "this fold auc train 0.994735358249 auc val 0.991227090594\n",
      "========================\n",
      "0\n",
      "ls 0.0680414961441 0.0788490866282 auc 0.99101593409 0.987829273004\n",
      "1\n",
      "ls 0.0169373639121 0.0213281550989 auc 0.995070742312 0.990923155688\n",
      "2\n",
      "ls 0.0358051792456 0.0374082858359 auc 0.996074390444 0.995297469368\n",
      "3\n",
      "ls 0.00373053078017 0.00725700237243 auc 0.999356588762 0.995833491515\n",
      "4\n",
      "ls 0.0502104065541 0.0549416448589 auc 0.991554120974 0.989861965881\n",
      "5\n",
      "ls 0.014292915002 0.0188061096076 auc 0.995655317109 0.986461697323\n",
      "this fold avg train 0.0315029819397 avg val 0.036431714067\n",
      "this fold auc train 0.994787848949 auc val 0.991034508796\n",
      "========================\n",
      "0\n",
      "ls 0.0670885158246 0.0781535495678 auc 0.991343856347 0.986959513336\n",
      "1\n",
      "ls 0.0172806682216 0.0201698454651 auc 0.994844958057 0.991149517368\n",
      "2\n",
      "ls 0.0351323540916 0.0433334292645 auc 0.996189669104 0.993871654289\n",
      "3\n",
      "ls 0.00390647082478 0.00639427993905 auc 0.999266090592 0.995125322578\n",
      "4\n",
      "ls 0.0505551910124 0.0573309831032 auc 0.991382451689 0.988769977408\n",
      "5\n",
      "ls 0.0143021710745 0.0175363736192 auc 0.995670587922 0.992317052958\n",
      "this fold avg train 0.0313775618416 avg val 0.0371530768265\n",
      "this fold auc train 0.994782935618 auc val 0.991365506323\n",
      "========================\n",
      "0\n",
      "ls 0.0674927759046 0.0784132357058 auc 0.991225349273 0.987375585136\n",
      "1\n",
      "ls 0.0167513499101 0.0220827465009 auc 0.995121826698 0.991864097567\n",
      "2\n",
      "ls 0.0326330520702 0.0354500925422 auc 0.996825645888 0.996031172089\n",
      "3\n",
      "ls 0.00361312731612 0.00541142726878 auc 0.999466336116 0.993609919788\n",
      "4\n",
      "ls 0.047591746505 0.0512956062264 auc 0.992511259191 0.991345880332\n",
      "5\n",
      "ls 0.0143342524018 0.0189086136877 auc 0.995522161798 0.993123262889\n",
      "this fold avg train 0.0304027173513 avg val 0.0352602869886\n",
      "this fold auc train 0.995112096494 auc val 0.9922249863\n",
      "========================\n",
      "0\n",
      "ls 0.0699144491251 0.0753188173089 auc 0.990418513826 0.988465606126\n",
      "1\n",
      "ls 0.0172962424192 0.019708089183 auc 0.994816070879 0.992053254452\n",
      "2\n",
      "ls 0.0351260416509 0.0418429444812 auc 0.99621951648 0.994419272583\n",
      "3\n",
      "ls 0.0041341810355 0.0065222418196 auc 0.999118390965 0.993692030631\n",
      "4\n",
      "ls 0.0468559950732 0.0570042317892 auc 0.99276117117 0.988513640688\n",
      "5\n",
      "ls 0.014287214548 0.0170208107195 auc 0.995632017587 0.99361636347\n",
      "this fold avg train 0.031269020642 avg val 0.0362361892169\n",
      "this fold auc train 0.994827613485 auc val 0.991793361325\n",
      "========================\n",
      "0\n",
      "ls 0.0667938630053 0.0784950117425 auc 0.991457986958 0.987407137497\n",
      "1\n",
      "ls 0.017183901651 0.0213557426226 auc 0.994885896483 0.990556617969\n",
      "2\n",
      "ls 0.0355587970029 0.0426670879411 auc 0.996126850601 0.993646642092\n",
      "3\n",
      "ls 0.0037897929972 0.00914899024506 auc 0.999235297815 0.991927617787\n",
      "4\n",
      "ls 0.0487561074229 0.0534176157925 auc 0.992090414505 0.989626777211\n",
      "5\n",
      "ls 0.0149599908578 0.0188256652192 auc 0.995159317395 0.985952006697\n",
      "this fold avg train 0.0311737421562 avg val 0.0373183522605\n",
      "this fold auc train 0.994825960626 auc val 0.989852799876\n",
      "========================\n",
      "0\n",
      "ls 0.0722721801369 0.0749999147184 auc 0.989646963346 0.98934637247\n",
      "1\n",
      "ls 0.0168820708922 0.0224775806813 auc 0.995114518375 0.989880990061\n",
      "2\n",
      "ls 0.0360275850864 0.0389522866479 auc 0.995991204726 0.995360216288\n",
      "3\n",
      "ls 0.00345020334604 0.00687584705095 auc 0.99949922187 0.996544650804\n",
      "4\n",
      "ls 0.0451329535903 0.0575921346566 auc 0.993300002815 0.989482817316\n",
      "5\n",
      "ls 0.0148208643401 0.0188712711792 auc 0.995189245511 0.988806943657\n",
      "this fold avg train 0.031430976232 avg val 0.0366281724891\n",
      "this fold auc train 0.994790192774 auc val 0.991570331766\n",
      "========================\n",
      "all loss avg 0.0312749051012 0.0362312513023\n",
      "all auc avg 0.994848895303 0.991124180632\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999259      0.279870  0.978139  0.270651  0.920685   \n",
      "1  0000247867823ef7  0.000205      0.000023  0.000060  0.000018  0.000036   \n",
      "2  00013b17ad220c46  0.000426      0.000025  0.000327  0.000018  0.000229   \n",
      "3  00017563c3f7919a  0.000165      0.000025  0.000086  0.000017  0.000056   \n",
      "4  00017695ad8997eb  0.002472      0.000040  0.000260  0.000020  0.000200   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.529059  \n",
      "1       0.000033  \n",
      "2       0.000071  \n",
      "3       0.000037  \n",
      "4       0.000083  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "xgb_res = simple_ens('xgb',k=10)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = xgb_res\n",
    "sample_submission.to_csv(\"../results/xgb_some_fold10.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "# all train avg 0.0317569652451 all val avg 0.0368260957171, PUB 9863\n",
    "\n",
    "# all loss avg 0.0312749051012 0.0362312513023 all auc avg 0.994848895303 0.991124180632\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
