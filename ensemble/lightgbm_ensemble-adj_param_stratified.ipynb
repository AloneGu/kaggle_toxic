{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/cnn_word_char_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/gbrt1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_adj2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/pool_gru_fasttext_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj1_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/rf1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 331)\n",
      "(159571, 331)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(25200)\n",
    "# print('sleep done =======================')\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat or 'tfidf' in feat:\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.hstack(train_x)\n",
    "test_x = np.hstack(test_x)\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,feature_fraction=0.9,bagging_fraction=0.9,\n",
    "               bag_frec=3,met='binary_logloss',max_d=3):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    cls_ls_res = [0,0,0,0,0,0]\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "\n",
    "            # share params\n",
    "            params = {\n",
    "                    'application': 'binary',\n",
    "                    #'num_leaves': 8,\n",
    "                    'lambda_l1': 1.0,\n",
    "                    'lambda_l2': 1.0,\n",
    "                    'max_depth': max_d,\n",
    "                    'metric': met, # or auc\n",
    "                    'data_random_seed': 2,\n",
    "                    'learning_rate':lr,\n",
    "                    # 'bagging_fraction': bagging_fraction,\n",
    "                    # 'bagging_freq':bag_frec,\n",
    "                    'feature_fraction': feature_fraction,\n",
    "\n",
    "                    }\n",
    "            if met == 'auc':\n",
    "                s_round = 60\n",
    "            else:\n",
    "                s_round = 30\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i], lr, feature_fraction, max_d)\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        cls_ls_res[i] = val_loss_l\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred, cls_ls_res\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.07262667098305266 0.07828527004452254 auc 0.989765181925294 0.9871551050879644\n",
      "1 fold: ls 0.07455575160517923 0.07205779794971094 auc 0.9892165966191713 0.990177301398334\n",
      "2 fold: ls 0.06770034443410783 0.07456530478716718 auc 0.9913176692577903 0.9891083242279445\n",
      "3 fold: ls 0.06687598087605144 0.07268668991586692 auc 0.991598439070832 0.9893534449173811\n",
      "4 fold: ls 0.0680157879370256 0.07619549714256563 auc 0.9912024125191861 0.9885683911977708\n",
      "5 fold: ls 0.06728444951643983 0.07549045627834228 auc 0.991484014303831 0.9879834066562311\n",
      "6 fold: ls 0.06833645150737794 0.07680321994825035 auc 0.9911140850498685 0.9882195309860938\n",
      "7 fold: ls 0.07000853538047425 0.0689688934177122 auc 0.9906002967183486 0.9908057447877121\n",
      "8 fold: ls 0.0635268524739894 0.07350598279609777 auc 0.9925733783469457 0.9896426759233456\n",
      "9 fold: ls 0.07355503987774081 0.07712575683555574 auc 0.9895195551714439 0.9881616625828245\n",
      "toxic 0.05 0.9 3\n",
      "this class avg train 0.0692485864591439 avg val 0.07456848691157915\n",
      "this class auc train 0.9908391628982711 auc val 0.9889175587765602\n",
      "========================\n",
      "0 fold: ls 0.125624146136663 0.12659840290139354 auc 0.9917723050504048 0.991753307380681\n",
      "1 fold: ls 0.018983496362411558 0.020953169856438848 auc 0.9938318081231344 0.9917784292315484\n",
      "2 fold: ls 0.1376973316367231 0.13829160491149575 auc 0.992358790764465 0.9890247183187745\n",
      "3 fold: ls 0.19271355665453752 0.1928072464592896 auc 0.9899248172151874 0.992673518799848\n",
      "4 fold: ls 0.0207830900748175 0.022732268324911098 auc 0.9936728303917801 0.9917303614381567\n",
      "5 fold: ls 0.04668850975276521 0.04746066231185734 auc 0.9929141918535872 0.9923569260020974\n",
      "6 fold: ls 0.21318647482525577 0.21301998413454343 auc 0.9911651028742634 0.9929968392215225\n",
      "7 fold: ls 0.02745392104214712 0.02858618134612125 auc 0.9931392929478573 0.9923618169678742\n",
      "8 fold: ls 0.019104358456923064 0.021355053981792167 auc 0.993845430847979 0.991499858861825\n",
      "9 fold: ls 0.0218434794411676 0.021356187540260366 auc 0.9934249483971722 0.9938536215976045\n",
      "severe_toxic 0.05 0.9 3\n",
      "this class avg train 0.08240783643834114 avg val 0.08331607617681033\n",
      "this class auc train 0.9926049518465832 auc val 0.9920029397819933\n",
      "========================\n",
      "0 fold: ls 0.03900929301323196 0.03946944062108216 auc 0.9958507407659908 0.9957009855146456\n",
      "1 fold: ls 0.03680831899932793 0.03703935054819164 auc 0.9959924635831783 0.9957375150591383\n",
      "2 fold: ls 0.0341184253638261 0.038485520639421095 auc 0.9964761773519685 0.9954694102574544\n",
      "3 fold: ls 0.03587840095529546 0.03419093925051683 auc 0.9960910228542855 0.9963524813542121\n",
      "4 fold: ls 0.05364570922505036 0.05845203940065373 auc 0.9954806014975304 0.9945051309199006\n",
      "5 fold: ls 0.034910442643980175 0.03746455371363743 auc 0.9963070547346965 0.9955812379988785\n",
      "6 fold: ls 0.0387729435095767 0.03986131994179091 auc 0.9959513427142642 0.9956594312760579\n",
      "7 fold: ls 0.03708304763311489 0.039019419635121 auc 0.995994503742163 0.9955797892501277\n",
      "8 fold: ls 0.03545316246995613 0.03851608907093278 auc 0.9962020587030976 0.9954095025388342\n",
      "9 fold: ls 0.04933162953321072 0.050980502332960975 auc 0.9955275598765047 0.9954759987982308\n",
      "obscene 0.05 0.9 3\n",
      "this class avg train 0.039501137334657044 avg val 0.04134791751543086\n",
      "this class auc train 0.9959873525823679 auc val 0.9955471482967481\n",
      "========================\n",
      "0 fold: ls 0.020526551957732866 0.021326990643076648 auc 0.9904712931199283 0.9962896763042112\n",
      "1 fold: ls 0.0054416846307294335 0.006330092754846563 auc 0.9980409523087141 0.9971061177456527\n",
      "2 fold: ls 0.04861335556802657 0.049141624544892806 auc 0.9890220987323266 0.996598051539912\n",
      "3 fold: ls 0.06118301861520093 0.06151362492253033 auc 0.9857398319157397 0.9861445041590713\n",
      "4 fold: ls 0.0052545799737459455 0.006732328797251506 auc 0.9982021182047105 0.9967942673958137\n",
      "5 fold: ls 0.030250688863424587 0.030857209686565582 auc 0.9903415868214456 0.9974555806985563\n",
      "6 fold: ls 0.24077768489877077 0.2409258952218725 auc 0.968704100367197 0.9770118329247597\n",
      "7 fold: ls 0.00577891274954497 0.00761612195340194 auc 0.9968924762933108 0.9958920003352401\n",
      "8 fold: ls 0.005704484346733323 0.007346255733726435 auc 0.9969663991154827 0.9952535898989331\n",
      "9 fold: ls 0.006753749166346313 0.008196458109328302 auc 0.9967064995661158 0.9950870843882026\n",
      "threat 0.05 0.9 3\n",
      "this class avg train 0.04302847107702557 avg val 0.04399866023674926\n",
      "this class auc train 0.9911087356444972 auc val 0.9933632705390352\n",
      "========================\n",
      "0 fold: ls 0.05193918420156152 0.050178149808735736 auc 0.9909316552982764 0.99147646470291\n",
      "1 fold: ls 0.05599666086615089 0.05727905680392306 auc 0.9902962196111298 0.9900293710201474\n",
      "2 fold: ls 0.05145762271672194 0.05504717922540671 auc 0.9911695996728047 0.9893027498837207\n",
      "3 fold: ls 0.05244161721880613 0.052080162964469476 auc 0.9907754953584094 0.9908693437153882\n",
      "4 fold: ls 0.049357469341825506 0.05498066915872608 auc 0.9919073725799138 0.9891150231921703\n",
      "5 fold: ls 0.04727577007590673 0.051992280690614456 auc 0.9926930357219014 0.9907550899459993\n",
      "6 fold: ls 0.052666923304626564 0.05676581526827153 auc 0.9907855533598262 0.9894861380728063\n",
      "7 fold: ls 0.07121581157443299 0.07432840581483505 auc 0.9897915678013268 0.9896300076319298\n",
      "8 fold: ls 0.05172861887702696 0.052583324336531 auc 0.9910026646183774 0.9905402938833238\n",
      "9 fold: ls 0.06085831602835747 0.06271490947852101 auc 0.9901444095572943 0.9894007398054766\n",
      "insult 0.05 0.9 3\n",
      "this class avg train 0.054493799420541666 avg val 0.05679499535500341\n",
      "this class auc train 0.990949757357926 auc val 0.9900605221853873\n",
      "========================\n",
      "0 fold: ls 0.01691816880994747 0.018379612230689072 auc 0.9938445349951549 0.9924369909922756\n",
      "1 fold: ls 0.016477694816498176 0.01756976075391598 auc 0.9939367713075663 0.9928566848578847\n",
      "2 fold: ls 0.06215778927232074 0.06326714488994116 auc 0.9870346151164755 0.9933815263853372\n",
      "3 fold: ls 0.03849985883041909 0.03968255537814632 auc 0.9900970318670346 0.9881001543809808\n",
      "4 fold: ls 0.023270894295831183 0.025401477402629113 auc 0.9931733513131525 0.9886720769510495\n",
      "5 fold: ls 0.015387079553352301 0.01627856960946277 auc 0.9949119512175958 0.993839810691932\n",
      "6 fold: ls 0.022802059325380618 0.022798629194364803 auc 0.9926929810312777 0.9931687621938002\n",
      "7 fold: ls 0.02644060580860354 0.028455863741910663 auc 0.9930384552909866 0.9881887238962355\n",
      "8 fold: ls 0.012945681611536758 0.016758773177950992 auc 0.9968402937146237 0.992881078835176\n",
      "9 fold: ls 0.018180433308327432 0.01832191883732766 auc 0.993044191861345 0.9927880446564058\n",
      "identity_hate 0.05 0.9 3\n",
      "this class avg train 0.02530802656322173 avg val 0.026691430521633853\n",
      "this class auc train 0.9928614177715213 auc val 0.9916313853841077\n",
      "========================\n",
      "all loss avg 0.05233130954882184 0.054452927786201144\n",
      "all auc avg 0.9923918963501944 0.9919204708273053\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.997920      0.428518  0.975682  0.192162  0.926817   \n",
      "1  0000247867823ef7  0.000560      0.061144  0.004183  0.035086  0.004243   \n",
      "2  00013b17ad220c46  0.000690      0.061144  0.004191  0.035086  0.004265   \n",
      "3  00017563c3f7919a  0.000425      0.061144  0.004181  0.035097  0.004244   \n",
      "4  00017695ad8997eb  0.000880      0.061144  0.004196  0.035095  0.004273   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.427824  \n",
      "1       0.010191  \n",
      "2       0.010195  \n",
      "3       0.010192  \n",
      "4       0.010193  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "lgb_res,tmp_ls_res = simple_ens('lgb',10,666,0.05,0.9)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_log_csv_fold10_stratified.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# add 7 base fasttext NN models\n",
    "# all loss avg 0.0319116484218 0.0358161833583 all auc avg 0.994546891137 0.991249510282 PUB 9866\n",
    "\n",
    "# rm tfidf test\n",
    "# all loss avg 0.0319555637279 0.0357756335619 all auc avg 0.994512718368 0.991251471241\n",
    "\n",
    "# change to stratified\n",
    "# all loss avg 0.0316412744167 0.0357232681944 all auc avg 0.994584962017 0.991293307537 pub 9866\n",
    "\n",
    "# change to base model 5 fold, add word batch, tilli, lgb feat\n",
    "# feat dim 217\n",
    "# all loss avg 0.031983129767200906 0.035387924581 all auc avg 0.9945427088311004 0.99188514127 PUB 9870\n",
    "\n",
    "# add more base models\n",
    "# feat dim 247, all loss avg 0.031885221777487156 0.0353958  all auc avg 0.9945986685511962 0.9919487331094685\n",
    "\n",
    "# change early stopping to 30, and test later part\n",
    "# all loss avg 0.03208696729295824 0.035406264613808025 all auc avg 0.9945261773637045 0.991946174432887\n",
    "\n",
    "# add fasttext lstm v1\n",
    "# all loss avg 0.031977558955160815 0.03537909655655411 all auc avg 0.9945299073418443 0.9919307178575658\n",
    "\n",
    "# add muse base model, feat dim 295, lower loss, but lower auc\n",
    "# all loss avg 0.03196610276261484 0.03530962013098028 all auc avg 0.9945635742334386 0.9918682952070021\n",
    "\n",
    "# fix pool gru fold to 5, and adj params\n",
    "# all loss avg 0.03172365669814678 0.03528054768190897 all auc avg 0.9946437910139179 0.991903618166854\n",
    "\n",
    "# updated pool gru v2\n",
    "# all loss avg 0.03204678384090053 0.035272185628381025 all auc avg 0.9944980867573662 0.9919033708443966\n",
    "\n",
    "# updated other feat, change some cnt to ratio, a bit worse\n",
    "# all loss avg 0.03197308230179816 0.03527747086566205 all auc avg 0.9945654974291834 0.991902658184264\n",
    "\n",
    "# updated pool gru v2 10 fold PUB 9870\n",
    "# all loss avg 0.03188938973578164 0.035151701851945265 all auc avg 0.9945816630869728 0.9919824289801534\n",
    "\n",
    "# rm lr, mnb feat1\n",
    "# worse all loss avg 0.032097370918022436 0.03520186226431002\n",
    "# all auc avg 0.9944811076726177 0.9919214297276806\n",
    "\n",
    "# add ridge , change lr,mnb,ridge to fold 6\n",
    "# all loss avg 0.03187703661408994 0.0351472518210873 all auc avg 0.9946115814252721 0.9920990121249438\n",
    "\n",
    "# ridge, lr, mnb fold 10, feat frac 0.6\n",
    "# all loss avg 0.031754244562510095 0.03510006533423048 all auc avg 0.9946626820706096 0.9921195758845225\n",
    "\n",
    "# lgb v1 feat fold 10, 5 fold is better, change feat file back\n",
    "# all loss avg 0.031846113030824186 0.03510235514884163 all auc avg 0.9946147807624219 0.9920710159886731\n",
    "\n",
    "# feat frac 0.45 PUB 9871, HIGHER TRAIN CV AUC?\n",
    "# all loss avg 0.03178085998781136 0.035091249281527216 all auc avg 0.9946391992503673 0.9921624859169091\n",
    "\n",
    "# tilli feat 10 fold\n",
    "# all loss avg 0.031769047743715494 0.035076842284761586 all auc avg 0.9946504692097619 0.9921581333743735\n",
    "\n",
    "# update pool gru v2\n",
    "# all loss avg 0.031982729360866644 0.035041219881259106 all auc avg 0.9945421522346883 0.9922172582996719\n",
    "\n",
    "# updated rf, gbrt feat, new lstm att 5 fold feat, PUB 9870\n",
    "# all loss avg 0.031907540308479226 0.034974441310714074 all auc avg 0.994571268692479 0.9922363188745803\n",
    "\n",
    "# change feat frac to 0.6\n",
    "# all loss avg 0.03182978026930192 0.03499268176882896 all auc avg 0.9946172652665185 0.9922342808393372\n",
    "\n",
    "# change early stopping to 50, feat frac 0.6, new lstm fold 10, new gru v2\n",
    "# all loss avg 0.03156190981079377 0.03503282697860017 all auc avg 0.994716804833151 0.9922054864194375\n",
    "\n",
    "# change early stopping to 30, feat frac 0.45, new lstm fold 10, old gru v2\n",
    "# all loss avg 0.031911144927610886 0.034940109753291336 all auc avg 0.9945906725858359 0.992327389697206\n",
    "\n",
    "# add cnn word char， sub fail\n",
    "# all loss avg 0.03173121587236955 0.03490815640935038 all auc avg 0.9946490329906843 0.9923210856840503\n",
    "\n",
    "# new gru v2, add cnn word char, make train loss around 317 PUB 9871\n",
    "# all loss avg 0.03172599356966326 0.03489449783051101 all auc avg 0.9946648942774441 0.9923032825244114\n",
    "\n",
    "# add l1 lambda 1, feat frac 0.9, auc metrics, worse private score\n",
    "# all loss avg 0.05233130954882184 0.054452927786201144 all auc avg 0.9923918963501944 0.9919204708273053"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'feature_fraction': 0.4, 'application': 'binary', 'lambda_l2': 1.0, 'metric': 'binary_logloss', 'learning_rate': 0.05, 'data_random_seed': 2, 'max_depth': 3}\n",
      "0 fold: ls 0.06612442029795938 0.07766324888629762 auc 0.9918567731934822 0.9872249583688942\n",
      "1 fold: ls 0.0664403593273723 0.06995272624421472 auc 0.9917646961085009 0.9902594990495966\n",
      "2 fold: ls 0.06826129136259998 0.07485690710173401 auc 0.9911838306121467 0.9889605541874823\n",
      "3 fold: ls 0.06781354002150908 0.07239196703372557 auc 0.991349777954703 0.9894264239287804\n",
      "4 fold: ls 0.06625163494405484 0.07637490464784137 auc 0.9917910366573852 0.988441829644886\n",
      "5 fold: ls 0.06409771584762254 0.07517679427758434 auc 0.9924794324443418 0.9880465514424662\n",
      "6 fold: ls 0.06433231128901244 0.07663567644798093 auc 0.9923869038338159 0.9882604187084084\n",
      "7 fold: ls 0.06953701922119335 0.06908538269027603 auc 0.9907728711535466 0.9907141717012599\n",
      "8 fold: ls 0.06599396517309118 0.0736002103113236 auc 0.9918627330062167 0.9895389535363146\n",
      "9 fold: ls 0.06569371146317371 0.07565434671699109 auc 0.9919851923361235 0.9885097536443708\n",
      "toxic\n",
      "this class avg train 0.06645459689475888 avg val 0.07413921643579692\n",
      "this class auc train 0.9917433247300261 auc val 0.988938311421246\n",
      "========================\n",
      "{'feature_fraction': 0.6, 'application': 'binary', 'lambda_l2': 1.0, 'metric': 'binary_logloss', 'learning_rate': 0.075, 'data_random_seed': 2, 'max_depth': 3}\n",
      "0 fold: ls 0.01770577760503194 0.02092653597745139 auc 0.9944073932352452 0.9914308773262439\n",
      "1 fold: ls 0.017543581014226723 0.020511833689858552 auc 0.994526922452755 0.9915638055450057\n",
      "2 fold: ls 0.017934503853772345 0.02072646875904427 auc 0.9942622373103195 0.9913414672743384\n",
      "3 fold: ls 0.01791109582988108 0.01966914443927915 auc 0.9942530374767498 0.9927075420939361\n",
      "4 fold: ls 0.01792358514846317 0.020748038745711194 auc 0.9942451512049258 0.9915768609950627\n",
      "5 fold: ls 0.01592351166418456 0.019703365179709102 auc 0.9959233885915928 0.9925040268611345\n",
      "6 fold: ls 0.01792013129665702 0.01936783959055247 auc 0.9942618676316082 0.9927006282141781\n",
      "7 fold: ls 0.017349092281307554 0.01999129667477023 auc 0.9947390481977277 0.992348280443345\n",
      "8 fold: ls 0.017845094386667044 0.020738015407830927 auc 0.9943342267890525 0.9914492959613778\n",
      "9 fold: ls 0.01806307990611594 0.018423584528764386 auc 0.9941311475210707 0.993720645150759\n",
      "severe_toxic\n",
      "this class avg train 0.01761194529863074 avg val 0.020080612299297168\n",
      "this class auc train 0.9945084420411046 auc val 0.9921343429865381\n",
      "========================\n",
      "{'feature_fraction': 0.6, 'application': 'binary', 'lambda_l2': 1.0, 'metric': 'binary_logloss', 'learning_rate': 0.095, 'data_random_seed': 2, 'max_depth': 3}\n",
      "0 fold: ls 0.03561912461322093 0.03718207681527333 auc 0.9961528421419988 0.9956749489154093\n",
      "1 fold: ls 0.03533559719435012 0.036559706651673934 auc 0.9962188812821431 0.9957112826959977\n",
      "2 fold: ls 0.03492394861607481 0.038573970333671805 auc 0.9962969065074392 0.9954584467533931\n",
      "3 fold: ls 0.03241641877876477 0.033959987483013064 auc 0.9968677806707362 0.9964338070611232\n",
      "4 fold: ls 0.031194225668474754 0.04182876633927596 auc 0.9971143577917906 0.994533831807318\n",
      "5 fold: ls 0.035628016908692724 0.03740741165920322 auc 0.9961522752933699 0.99556843419235\n",
      "6 fold: ls 0.035688397007189565 0.0375042385139864 auc 0.9961440995670515 0.9956268931622192\n",
      "7 fold: ls 0.03470880636901714 0.03816199268673697 auc 0.9963442634509383 0.9955375406041204\n",
      "8 fold: ls 0.033162129893635865 0.03831098690026757 auc 0.9967040292508739 0.9954303331965506\n",
      "9 fold: ls 0.034860292186597835 0.038360666308941575 auc 0.9963163912465152 0.9954451470097521\n",
      "obscene\n",
      "this class avg train 0.03435369572360185 avg val 0.037784980369204385\n",
      "this class auc train 0.9964311827202856 auc val 0.9955420665398235\n",
      "========================\n",
      "{'feature_fraction': 0.6, 'application': 'binary', 'lambda_l2': 1.0, 'metric': 'binary_logloss', 'learning_rate': 0.05, 'data_random_seed': 2, 'max_depth': 3}\n",
      "0 fold: ls 0.0049249933565051005 0.00684683600270485 auc 0.9986035596989314 0.9963204483553321\n",
      "1 fold: ls 0.004315473471404687 0.00616157879803527 auc 0.9990933378637745 0.9971074271946365\n",
      "2 fold: ls 0.005033790304726797 0.006948272418315498 auc 0.9985407355589342 0.9952519379844962\n",
      "3 fold: ls 0.004042659402566583 0.006517485296541158 auc 0.9992598968443921 0.9934444863494458\n",
      "4 fold: ls 0.00454285932332657 0.00666830258061574 auc 0.9989236487381986 0.996647599891047\n",
      "5 fold: ls 0.004884156891887143 0.006816697128818138 auc 0.9986589056332135 0.9967405766128187\n",
      "6 fold: ls 0.004908633203203538 0.006224748590782825 auc 0.9986181709272441 0.995419259538626\n",
      "7 fold: ls 0.004968953900412336 0.007284838542519944 auc 0.9985198505354633 0.9957964045508831\n",
      "8 fold: ls 0.004162501282243606 0.007024929626897512 auc 0.9991741031091629 0.9956334096985113\n",
      "9 fold: ls 0.004645103763346125 0.007209573454188703 auc 0.998843885329577 0.9942920038570434\n",
      "threat\n",
      "this class avg train 0.004642912489962249 avg val 0.006770326243941964\n",
      "this class auc train 0.9988236094238893 auc val 0.995665355403284\n",
      "========================\n",
      "{'feature_fraction': 0.4, 'application': 'binary', 'lambda_l2': 1.0, 'metric': 'binary_logloss', 'learning_rate': 0.075, 'data_random_seed': 2, 'max_depth': 3}\n",
      "0 fold: ls 0.050598327160622264 0.04970293179682008 auc 0.9914680450521929 0.9915571074355276\n",
      "1 fold: ls 0.05126370851348022 0.05356590535525401 auc 0.9912015312753288 0.9896591589732606\n",
      "2 fold: ls 0.051088064952747404 0.05493818243580072 auc 0.9912792007861275 0.9890286147853933\n",
      "3 fold: ls 0.05050888318009908 0.05127458446610624 auc 0.9914831971679455 0.9907789552583411\n",
      "4 fold: ls 0.05100923306936148 0.05509066367706875 auc 0.9913031005659619 0.9890705998374323\n",
      "5 fold: ls 0.04602282850171747 0.05186603931801244 auc 0.9931534295598519 0.9907806061855381\n",
      "6 fold: ls 0.0474609544714756 0.05513097367483142 auc 0.9926167575262584 0.989646597572594\n",
      "7 fold: ls 0.05004692010098924 0.05518353325373208 auc 0.9916568724658427 0.9896201232316663\n",
      "8 fold: ls 0.0487585488756391 0.05206344792493019 auc 0.9921503007994372 0.9905310796118916\n",
      "9 fold: ls 0.04994359582798501 0.05433694204280859 auc 0.9917039383509015 0.9895789940746371\n",
      "insult\n",
      "this class avg train 0.04967010646541169 avg val 0.05331532039453646\n",
      "this class auc train 0.9918016373549848 auc val 0.9900251836966282\n",
      "========================\n",
      "{'feature_fraction': 0.4, 'application': 'binary', 'lambda_l2': 1.0, 'metric': 'binary_logloss', 'learning_rate': 0.095, 'data_random_seed': 2, 'max_depth': 3}\n",
      "0 fold: ls 0.01411776509000187 0.017249667236072064 auc 0.9960175766086043 0.9925239788233955\n",
      "1 fold: ls 0.014998396810100558 0.016908514328402787 auc 0.9953448839992206 0.9927665582905905\n",
      "2 fold: ls 0.014740754907529638 0.017400433874796443 auc 0.9955433399068843 0.9928840367016905\n",
      "3 fold: ls 0.014315043760010264 0.018001263292881407 auc 0.9958951506330803 0.9892995999904941\n",
      "4 fold: ls 0.01492528780403748 0.018476026334477893 auc 0.9953182540708693 0.9889870715456974\n",
      "5 fold: ls 0.015141241066840134 0.016174295823192575 auc 0.9951950582292362 0.9939400644875767\n",
      "6 fold: ls 0.014051442919815858 0.016319675268488067 auc 0.9960566271132083 0.9912466579955199\n",
      "7 fold: ls 0.015131316932512759 0.018386307497301045 auc 0.9951790651278443 0.9892261001517451\n",
      "8 fold: ls 0.014136950810409523 0.016734915020176745 auc 0.9960030321078565 0.9928679817905918\n",
      "9 fold: ls 0.014245825317092703 0.016303288439774275 auc 0.9959507976637221 0.9927496567671075\n",
      "identity_hate\n",
      "this class avg train 0.014580402541835078 avg val 0.017195438711556328\n",
      "this class auc train 0.9956503785460527 auc val 0.9916491706544409\n",
      "========================\n",
      "all loss avg 0.031218943235700083 0.03488098240905553\n",
      "all auc avg 0.9948264291360571 0.9923257384503268\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999405      0.394536  0.983376  0.153944  0.932097   \n",
      "1  0000247867823ef7  0.000244      0.000030  0.000050  0.000024  0.000083   \n",
      "2  00013b17ad220c46  0.000339      0.000034  0.000065  0.000029  0.000149   \n",
      "3  00017563c3f7919a  0.000062      0.000030  0.000047  0.000053  0.000081   \n",
      "4  00017695ad8997eb  0.000775      0.000034  0.000076  0.000091  0.000185   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.419083  \n",
      "1       0.000033  \n",
      "2       0.000040  \n",
      "3       0.000033  \n",
      "4       0.000037  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "def special_ens(model_name,k=3,rnd=233):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    params_list = [\n",
    "        [0.05, 3,0.4], # depth should be 3\n",
    "        [0.075,3,0.6],\n",
    "        [0.095,3,0.6],\n",
    "        [0.05, 3,0.6],\n",
    "        [0.075,3,0.4],\n",
    "        [0.095,3,0.4],\n",
    "    ]\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        \n",
    "        # special params\n",
    "        params = {\n",
    "                'application': 'binary',\n",
    "                #'num_leaves': 8,\n",
    "                #'lambda_l1': 1,\n",
    "                'lambda_l2': 1.0,\n",
    "                'max_depth': params_list[i][1],\n",
    "                'metric': 'binary_logloss', # or auc\n",
    "                'data_random_seed': 2,\n",
    "                'learning_rate':params_list[i][0],\n",
    "                'feature_fraction': params_list[i][2],\n",
    "\n",
    "                }\n",
    "        print(params)\n",
    "            \n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "            s_round = 50\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i])\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred\n",
    "\n",
    "print('done')\n",
    "\n",
    "lgb_res = special_ens('lgb',10,666)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_log_csv_fold10_stratified_special.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# best params changed when base models changed\n",
    "# all loss avg 0.03111512578966158 0.03534320053008906 all auc avg 0.9948524214184179 0.9918450388634261\n",
    "\n",
    "# change lr, ridge, mnb fold to 10, train loss too low ?\n",
    "# all loss avg 0.030612393454975732 0.035095753229703264 all auc avg 0.9950485697696142 0.9921210772750391\n",
    "\n",
    "# tilli feat fold 10\n",
    "# all loss avg 0.03172720686364922 0.0350882288604753 all auc avg 0.9946451061433877 0.9921962852188195\n",
    "\n",
    "# update pool gru v2\n",
    "# all loss avg 0.03178018332247421 0.035037128263355345 all auc avg 0.9946071803186727 0.9922390115824994\n",
    "\n",
    "# update lstm, s_round = 50 , new gru v2\n",
    "# all loss avg 0.03141437460591465 0.035002079150415574 all auc avg 0.9947934528849732 0.9921988821755023\n",
    "\n",
    "# change to 9865 10fold gru v2, PUB 9870\n",
    "# all loss avg 0.03142825818869454 0.03493827055369443 all auc avg 0.9947738168751431 0.9922685221839775\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.095 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.06846237090011284 0.07764328091583284 auc 0.9911300108050973 0.9871456372956724\n",
      "1 fold: ls 0.06723213171044759 0.06989098010442507 auc 0.9915405244061806 0.9902482192396412\n",
      "2 fold: ls 0.06874061258786318 0.07475439565814274 auc 0.9910031482726716 0.9889943483169075\n",
      "3 fold: ls 0.06769464391444865 0.07237637236902984 auc 0.9913613337508438 0.9894165484325141\n",
      "4 fold: ls 0.0672207378112683 0.0764113723781736 auc 0.9914930997122849 0.988422020404696\n",
      "5 fold: ls 0.06911278596677176 0.07570570445992433 auc 0.9909296947299122 0.9878458298965587\n",
      "6 fold: ls 0.06606093078530936 0.07677310545686032 auc 0.991857284217556 0.9881428325092025\n",
      "7 fold: ls 0.0713609970818049 0.06920464332797222 auc 0.9901563076566734 0.990652337201299\n",
      "8 fold: ls 0.06743963081989617 0.07330889354691171 auc 0.9913926944401048 0.9895555454915826\n",
      "9 fold: ls 0.07095720160435365 0.07622636321176418 auc 0.9902891628987092 0.9882017598080557\n",
      "toxic 0.095 0.4 3\n",
      "this class avg train 0.06842820431822765 avg val 0.07422951114290369\n",
      "this class auc train 0.9911153260890032 auc val 0.9888625078596128\n",
      "========================\n",
      "0 fold: ls 0.017929829830703115 0.020978518532784186 auc 0.9942523929492514 0.9914364160020256\n",
      "1 fold: ls 0.016649938192686908 0.020184298584551712 auc 0.9952986252743102 0.992110156348905\n",
      "2 fold: ls 0.01739448710730529 0.020837014842046787 auc 0.9946916372140473 0.9914142612988985\n",
      "3 fold: ls 0.01746871136677717 0.019450919868185233 auc 0.994624743592012 0.9928772629446765\n",
      "4 fold: ls 0.01800551964120785 0.02064283575384671 auc 0.9941988873641878 0.9916900082288899\n",
      "5 fold: ls 0.016403109496124585 0.019882376045534896 auc 0.9955070492866821 0.9923001956302088\n",
      "6 fold: ls 0.018071666606063 0.019093351787926403 auc 0.9941287769364292 0.9929757381685799\n",
      "7 fold: ls 0.017712565606902975 0.01987354203106189 auc 0.9944313815761414 0.9924601558372479\n",
      "8 fold: ls 0.01718514803805481 0.020608663814316443 auc 0.9948754768132234 0.9916551307608363\n",
      "9 fold: ls 0.01806678233999739 0.018427526766767933 auc 0.9941323058749298 0.9937071086262299\n",
      "severe_toxic 0.095 0.4 3\n",
      "this class avg train 0.017488775822582313 avg val 0.019997904802702218\n",
      "this class auc train 0.9946141276881215 auc val 0.99226264338465\n",
      "========================\n",
      "0 fold: ls 0.03531264889016257 0.03701884984588255 auc 0.9962142371685812 0.9957376716702616\n",
      "1 fold: ls 0.03564799851949711 0.036417510421016604 auc 0.9961477223041243 0.9957529412547763\n",
      "2 fold: ls 0.03528093380119962 0.038519847132704156 auc 0.9962198295828806 0.9954507723005503\n",
      "3 fold: ls 0.03499238211410079 0.033967872386460755 auc 0.9962826276311567 0.9964406984065329\n",
      "4 fold: ls 0.03361562691665222 0.04179716682678142 auc 0.9965885663944396 0.9945276452585976\n",
      "5 fold: ls 0.032518600063716915 0.037551651598561255 auc 0.9968339145700562 0.9955158485282278\n",
      "6 fold: ls 0.0334587222907468 0.03736121234449818 auc 0.9966320058930593 0.9956031650070011\n",
      "7 fold: ls 0.03534802946426822 0.03812186434195047 auc 0.9962057605265511 0.9955658499378213\n",
      "8 fold: ls 0.03412022462650869 0.038190447368661666 auc 0.9964923412236336 0.9954763016028644\n",
      "9 fold: ls 0.034835950542007305 0.038169442629467365 auc 0.9963238707793124 0.9954999510761983\n",
      "obscene 0.095 0.4 3\n",
      "this class avg train 0.03451311172288603 avg val 0.03771158648959844\n",
      "this class auc train 0.9963940876073796 auc val 0.9955570845042832\n",
      "========================\n",
      "0 fold: ls 0.004762673534945639 0.0068833655993560915 auc 0.9987482436283768 0.9962589042530904\n",
      "1 fold: ls 0.004723749743497208 0.006204342922281058 auc 0.9987848287822918 0.9971493295621203\n",
      "2 fold: ls 0.004419313317558806 0.0069904186724989 auc 0.9990013430527757 0.9958621412109784\n",
      "3 fold: ls 0.004053739151915708 0.006296483676031915 auc 0.9992357451267779 0.9948050893624154\n",
      "4 fold: ls 0.004327751257253696 0.006391756199898984 auc 0.999049897091269 0.9971190311563686\n",
      "5 fold: ls 0.004903041473124707 0.006805501381097515 auc 0.9986126567993826 0.9967693863012551\n",
      "6 fold: ls 0.004810797786359087 0.006307335682230776 auc 0.9986751069367241 0.995246401408008\n",
      "7 fold: ls 0.004882344949307444 0.007348569645835753 auc 0.9985235130857808 0.9958500953338782\n",
      "8 fold: ls 0.004112513284285876 0.0068757531942240145 auc 0.9991744596020916 0.9955451417169192\n",
      "9 fold: ls 0.003983508037470324 0.007160804181431768 auc 0.9992739049250212 0.994769453393837\n",
      "threat 0.095 0.4 3\n",
      "this class avg train 0.004497943253571849 avg val 0.006726433115488678\n",
      "this class auc train 0.9989079699030491 auc val 0.9959374973698873\n",
      "========================\n",
      "0 fold: ls 0.048765961871414056 0.04974145255349548 auc 0.9921544200353671 0.9915173716492275\n",
      "1 fold: ls 0.05115088645719552 0.05361115754662766 auc 0.9912466797136565 0.989656189246074\n",
      "2 fold: ls 0.05122030828829476 0.0550268150323193 auc 0.9912294600617714 0.9887938390290749\n",
      "3 fold: ls 0.05085256325226622 0.05146955663478342 auc 0.9913581200145438 0.9907735177296896\n",
      "4 fold: ls 0.04930680819097914 0.05502123027908711 auc 0.9919471705507724 0.9891755928886491\n",
      "5 fold: ls 0.04530539281400865 0.05205106037051311 auc 0.9933996918711786 0.9907094953540366\n",
      "6 fold: ls 0.04976003190017729 0.05538607873477255 auc 0.991756074163042 0.989559926017964\n",
      "7 fold: ls 0.04740548260334298 0.05523328726517931 auc 0.9926290431264236 0.9896270758182921\n",
      "8 fold: ls 0.05010814517291331 0.05214616810659141 auc 0.9916324066356925 0.9905500945174833\n",
      "9 fold: ls 0.05088803984810269 0.05433852133407399 auc 0.9913468221513804 0.9895799573848323\n",
      "insult 0.095 0.4 3\n",
      "this class avg train 0.04947636203986946 avg val 0.053402532785744326\n",
      "this class auc train 0.9918699888323828 auc val 0.9899943059635323\n",
      "========================\n",
      "0 fold: ls 0.01411776509000187 0.017249667236072064 auc 0.9960175766086043 0.9925239788233955\n",
      "1 fold: ls 0.014998396810100558 0.016908514328402787 auc 0.9953448839992206 0.9927665582905905\n",
      "2 fold: ls 0.014740754907529638 0.017400433874796443 auc 0.9955433399068843 0.9928840367016905\n",
      "3 fold: ls 0.014315043760010264 0.018001263292881407 auc 0.9958951506330803 0.9892995999904941\n",
      "4 fold: ls 0.01492528780403748 0.018476026334477893 auc 0.9953182540708693 0.9889870715456974\n",
      "5 fold: ls 0.015141241066840134 0.016174295823192575 auc 0.9951950582292362 0.9939400644875767\n",
      "6 fold: ls 0.014051442919815858 0.016319675268488067 auc 0.9960566271132083 0.9912466579955199\n",
      "7 fold: ls 0.015131316932512759 0.018386307497301045 auc 0.9951790651278443 0.9892261001517451\n",
      "8 fold: ls 0.014136950810409523 0.016734915020176745 auc 0.9960030321078565 0.9928679817905918\n",
      "9 fold: ls 0.014245825317092703 0.016303288439774275 auc 0.9959507976637221 0.9927496567671075\n",
      "identity_hate 0.095 0.4 3\n",
      "this class avg train 0.014580402541835078 avg val 0.017195438711556328\n",
      "this class auc train 0.9956503785460527 auc val 0.9916491706544409\n",
      "========================\n",
      "all loss avg 0.03149746661649539 0.03487723450799894\n",
      "all auc avg 0.9947586464443315 0.9923772016227345\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.48\n",
      "0 fold: ls 0.06726052671189038 0.07783932427555669 auc 0.9914956065851186 0.9872346073629527\n",
      "1 fold: ls 0.06883711705351256 0.07022393981391263 auc 0.9910247698593792 0.9901905517774987\n",
      "2 fold: ls 0.06764898174452375 0.07451612752820501 auc 0.9913686534102207 0.9890896151455684\n",
      "3 fold: ls 0.06813202156032619 0.07281453787200237 auc 0.9912400171948546 0.9892602619090332\n",
      "4 fold: ls 0.067026465312176 0.07608154327896914 auc 0.9915498948446075 0.9885666233250765\n",
      "5 fold: ls 0.06596941931498093 0.07526013019788272 auc 0.99190539732707 0.9879423376136403\n",
      "6 fold: ls 0.06744178120886814 0.07685824566954885 auc 0.9914510620046062 0.988142016567959\n",
      "7 fold: ls 0.07003772518837202 0.06916988823027188 auc 0.9905902837678671 0.990683526450546\n",
      "8 fold: ls 0.06648765332723613 0.07346249139575697 auc 0.9916838924344934 0.9896221853119218\n",
      "9 fold: ls 0.06556533796807103 0.0758541944351547 auc 0.992017887581874 0.9884090685824845\n",
      "toxic 0.095 0.48 3\n",
      "this class avg train 0.06744070293899572 avg val 0.0742080422697261\n",
      "this class auc train 0.9914327465010091 auc val 0.988914079404668\n",
      "========================\n",
      "0 fold: ls 0.017881703802554348 0.020850164310979174 auc 0.9942883517020404 0.9915839821496392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold: ls 0.018158607121400026 0.020387808432662743 auc 0.9941049431926207 0.9918939501835675\n",
      "2 fold: ls 0.017503160401603785 0.020721186351445684 auc 0.994590823800739 0.9914941764780352\n",
      "3 fold: ls 0.016849853684874554 0.019565526594103582 auc 0.9951433505127277 0.992801303962527\n",
      "4 fold: ls 0.01800482937302796 0.02071777496890712 auc 0.9941975885064915 0.9916468856817318\n",
      "5 fold: ls 0.016245737839298183 0.01979017735839065 auc 0.9956336952333791 0.9924490879746739\n",
      "6 fold: ls 0.018088670450469812 0.019276437742634284 auc 0.9941415090330736 0.9928813806299501\n",
      "7 fold: ls 0.01779503119117521 0.019941951925378464 auc 0.994358593852259 0.9924127780013958\n",
      "8 fold: ls 0.017880622501309375 0.020834070690092528 auc 0.9942898346528969 0.9914636287520558\n",
      "9 fold: ls 0.018016054450055986 0.01835805718354905 auc 0.9941707666515791 0.9937481163328917\n",
      "severe_toxic 0.095 0.48 3\n",
      "this class avg train 0.017642427081576924 avg val 0.020044315555814325\n",
      "this class auc train 0.9944919457137807 auc val 0.9922375290146469\n",
      "========================\n",
      "0 fold: ls 0.03530488670871085 0.037064428878462854 auc 0.996217140343882 0.9957063494456162\n",
      "1 fold: ls 0.03560185057344416 0.03653747790748952 auc 0.9961546942759834 0.9957398642259867\n",
      "2 fold: ls 0.035125951688444006 0.03834027217262407 auc 0.9962532900638824 0.995503083877071\n",
      "3 fold: ls 0.03580712680423307 0.03419221656968923 auc 0.9961057735837338 0.9963712759326027\n",
      "4 fold: ls 0.0351431075124127 0.041975187096198636 auc 0.9962583852078519 0.9944672676755179\n",
      "5 fold: ls 0.03374237610959969 0.037390887341976695 auc 0.9965622692826568 0.9955874245475989\n",
      "6 fold: ls 0.035614461440742005 0.03749798851918853 auc 0.9961582169689924 0.9956048095326102\n",
      "7 fold: ls 0.03538815034543038 0.0381980585094954 auc 0.996199288499573 0.9955382062454383\n",
      "8 fold: ls 0.03525207417288788 0.038530873326913885 auc 0.9962438404307075 0.9953941536331486\n",
      "9 fold: ls 0.03544359111532854 0.038205532762811294 auc 0.9961954063110086 0.9955041064632106\n",
      "obscene 0.095 0.48 3\n",
      "this class avg train 0.03524235764712332 avg val 0.037793292308485014\n",
      "this class auc train 0.9962348304968272 auc val 0.9955416541578799\n",
      "========================\n",
      "0 fold: ls 0.004851237324370293 0.006932491101497643 auc 0.9986602281126983 0.9961476010894615\n",
      "1 fold: ls 0.004406980597186124 0.006166346423217054 auc 0.9990152868284188 0.9971637335009429\n",
      "2 fold: ls 0.004720884222794598 0.006931670961420625 auc 0.9987946633264407 0.9958686884558977\n",
      "3 fold: ls 0.0049999176507131236 0.006562583257400492 auc 0.9986101393035991 0.9935918086198587\n",
      "4 fold: ls 0.0043777059592495435 0.006609824019313685 auc 0.9990310077519379 0.9967981959896914\n",
      "5 fold: ls 0.004876780820177928 0.00682601578899627 auc 0.9986330728780921 0.9966436712971694\n",
      "6 fold: ls 0.004608035470564796 0.006276158392006535 auc 0.9988180113667371 0.9955829509501959\n",
      "7 fold: ls 0.005004366615981053 0.0072426365123007 auc 0.9984501565070283 0.996074025184906\n",
      "8 fold: ls 0.0042766656927110454 0.0070903915461065625 auc 0.9990564847489732 0.9952990612833897\n",
      "9 fold: ls 0.00467031619880304 0.007189679623610296 auc 0.9987820500106429 0.9944966250870978\n",
      "threat 0.095 0.48 3\n",
      "this class avg train 0.004679289055255155 avg val 0.006782779762586987\n",
      "this class auc train 0.9987851100834568 auc val 0.9957666361458613\n",
      "========================\n",
      "0 fold: ls 0.05009630598474876 0.0498665869076729 auc 0.9916537420343702 0.9914654641641766\n",
      "1 fold: ls 0.05029159880663574 0.05352557270208937 auc 0.991570930031151 0.989697054365248\n",
      "2 fold: ls 0.05054889422670089 0.05490259225557128 auc 0.9914806553694012 0.9892192210782034\n",
      "3 fold: ls 0.049492181621905225 0.0515122237573741 auc 0.9918590707939209 0.9904092033100329\n",
      "4 fold: ls 0.050546656267924234 0.05529820341266425 auc 0.991479895825253 0.9890369685971222\n",
      "5 fold: ls 0.04761881301050263 0.05222081699208552 auc 0.9925759418101849 0.9906146251388334\n",
      "6 fold: ls 0.049468436441173526 0.0553542993256079 auc 0.9918631879721941 0.9895540698318405\n",
      "7 fold: ls 0.04899565941172917 0.05516913365760241 auc 0.9920514183275225 0.9896393056694658\n",
      "8 fold: ls 0.05047524537426412 0.05232631631407952 auc 0.9914881577251593 0.9903839025672886\n",
      "9 fold: ls 0.04996258470892264 0.054242549038406355 auc 0.9916969256807278 0.9896835341723402\n",
      "insult 0.095 0.48 3\n",
      "this class avg train 0.04974963758545069 avg val 0.053441829436315355\n",
      "this class auc train 0.9917719925569883 auc val 0.9899703348894551\n",
      "========================\n",
      "0 fold: ls 0.014382851615896905 0.017381378564248404 auc 0.9958010496976987 0.9922849864832568\n",
      "1 fold: ls 0.015117918523739957 0.016923792247453013 auc 0.9952607925600221 0.992784942316755\n",
      "2 fold: ls 0.014515667758176446 0.017321749273031848 auc 0.9957085049305189 0.993071464090392\n",
      "3 fold: ls 0.014117305142325155 0.01797622389483803 auc 0.9960275861047875 0.9889942457998105\n",
      "4 fold: ls 0.014380166657937905 0.018427470456709193 auc 0.9958078106927607 0.9891099306473823\n",
      "5 fold: ls 0.014684838802131622 0.016145369747340665 auc 0.9955403100052181 0.9938655515313541\n",
      "6 fold: ls 0.014652742825663036 0.01616878236413841 auc 0.9955550242887944 0.9925717176096539\n",
      "7 fold: ls 0.015015957509975406 0.018458019049824744 auc 0.9953018904859285 0.9887304447575691\n",
      "8 fold: ls 0.01317058177213816 0.016647117194672215 auc 0.9967296995242201 0.9930567598814944\n",
      "9 fold: ls 0.014956693042331113 0.016318311421059804 auc 0.9953833103565087 0.9926250090324445\n",
      "identity_hate 0.095 0.48 3\n",
      "this class avg train 0.014499472365031569 avg val 0.01717682142133163\n",
      "this class auc train 0.9957115978646458 auc val 0.9917095052150113\n",
      "========================\n",
      "all loss avg 0.03154231444557223 0.034907846792376566\n",
      "all auc avg 0.9947380372027846 0.9923566231379204\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.56\n",
      "0 fold: ls 0.06920521128992073 0.07788555936715442 auc 0.990898389354667 0.9871810622409949\n",
      "1 fold: ls 0.06658467964632457 0.07030283448086072 auc 0.9917063388623965 0.9900685576883004\n",
      "2 fold: ls 0.06753953824526772 0.07452923527931803 auc 0.9913849377219858 0.9890491165507882\n",
      "3 fold: ls 0.06720597904457669 0.07251417617343396 auc 0.9915365990835723 0.9893839321145703\n",
      "4 fold: ls 0.06647544790042313 0.07611611501338243 auc 0.9917171202941062 0.9885105953596877\n",
      "5 fold: ls 0.06872659402685127 0.0757017342125408 auc 0.9910472125496621 0.9879138250001859\n",
      "6 fold: ls 0.06848794241798394 0.0768602764195773 auc 0.9910802778640054 0.9882155419400146\n",
      "7 fold: ls 0.06948716005618646 0.06898329930904028 auc 0.9907935298224041 0.9907733768749759\n",
      "8 fold: ls 0.0682574692021757 0.0734761240816086 auc 0.9911301019184309 0.9895865987411965\n",
      "9 fold: ls 0.06738249977841487 0.07600608681973103 auc 0.9914535752714755 0.9883328634546001\n",
      "toxic 0.095 0.56 3\n",
      "this class avg train 0.0679352521608125 avg val 0.07423754411566477\n",
      "this class auc train 0.9912748082742706 auc val 0.9889015469965315\n",
      "========================\n",
      "0 fold: ls 0.017970090560970133 0.020799045815986468 auc 0.9942075284438805 0.9916267090770985\n",
      "1 fold: ls 0.01725461115886156 0.02035834522195941 auc 0.9947851549681821 0.9918043423218128\n",
      "2 fold: ls 0.018001107951746143 0.020908070465440052 auc 0.9942067270731889 0.9910985567793392\n"
     ]
    }
   ],
   "source": [
    "# find best params for each column, early stopping = 30\n",
    "\n",
    "best_pred = np.zeros((153164,6))\n",
    "val_ls_res = [0,0,0,0,0,0]\n",
    "for lr in [0.095,0.075,0.05]:\n",
    "    for max_d in [3]:\n",
    "        for s_rate in [0.4,0.48,0.56,0.64]:\n",
    "            print('learning rate',lr,'max depth',max_d,'feature fraction',s_rate)\n",
    "            lgb_res,tmp_ls_res = simple_ens('lgb',k=10,rnd=666,lr=lr,\n",
    "                                 feature_fraction=s_rate,bagging_fraction=0.9,\n",
    "                                 bag_frec=3,met='binary_logloss',max_d=max_d)\n",
    "            # check for each cls\n",
    "            for i in range(6):\n",
    "                # find better params for this class\n",
    "                if tmp_ls_res[i] < val_ls_res[i]:\n",
    "                    val_ls_res[i] = tmp_ls_res[i]\n",
    "                    best_pred[:,i] = lgb_res[:,i]\n",
    "                    print('FIND BETTER PARAMS',lr,max_d,s_rate,list_classes[i])\n",
    "            print('TEST PARAM DONE ------------------------------------------')\n",
    "\n",
    "print(val_auc_res)\n",
    "print(np.mean(val_auc_res))\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = best_pred\n",
    "sample_submission.to_csv(\"../results/lgb_grid_search_fold10_stratified.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "                \n",
    "            \n",
    "# best auc params\n",
    "# toxic 0.05,4,0.5\n",
    "# severe toxic 0.075 3 0.5\n",
    "# obs 0.075 3 0.6\n",
    "# threat 0.095 3 0.5\n",
    "# insult 0.075 0.5 4\n",
    "# hate 0.05 0.5 3\n",
    "\n",
    "# TEST PARAM DONE ------------------------------------------\n",
    "# [0.9884873039634368, 0.9920148292218363, 0.9954573112511824, \n",
    "#  0.994769628570376, 0.9898761527824232, 0.9912880359235366]\n",
    "# 0.9919822102854652 PUB 9870\n",
    "\n",
    "# updated pool gru v2 10 fold\n",
    "# TEST PARAM DONE ------------------------------------------\n",
    "# [0.9887825645527965, 0.9921426168741385, 0.9955318446611215, \n",
    "#  0.9955039709421902, 0.9900536177079309, 0.9915995552784374]\n",
    "# 0.9922690283361025 PUB 9869\n",
    "\n",
    "# change to val loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
