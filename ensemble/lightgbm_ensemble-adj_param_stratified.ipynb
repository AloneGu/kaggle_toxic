{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping\n",
      "sleep done =======================\n",
      "file path ../features/fasttext_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_adj2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/pool_gru_fasttext_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj1_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 313)\n",
      "(159571, 313)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(24400)\n",
    "# print('sleep done =======================')\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat or 'tfidf' in feat:\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.hstack(train_x)\n",
    "test_x = np.hstack(test_x)\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,feature_fraction=0.9,bagging_fraction=0.9,\n",
    "               bag_frec=3,met='binary_logloss',max_d=3):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    cls_auc_res = [0,0,0,0,0,0]\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "\n",
    "            # share params\n",
    "            params = {\n",
    "                    'application': 'binary',\n",
    "                    #'num_leaves': 8,\n",
    "                    #'lambda_l1': 1,\n",
    "                    'lambda_l2': 1.0,\n",
    "                    'max_depth': max_d,\n",
    "                    'metric': met, # or auc\n",
    "                    'data_random_seed': 2,\n",
    "                    'learning_rate':lr,\n",
    "                    # 'bagging_fraction': bagging_fraction,\n",
    "                    # 'bagging_freq':bag_frec,\n",
    "                    'feature_fraction': feature_fraction,\n",
    "\n",
    "                    }\n",
    "            if met == 'auc':\n",
    "                s_round = 60\n",
    "            else:\n",
    "                s_round = 30\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i], lr, feature_fraction, max_d)\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        cls_auc_res[i] = val_auc_l\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred, cls_auc_res\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.06960238770416617 0.0781605646273722 auc 0.9907893554131592 0.9871128397759622\n",
      "1 fold: ls 0.06965586749041318 0.07066149030897897 auc 0.9907712887458717 0.9900424193335037\n",
      "2 fold: ls 0.07141288345672146 0.07499926491809923 auc 0.9901234160818309 0.9888034975564943\n",
      "3 fold: ls 0.06657558553422858 0.07283725191739757 auc 0.9917452868695367 0.9892583592904864\n",
      "4 fold: ls 0.06970514972282499 0.07728551992825755 auc 0.99067586389295 0.9880644114896857\n",
      "5 fold: ls 0.06693818737383217 0.0751218141768801 auc 0.9916379375382057 0.9882031215010854\n",
      "6 fold: ls 0.06974700654827182 0.07725293056797954 auc 0.9906617045532646 0.9880672219539689\n",
      "7 fold: ls 0.07197855625583804 0.07002825637527361 auc 0.9899108479831085 0.9904254444796684\n",
      "8 fold: ls 0.0680760847028484 0.073680745182608 auc 0.991238367889631 0.9895219989153576\n",
      "9 fold: ls 0.0674869994506814 0.07622478882141936 auc 0.9914183749698059 0.9882837222537514\n",
      "toxic 0.05 0.45 3\n",
      "this class avg train 0.06911787082398262 avg val 0.07462526268242661\n",
      "this class auc train 0.9908972443937365 auc val 0.9887783036549964\n",
      "========================\n",
      "0 fold: ls 0.017987346888853478 0.0208894518142621 auc 0.9941943952770985 0.991499715153817\n",
      "1 fold: ls 0.01732651399758421 0.020675681379565554 auc 0.9947515121031725 0.9916116755285479\n",
      "2 fold: ls 0.018031773551823026 0.020756386964467074 auc 0.9941869525774313 0.9913790511457147\n",
      "3 fold: ls 0.017813870567409563 0.019562596811564616 auc 0.9943366863630676 0.9927641157108495\n",
      "4 fold: ls 0.017999989679332792 0.020954259006361966 auc 0.9941945790777159 0.9913758861881251\n",
      "5 fold: ls 0.018003144576727192 0.019831581074355398 auc 0.9941891492405593 0.9924216185314438\n",
      "6 fold: ls 0.01816665872272521 0.019148299043951735 auc 0.9940861431471462 0.9929677755070923\n",
      "7 fold: ls 0.01793293762550574 0.020397097630104832 auc 0.9942495322650381 0.9919541286997015\n",
      "8 fold: ls 0.018001148953483637 0.020756400952782184 auc 0.9941997965176347 0.9914855260711473\n",
      "9 fold: ls 0.018168249630771557 0.018795093021345438 auc 0.9940602944685385 0.9933460019277601\n",
      "severe_toxic 0.05 0.45 3\n",
      "this class avg train 0.01794316341942164 avg val 0.02017668476987609\n",
      "this class auc train 0.9942449041037401 auc val 0.9920805494464199\n",
      "========================\n",
      "0 fold: ls 0.03546660993797372 0.03677708365427342 auc 0.9962001370673906 0.9958018039252229\n",
      "1 fold: ls 0.035825538902254234 0.03667182058450849 auc 0.9961239003124888 0.9956999283895639\n",
      "2 fold: ls 0.03432863515892918 0.03891699262571719 auc 0.9964395270275367 0.9953614980531948\n",
      "3 fold: ls 0.03563514259123 0.03366700835860073 auc 0.9961480087931771 0.9964922660309923\n",
      "4 fold: ls 0.03514487297991607 0.042049304817378896 auc 0.9962732079722609 0.9945296813379234\n",
      "5 fold: ls 0.03442872954883285 0.03739362447087665 auc 0.9964176719105153 0.9956128755391694\n",
      "6 fold: ls 0.035756368829304094 0.03765403436837075 auc 0.9961370681813514 0.9953853045191564\n",
      "7 fold: ls 0.03522239034061242 0.03798185046808135 auc 0.9962473135794196 0.9955828825244878\n",
      "8 fold: ls 0.035051696580429996 0.0387120313238479 auc 0.9962966981378546 0.9952589109794795\n",
      "9 fold: ls 0.03521448738882167 0.038670444237857066 auc 0.9962521817318128 0.9953347548415747\n",
      "obscene 0.05 0.45 3\n",
      "this class avg train 0.03520744722583043 avg val 0.03784941949095124\n",
      "this class auc train 0.9962535714713809 auc val 0.9955059906140764\n",
      "========================\n",
      "0 fold: ls 0.005026954380770551 0.006708523174214938 auc 0.9984916765323413 0.9968782736224595\n",
      "1 fold: ls 0.0048936064463851025 0.006251861075130591 auc 0.9986787440174543 0.9968298240100566\n",
      "2 fold: ls 0.004555700582547964 0.007033539915598485 auc 0.9989183788058508 0.9944662685941755\n",
      "3 fold: ls 0.004577237876584939 0.00645182541116019 auc 0.9989042558997107 0.9930476983678003\n",
      "4 fold: ls 0.004372817905140133 0.006601461295906563 auc 0.9990491662053964 0.9967314098937708\n",
      "5 fold: ls 0.004916475194918716 0.0068033726180631035 auc 0.9986241966752164 0.9963542648815137\n",
      "6 fold: ls 0.004875262156339776 0.006184043707302638 auc 0.9986309208252448 0.9955842604814884\n",
      "7 fold: ls 0.005107530536504377 0.007332803328671676 auc 0.9983459240606329 0.9959797389318416\n",
      "8 fold: ls 0.005041162558673279 0.007045911917249528 auc 0.998483867981983 0.9944551658836226\n",
      "9 fold: ls 0.004594896450804474 0.007304062108877982 auc 0.9988653721306533 0.9941850123642043\n",
      "threat 0.05 0.45 3\n",
      "this class avg train 0.00479616440886693 avg val 0.0067717404552175686\n",
      "this class auc train 0.9986992503134484 auc val 0.9954511917030932\n",
      "========================\n",
      "0 fold: ls 0.049343552902700046 0.049787348021110865 auc 0.9919487456532095 0.991491104203126\n",
      "1 fold: ls 0.051056192441345044 0.053282837056848824 auc 0.9912843970076228 0.989697932735261\n",
      "2 fold: ls 0.051225604984930895 0.05509250240041487 auc 0.9912539992668626 0.9887495022569925\n",
      "3 fold: ls 0.05026100987643154 0.0513817379918849 auc 0.991581412367017 0.9907705061753596\n",
      "4 fold: ls 0.04896399469082867 0.055308563831465825 auc 0.9920766241256916 0.9890156353476718\n",
      "5 fold: ls 0.047578863927422506 0.05231915143230791 auc 0.9926064209545579 0.9905923716315636\n",
      "6 fold: ls 0.05002391541471699 0.05581932618655993 auc 0.9916470602184086 0.9893900966203784\n",
      "7 fold: ls 0.05061762785330415 0.05514777326018388 auc 0.9914354092837815 0.989662676412462\n",
      "8 fold: ls 0.0501075071406736 0.052309035895404254 auc 0.9916387870498049 0.9904407797518563\n",
      "9 fold: ls 0.050895238868401464 0.05451825489892317 auc 0.9913454724086606 0.9894613026986171\n",
      "insult 0.05 0.45 3\n",
      "this class avg train 0.05000735081007549 avg val 0.05349665309751045\n",
      "this class auc train 0.9916818328335616 auc val 0.9899271907833288\n",
      "========================\n",
      "0 fold: ls 0.014344278481195627 0.016965033099169285 auc 0.9958536064998388 0.9931176483512443\n",
      "1 fold: ls 0.01458246542721523 0.01755891384373002 auc 0.995679988148223 0.9918729152626428\n",
      "2 fold: ls 0.01513798332470182 0.01735028368491172 auc 0.9951977066126745 0.9929203563631374\n",
      "3 fold: ls 0.015418292035814343 0.01823698072523437 auc 0.9949974805636814 0.9886467428662132\n",
      "4 fold: ls 0.014981015845049928 0.018624797733989416 auc 0.9953717945936226 0.9904255094953496\n",
      "5 fold: ls 0.01500685113588815 0.016352529701609074 auc 0.9953155159463476 0.9936451738184052\n",
      "6 fold: ls 0.01503539287168468 0.016418319034395116 auc 0.9952836756255458 0.9922908085844353\n",
      "7 fold: ls 0.015243012312602645 0.018504550526959252 auc 0.9951716070310472 0.9876621323795072\n",
      "8 fold: ls 0.01361340959263402 0.016702207855076995 auc 0.9964290710282372 0.9927555278560589\n",
      "9 fold: ls 0.014881093743440764 0.016561971710651536 auc 0.9954606568734026 0.9922664209841752\n",
      "identity_hate 0.05 0.45 3\n",
      "this class avg train 0.01482437947702272 avg val 0.017327558791572682\n",
      "this class auc train 0.9954761102922621 auc val 0.9915603235961168\n",
      "========================\n",
      "all loss avg 0.031982729360866644 0.035041219881259106\n",
      "all auc avg 0.9945421522346883 0.9922172582996719\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999080      0.404933  0.982493  0.144690  0.924196   \n",
      "1  0000247867823ef7  0.000196      0.000038  0.000077  0.000029  0.000093   \n",
      "2  00013b17ad220c46  0.000370      0.000040  0.000137  0.000030  0.000193   \n",
      "3  00017563c3f7919a  0.000064      0.000038  0.000069  0.000056  0.000087   \n",
      "4  00017695ad8997eb  0.001307      0.000045  0.000188  0.000080  0.000236   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.398105  \n",
      "1       0.000045  \n",
      "2       0.000063  \n",
      "3       0.000046  \n",
      "4       0.000062  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "lgb_res,tmp_auc_res = simple_ens('lgb',10,666,0.05,0.45)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_log_csv_fold10_stratified.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# add 7 base fasttext NN models\n",
    "# all loss avg 0.0319116484218 0.0358161833583 all auc avg 0.994546891137 0.991249510282 PUB 9866\n",
    "\n",
    "# rm tfidf test\n",
    "# all loss avg 0.0319555637279 0.0357756335619 all auc avg 0.994512718368 0.991251471241\n",
    "\n",
    "# change to stratified\n",
    "# all loss avg 0.0316412744167 0.0357232681944 all auc avg 0.994584962017 0.991293307537 pub 9866\n",
    "\n",
    "# change to base model 5 fold, add word batch, tilli, lgb feat\n",
    "# feat dim 217\n",
    "# all loss avg 0.031983129767200906 0.035387924581 all auc avg 0.9945427088311004 0.99188514127 PUB 9870\n",
    "\n",
    "# add more base models\n",
    "# feat dim 247, all loss avg 0.031885221777487156 0.0353958  all auc avg 0.9945986685511962 0.9919487331094685\n",
    "\n",
    "# change early stopping to 30, and test later part\n",
    "# all loss avg 0.03208696729295824 0.035406264613808025 all auc avg 0.9945261773637045 0.991946174432887\n",
    "\n",
    "# add fasttext lstm v1\n",
    "# all loss avg 0.031977558955160815 0.03537909655655411 all auc avg 0.9945299073418443 0.9919307178575658\n",
    "\n",
    "# add muse base model, feat dim 295, lower loss, but lower auc\n",
    "# all loss avg 0.03196610276261484 0.03530962013098028 all auc avg 0.9945635742334386 0.9918682952070021\n",
    "\n",
    "# fix pool gru fold to 5, and adj params\n",
    "# all loss avg 0.03172365669814678 0.03528054768190897 all auc avg 0.9946437910139179 0.991903618166854\n",
    "\n",
    "# updated pool gru v2\n",
    "# all loss avg 0.03204678384090053 0.035272185628381025 all auc avg 0.9944980867573662 0.9919033708443966\n",
    "\n",
    "# updated other feat, change some cnt to ratio, a bit worse\n",
    "# all loss avg 0.03197308230179816 0.03527747086566205 all auc avg 0.9945654974291834 0.991902658184264\n",
    "\n",
    "# updated pool gru v2 10 fold PUB 9870\n",
    "# all loss avg 0.03188938973578164 0.035151701851945265 all auc avg 0.9945816630869728 0.9919824289801534\n",
    "\n",
    "# rm lr, mnb feat1\n",
    "# worse all loss avg 0.032097370918022436 0.03520186226431002\n",
    "# all auc avg 0.9944811076726177 0.9919214297276806\n",
    "\n",
    "# add ridge , change lr,mnb,ridge to fold 6\n",
    "# all loss avg 0.03187703661408994 0.0351472518210873 all auc avg 0.9946115814252721 0.9920990121249438\n",
    "\n",
    "# ridge, lr, mnb fold 10, feat frac 0.6\n",
    "# all loss avg 0.031754244562510095 0.03510006533423048 all auc avg 0.9946626820706096 0.9921195758845225\n",
    "\n",
    "# lgb v1 feat fold 10, 5 fold is better, change feat file back\n",
    "# all loss avg 0.031846113030824186 0.03510235514884163 all auc avg 0.9946147807624219 0.9920710159886731\n",
    "\n",
    "# feat frac 0.45 PUB 9871\n",
    "# all loss avg 0.03178085998781136 0.035091249281527216 all auc avg 0.9946391992503673 0.9921624859169091\n",
    "\n",
    "# tilli feat 10 fold\n",
    "# all loss avg 0.031769047743715494 0.035076842284761586 all auc avg 0.9946504692097619 0.9921581333743735\n",
    "\n",
    "# update pool gru v2\n",
    "# all loss avg 0.031982729360866644 0.035041219881259106 all auc avg 0.9945421522346883 0.9922172582996719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.05, 'feature_fraction': 0.4, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.07054908247034279 0.07799381380216308 auc 0.9904564854821903 0.9871499408376233\n",
      "1 fold: ls 0.07026490907419657 0.0708493844601407 auc 0.9905569248947377 0.9899895084177281\n",
      "2 fold: ls 0.07010288855777674 0.07511858865400087 auc 0.9905634430396628 0.9887420248572584\n",
      "3 fold: ls 0.06708963346279326 0.07305350836513544 auc 0.9915805911764245 0.9891753688814958\n",
      "4 fold: ls 0.06853288965246013 0.07729230316955127 auc 0.9910896922471606 0.9880766959384076\n",
      "5 fold: ls 0.06850396201558229 0.07522622483776802 auc 0.9911423516975264 0.9881758327994962\n",
      "6 fold: ls 0.06852703429135769 0.07702614104935028 auc 0.9911058203997253 0.9881049365714475\n",
      "7 fold: ls 0.0691254261083487 0.06993777341552614 auc 0.9909102763380195 0.9904143831761563\n",
      "8 fold: ls 0.06903230689714812 0.07367716090582431 auc 0.9908818403866766 0.9894666017313751\n",
      "9 fold: ls 0.07106641394238877 0.0766025842487476 auc 0.990227325082827 0.9880566935324875\n",
      "toxic\n",
      "this class avg train 0.0692794546472395 avg val 0.07467774829082077\n",
      "this class auc train 0.9908514750744949 auc val 0.9887351986743476\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.075, 'feature_fraction': 0.6, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.01784498841526735 0.02087871998450636 auc 0.9942924712865449 0.9915289910115205\n",
      "1 fold: ls 0.01781594032345846 0.020625638955607917 auc 0.9943121085445084 0.9917220534244842\n",
      "2 fold: ls 0.017835493542469882 0.020871663603681374 auc 0.9943292020019268 0.9913208950500065\n",
      "3 fold: ls 0.017939395948453903 0.01952206131785635 auc 0.9942188334071885 0.9927720281048235\n",
      "4 fold: ls 0.01800695175910923 0.02108096945110194 auc 0.9942052958790479 0.9912959710089885\n",
      "5 fold: ls 0.0180387032681827 0.019810200315302152 auc 0.9941364351978887 0.9924335617676308\n",
      "6 fold: ls 0.01743437647960038 0.019027871471517934 auc 0.9946804545942822 0.9930024130845638\n",
      "7 fold: ls 0.017888996377383302 0.02034744811700291 auc 0.9942910615773858 0.991997923337884\n",
      "8 fold: ls 0.017952101531367375 0.020722160816503136 auc 0.9942331561291927 0.9915569909579998\n",
      "9 fold: ls 0.01812928775527521 0.018665045609395237 auc 0.9940927528661005 0.9934773858423084\n",
      "severe_toxic\n",
      "this class avg train 0.01788862354005678 avg val 0.020155177964247532\n",
      "this class auc train 0.9942791771484065 auc val 0.9921108213590208\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.095, 'feature_fraction': 0.6, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.03501276186469422 0.036758738524518456 auc 0.9962926209678646 0.9957846550072295\n",
      "1 fold: ls 0.03514333622973654 0.03672021284038774 auc 0.9962708309572097 0.9956772197766961\n",
      "2 fold: ls 0.033778530438696434 0.03902448108665873 auc 0.9965548516155227 0.995319993359249\n",
      "3 fold: ls 0.034179909886822804 0.033580316387400444 auc 0.9964724600579196 0.9964720227038507\n",
      "4 fold: ls 0.034312788842217466 0.0421432376910739 auc 0.9964489882636581 0.9945054833182455\n",
      "5 fold: ls 0.03264592151472135 0.037358939252314295 auc 0.9968035158166149 0.9956358205869548\n",
      "6 fold: ls 0.03567122614237551 0.03747633400777585 auc 0.9961500746011458 0.9954713680260368\n",
      "7 fold: ls 0.03528975841651287 0.037961461803730986 auc 0.9962287304933677 0.9956100955077825\n",
      "8 fold: ls 0.034570252492217735 0.0386850759951574 auc 0.9964005686836236 0.9952729286025291\n",
      "9 fold: ls 0.03521429468407392 0.038755308139161886 auc 0.9962501824202603 0.9953141347135699\n",
      "obscene\n",
      "this class avg train 0.03458187805120689 avg val 0.03784641057281797\n",
      "this class auc train 0.9963872823877187 auc val 0.9955063721602144\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.05, 'feature_fraction': 0.6, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.0050021257405958655 0.006775744429503689 auc 0.9985310715560132 0.9966857846218311\n",
      "1 fold: ls 0.004914462829221575 0.0063051638525380305 auc 0.9986726532593109 0.9967420909281374\n",
      "2 fold: ls 0.00451360091728444 0.007045295420647893 auc 0.9989609004187031 0.9948027969830295\n",
      "3 fold: ls 0.004570055494448908 0.006456267698677444 auc 0.9989180452798415 0.9932126993106628\n",
      "4 fold: ls 0.004302584117033868 0.006676175831440495 auc 0.999082510843098 0.996680338173361\n",
      "5 fold: ls 0.005004129426191659 0.0068025925696379875 auc 0.998513654247454 0.9963123598801517\n",
      "6 fold: ls 0.0048657538264647165 0.00630932922710763 auc 0.9986680823113925 0.9953262828168542\n",
      "7 fold: ls 0.004943575434431532 0.00737169652842123 auc 0.9985501904201328 0.9958487858025855\n",
      "8 fold: ls 0.005146334496338494 0.007065649006775819 auc 0.9983973131192769 0.9945889052496713\n",
      "9 fold: ls 0.004710745035633763 0.007273805184082782 auc 0.9987906544536078 0.9942344959296424\n",
      "threat\n",
      "this class avg train 0.004797336731764482 avg val 0.006808171974883299\n",
      "this class auc train 0.998708507590883 auc val 0.9954434539695928\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.075, 'feature_fraction': 0.4, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.04964303462319662 0.049855262350365 auc 0.9918298576759376 0.991470525248537\n",
      "1 fold: ls 0.048738175748239236 0.05327538463273099 auc 0.9921642219751595 0.9900521668133405\n",
      "2 fold: ls 0.05097845663910874 0.054979824420283326 auc 0.9913190548440731 0.9891233114382181\n",
      "3 fold: ls 0.04764560117465539 0.05097872414886621 auc 0.9925681406283645 0.9909050222687712\n",
      "4 fold: ls 0.049573945956825974 0.05518398631959181 auc 0.9918353399833425 0.9891773497444861\n",
      "5 fold: ls 0.04865458548399649 0.05237936184561952 auc 0.9921926661951458 0.9905999010137224\n",
      "6 fold: ls 0.04835672286624806 0.05552788819068433 auc 0.9922873764282304 0.989531649004967\n",
      "7 fold: ls 0.049454954408126015 0.055150017757222745 auc 0.9918819328956269 0.9896573153818103\n",
      "8 fold: ls 0.050641004089096345 0.052421299267702046 auc 0.9914398696850006 0.9904412823484798\n",
      "9 fold: ls 0.04984429784189032 0.054342190610639086 auc 0.9917436795701485 0.9895776538169742\n",
      "insult\n",
      "this class avg train 0.049353077883138315 avg val 0.0534093939543705\n",
      "this class auc train 0.9919262139881029 auc val 0.9900536177079309\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.095, 'feature_fraction': 0.4, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.015044865604511298 0.017004252117725827 auc 0.9952924716984135 0.9929302209625428\n",
      "1 fold: ls 0.014510681199286076 0.017388951508259847 auc 0.995733664835694 0.9922132439421271\n",
      "2 fold: ls 0.015065767853567325 0.017345314941278073 auc 0.9952274266464548 0.9928914351512443\n",
      "3 fold: ls 0.015241910474482218 0.01820355859395391 auc 0.9951307274659509 0.9888996353236955\n",
      "4 fold: ls 0.01493036399125096 0.018774530235074843 auc 0.9953810482366792 0.9899452828606621\n",
      "5 fold: ls 0.015145607047772703 0.016354230878900953 auc 0.9951839598590526 0.9935512423341973\n",
      "6 fold: ls 0.013549668696278721 0.01624126921154971 auc 0.9964081628119746 0.9917858949346052\n",
      "7 fold: ls 0.015250248427493186 0.01856875381947178 auc 0.9951079383193737 0.9887128314907146\n",
      "8 fold: ls 0.014167483434083361 0.016853594003453102 auc 0.9960029043817014 0.9925509429872101\n",
      "9 fold: ls 0.01490069408566706 0.016524212920252166 auc 0.9954359529690084 0.9923653262518969\n",
      "identity_hate\n",
      "this class avg train 0.014780729081439289 avg val 0.01732586682299202\n",
      "this class auc train 0.9954904257224303 auc val 0.9915846056238896\n",
      "========================\n",
      "all loss avg 0.03178018332247421 0.035037128263355345\n",
      "all auc avg 0.9946071803186727 0.9922390115824994\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999070      0.403654  0.980888  0.147801  0.926281   \n",
      "1  0000247867823ef7  0.000191      0.000033  0.000055  0.000030  0.000052   \n",
      "2  00013b17ad220c46  0.000351      0.000035  0.000115  0.000031  0.000165   \n",
      "3  00017563c3f7919a  0.000062      0.000033  0.000040  0.000059  0.000047   \n",
      "4  00017695ad8997eb  0.001256      0.000039  0.000177  0.000078  0.000192   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.396194  \n",
      "1       0.000038  \n",
      "2       0.000056  \n",
      "3       0.000038  \n",
      "4       0.000055  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "def special_ens(model_name,k=3,rnd=233):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    params_list = [\n",
    "        [0.05, 3,0.4], # depth should be 3\n",
    "        [0.075,3,0.6],\n",
    "        [0.095,3,0.6],\n",
    "        [0.05, 3,0.6],\n",
    "        [0.075,3,0.4],\n",
    "        [0.095,3,0.4],\n",
    "    ]\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        \n",
    "        # special params\n",
    "        params = {\n",
    "                'application': 'binary',\n",
    "                #'num_leaves': 8,\n",
    "                #'lambda_l1': 1,\n",
    "                'lambda_l2': 1.0,\n",
    "                'max_depth': params_list[i][1],\n",
    "                'metric': 'binary_logloss', # or auc\n",
    "                'data_random_seed': 2,\n",
    "                'learning_rate':params_list[i][0],\n",
    "                'feature_fraction': params_list[i][2],\n",
    "\n",
    "                }\n",
    "        print(params)\n",
    "            \n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "            s_round = 30\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i])\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred\n",
    "\n",
    "print('done')\n",
    "\n",
    "lgb_res = special_ens('lgb',10,666)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_log_csv_fold10_stratified_special.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# best params changed when base models changed\n",
    "# all loss avg 0.03111512578966158 0.03534320053008906 all auc avg 0.9948524214184179 0.9918450388634261\n",
    "\n",
    "# change lr, ridge, mnb fold to 10, train loss too low ?\n",
    "# all loss avg 0.030612393454975732 0.035095753229703264 all auc avg 0.9950485697696142 0.9921210772750391\n",
    "\n",
    "# tilli feat fold 10\n",
    "# all loss avg 0.03172720686364922 0.0350882288604753 all auc avg 0.9946451061433877 0.9921962852188195\n",
    "\n",
    "# update pool gru v2\n",
    "# all loss avg 0.03178018332247421 0.035037128263355345 all auc avg 0.9946071803186727 0.9922390115824994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.095 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.07085827353248232 0.07798084842168852 auc 0.9903233115480082 0.9872524557369384\n",
      "1 fold: ls 0.06625255482436719 0.0705600833910215 auc 0.9918243707478177 0.9900669268724033\n",
      "2 fold: ls 0.06818108014013745 0.0748847775176703 auc 0.9912062114694379 0.9887604168365433\n",
      "3 fold: ls 0.06601199204171145 0.07305479782103391 auc 0.9919402453983566 0.9891495476297903\n",
      "4 fold: ls 0.0693323629687326 0.07722464066960674 auc 0.990792059100126 0.9880921988220346\n",
      "5 fold: ls 0.06440595724399693 0.07492308373241514 auc 0.9923908487328252 0.9881962313305844\n",
      "6 fold: ls 0.06877299277394358 0.07731179436612658 auc 0.9910170358346071 0.9880255182904107\n",
      "7 fold: ls 0.07217164787214733 0.07017723982894845 auc 0.98989274835965 0.9903317860655048\n",
      "8 fold: ls 0.06644510648395821 0.07360303958204517 auc 0.9917515689468738 0.9895266229028914\n",
      "9 fold: ls 0.06653699594977339 0.07624970565715444 auc 0.991711174745344 0.9883336341191891\n",
      "toxic 0.095 0.4 3\n",
      "this class avg train 0.06789689638312504 avg val 0.07459700109877107\n",
      "this class auc train 0.9912849574883046 auc val 0.988773533860629\n",
      "========================\n",
      "0 fold: ls 0.01801499655757402 0.020911261210646275 auc 0.9941860752358176 0.9915032757311051\n",
      "1 fold: ls 0.01767961315412588 0.020745269863634546 auc 0.9944350221440534 0.9915064406886946\n",
      "2 fold: ls 0.01819458630197277 0.020776290106638617 auc 0.9940752532662227 0.9913865679199899\n",
      "3 fold: ls 0.01666294388112957 0.019484670381444334 auc 0.9953347947851402 0.9928606469173313\n",
      "4 fold: ls 0.017977517746377274 0.020945829389014747 auc 0.9942210047051488 0.991400810229143\n",
      "5 fold: ls 0.01776134254002069 0.019697937101721116 auc 0.9943808299894468 0.9924681971525733\n",
      "6 fold: ls 0.01800103309945274 0.018962232992595758 auc 0.9942047874799278 0.9930852247640365\n",
      "7 fold: ls 0.017997373158664825 0.020404243462768243 auc 0.994184189352427 0.9919644801596355\n",
      "8 fold: ls 0.01797567320708851 0.020728300068826915 auc 0.99421191637651 0.9915225524470651\n",
      "9 fold: ls 0.01799474017040712 0.01870263748291824 auc 0.994192692110563 0.9934694231808205\n",
      "severe_toxic 0.095 0.4 3\n",
      "this class avg train 0.017825981981681338 avg val 0.02013586720602088\n",
      "this class auc train 0.9943426565445256 auc val 0.9921167619190395\n",
      "========================\n",
      "0 fold: ls 0.03558778112577781 0.03690549950501394 auc 0.9961635449721086 0.9957740054508502\n",
      "1 fold: ls 0.034114952352247256 0.0365495615125795 auc 0.996495358812336 0.9957128488072301\n",
      "2 fold: ls 0.0352532912363591 0.03890722992283616 auc 0.996230910880435 0.9953659617655628\n",
      "3 fold: ls 0.03556224211942488 0.03367905772394987 auc 0.996151979418229 0.9965092203069155\n",
      "4 fold: ls 0.03488165323187822 0.041938994834294384 auc 0.9963224881041755 0.9945578732055094\n",
      "5 fold: ls 0.03532857592797993 0.03730744925442132 auc 0.9962162331528599 0.9956600186066326\n",
      "6 fold: ls 0.035841312057100144 0.03751669906497995 auc 0.9961186475365534 0.995522504941408\n",
      "7 fold: ls 0.03537278992242763 0.03802061519435101 auc 0.9962105346556618 0.995579515162526\n",
      "8 fold: ls 0.03456420895857297 0.03854725840086725 auc 0.9963993479569396 0.9953046444535633\n",
      "9 fold: ls 0.03361461463899195 0.03860366786426793 auc 0.9965963403027419 0.9953318539110189\n",
      "obscene 0.095 0.4 3\n",
      "this class avg train 0.035012142157075994 avg val 0.03779760332775613\n",
      "this class auc train 0.9962905385792041 auc val 0.9955318446611215\n",
      "========================\n",
      "0 fold: ls 0.004881216587819931 0.006765270596417183 auc 0.9986558914929 0.9967473287240729\n",
      "1 fold: ls 0.004670906935882958 0.006273687487112723 auc 0.9988456957586722 0.9968049444793631\n",
      "2 fold: ls 0.004907307727325069 0.007165680132466428 auc 0.9986236348377723 0.9942763984915148\n",
      "3 fold: ls 0.004406989311642278 0.0064552591080812415 auc 0.999028587707604 0.9936238921365265\n",
      "4 fold: ls 0.004225969699796507 0.00660002201609406 auc 0.999131999937631 0.9965755756699562\n",
      "5 fold: ls 0.004863099794036336 0.006903486185929679 auc 0.9986799145415753 0.996132954093071\n",
      "6 fold: ls 0.004569503380827582 0.006188719902135316 auc 0.9988580963963754 0.9954794979780837\n",
      "7 fold: ls 0.00498709779549681 0.0073243269307647785 auc 0.9985023011535654 0.9959417625243572\n",
      "8 fold: ls 0.005092622139933012 0.007118847270581577 auc 0.9984292111342984 0.994659787113677\n",
      "9 fold: ls 0.00388445760690332 0.007313659634954247 auc 0.9993480797605612 0.9944511537026411\n",
      "threat 0.095 0.4 3\n",
      "this class avg train 0.0046489170979663805 avg val 0.006810895926453722\n",
      "this class auc train 0.9988103412720957 auc val 0.9954693294913264\n",
      "========================\n",
      "0 fold: ls 0.05018096249458346 0.04984493632321642 auc 0.9916112320035961 0.991482989737292\n",
      "1 fold: ls 0.05066879284324924 0.053310952491023925 auc 0.9914261451563948 0.9898102386154879\n",
      "2 fold: ls 0.051260659163783404 0.055056485445885116 auc 0.9912489229025343 0.9888377993568657\n",
      "3 fold: ls 0.050303175330295 0.051257370904934305 auc 0.9915731070919366 0.990785731255584\n",
      "4 fold: ls 0.048406752814799635 0.05523427556396291 auc 0.9922737825552219 0.9891604504645295\n",
      "5 fold: ls 0.04345910969555771 0.05190878569321837 auc 0.9940351921313106 0.9907523291725411\n",
      "6 fold: ls 0.049056235787947315 0.05555562067844542 auc 0.9920480291015203 0.9894918269393262\n",
      "7 fold: ls 0.04936790799802893 0.05514170144326197 auc 0.9919040835528452 0.989654383568173\n",
      "8 fold: ls 0.049367436923989545 0.05228824970179308 auc 0.9919220165869072 0.9902202236002119\n",
      "9 fold: ls 0.04991022207271313 0.05417257972189585 auc 0.9917088610635384 0.9896387611897903\n",
      "insult 0.095 0.4 3\n",
      "this class avg train 0.049198125512494736 avg val 0.05337709579676374\n",
      "this class auc train 0.9919751372145805 auc val 0.9899834733899799\n",
      "========================\n",
      "0 fold: ls 0.015044865604511298 0.017004252117725827 auc 0.9952924716984135 0.9929302209625428\n",
      "1 fold: ls 0.014510681199286076 0.017388951508259847 auc 0.995733664835694 0.9922132439421271\n",
      "2 fold: ls 0.015065767853567325 0.017345314941278073 auc 0.9952274266464548 0.9928914351512443\n",
      "3 fold: ls 0.015241910474482218 0.01820355859395391 auc 0.9951307274659509 0.9888996353236955\n",
      "4 fold: ls 0.01493036399125096 0.018774530235074843 auc 0.9953810482366792 0.9899452828606621\n",
      "5 fold: ls 0.015145607047772703 0.016354230878900953 auc 0.9951839598590526 0.9935512423341973\n",
      "6 fold: ls 0.013549668696278721 0.01624126921154971 auc 0.9964081628119746 0.9917858949346052\n",
      "7 fold: ls 0.015250248427493186 0.01856875381947178 auc 0.9951079383193737 0.9887128314907146\n",
      "8 fold: ls 0.014167483434083361 0.016853594003453102 auc 0.9960029043817014 0.9925509429872101\n",
      "9 fold: ls 0.01490069408566706 0.016524212920252166 auc 0.9954359529690084 0.9923653262518969\n",
      "identity_hate 0.095 0.4 3\n",
      "this class avg train 0.014780729081439289 avg val 0.01732586682299202\n",
      "this class auc train 0.9954904257224303 auc val 0.9915846056238896\n",
      "========================\n",
      "all loss avg 0.03156046536896379 0.03500738836312626\n",
      "all auc avg 0.9946990094701901 0.9922432581576642\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.095 3 0.4 toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.4 severe_toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.4 obscene\n",
      "FIND BETTER PARAMS 0.095 3 0.4 threat\n",
      "FIND BETTER PARAMS 0.095 3 0.4 insult\n",
      "FIND BETTER PARAMS 0.095 3 0.4 identity_hate\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.48\n",
      "0 fold: ls 0.07057631307231102 0.07809440281598456 auc 0.9904510329458328 0.9871768492999269\n",
      "1 fold: ls 0.06920794057767342 0.07032593588374933 auc 0.9909131735894722 0.9901183428736063\n",
      "2 fold: ls 0.07002299503635256 0.07514293804059685 auc 0.9905902222949332 0.9887618211502325\n",
      "3 fold: ls 0.06833952224135462 0.07306878721248639 auc 0.9911923677901603 0.9891841118667224\n",
      "4 fold: ls 0.06843315701509747 0.07709138220250707 auc 0.9911115321390505 0.9881631403801524\n",
      "5 fold: ls 0.06391007900536469 0.07452495340744476 auc 0.9925522276081291 0.9883846230977009\n",
      "6 fold: ls 0.07002578174279787 0.07763772335013883 auc 0.9905827634930632 0.9879246135566281\n",
      "7 fold: ls 0.07156677935160481 0.0700904634555228 auc 0.9900729741951266 0.9904280738059131\n",
      "8 fold: ls 0.0701030434290233 0.07375622703225056 auc 0.9905358388741319 0.9894631790739359\n",
      "9 fold: ls 0.06777189345684807 0.07633387492861321 auc 0.9913414035949999 0.988220890423146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.095 0.48 3\n",
      "this class avg train 0.06899575049284278 avg val 0.07460666883292945\n",
      "this class auc train 0.99093435365249 auc val 0.9887825645527965\n",
      "========================\n",
      "0 fold: ls 0.017338010487293602 0.02082985361076282 auc 0.9947028294463096 0.9915349253070009\n",
      "1 fold: ls 0.018022351308761387 0.020722802597329087 auc 0.9941522314154662 0.9915456070388657\n",
      "2 fold: ls 0.01818077125666185 0.02086609192852913 auc 0.9940943562770582 0.991387754779086\n",
      "3 fold: ls 0.017991023999446413 0.01950313932475885 auc 0.9941754221520326 0.992819502468667\n",
      "4 fold: ls 0.01813337086122452 0.020977748020218647 auc 0.9941094107729609 0.9914779560703886\n",
      "5 fold: ls 0.017902591874846427 0.019777176281426098 auc 0.9942654370520911 0.9924216185314437\n",
      "6 fold: ls 0.018022509887937056 0.019134665205145934 auc 0.9941715013369021 0.9929566277810091\n",
      "7 fold: ls 0.018005706358819408 0.020392079524569354 auc 0.9941866897314758 0.9919194911222295\n",
      "8 fold: ls 0.017943241774021 0.020913292443824172 auc 0.9942410490985106 0.9913308513717476\n",
      "9 fold: ls 0.01802601292966598 0.018703255187087146 auc 0.9941816938712804 0.9934156752157782\n",
      "severe_toxic 0.095 0.48 3\n",
      "this class avg train 0.017956559073867766 avg val 0.020182010412365125\n",
      "this class auc train 0.9942280621154087 auc val 0.9920810009686217\n",
      "========================\n",
      "0 fold: ls 0.03550823280129518 0.036694595291195446 auc 0.9961860843452476 0.9958084598979601\n",
      "1 fold: ls 0.035571531164850616 0.03673256011562219 auc 0.9961737295334554 0.9956655522480156\n",
      "2 fold: ls 0.034553696223880694 0.03900534039928079 auc 0.9963831028646477 0.9953326796996627\n",
      "3 fold: ls 0.03538146673332712 0.03351507437516874 auc 0.996206434947828 0.9965472793281565\n",
      "4 fold: ls 0.034444701384834374 0.042089385015586074 auc 0.9964165377224755 0.9945048568322991\n",
      "5 fold: ls 0.03336920019612725 0.03752901032715062 auc 0.9966483965056411 0.9955554737643347\n",
      "6 fold: ls 0.035853872556871995 0.03748491042117706 auc 0.9961205073922181 0.9955295529083045\n",
      "7 fold: ls 0.03533213249715676 0.03795979850995175 auc 0.9962191580623404 0.9956002675094991\n",
      "8 fold: ls 0.03344546706578972 0.038558890807231364 auc 0.9966479304539486 0.9953086383014712\n",
      "9 fold: ls 0.035297571340160074 0.038720518805552766 auc 0.9962264488132494 0.9953258952428502\n",
      "obscene 0.095 0.48 3\n",
      "this class avg train 0.03487578719642938 avg val 0.037829008406791674\n",
      "this class auc train 0.9963228330641052 auc val 0.9955178655732555\n",
      "========================\n",
      "0 fold: ls 0.004720921465404068 0.006743409189791411 auc 0.9987919671508358 0.9968193484181856\n",
      "1 fold: ls 0.004851117067997533 0.00632199781183387 auc 0.9987270477900374 0.9966569767441861\n",
      "2 fold: ls 0.004673914568119069 0.0071968583072768945 auc 0.9988329944976904 0.9943169914100146\n",
      "3 fold: ls 0.0037109069136083686 0.006358237425749418 auc 0.999439865309103 0.9932454375929768\n",
      "4 fold: ls 0.004132674502764672 0.006624804568681951 auc 0.9991871737000626 0.9966109330148554\n",
      "5 fold: ls 0.005118062510960585 0.006980587027941964 auc 0.9983916937581695 0.996076644247491\n",
      "6 fold: ls 0.004017483937043904 0.006268604303612544 auc 0.9992611880761004 0.9957086659542816\n",
      "7 fold: ls 0.004963968343507584 0.007373811679452376 auc 0.998547096336605 0.9959286672114316\n",
      "8 fold: ls 0.0052450674336632684 0.0071572570346669015 auc 0.9982921558073736 0.9945942548243133\n",
      "9 fold: ls 0.004326830528690064 0.007379093497507071 auc 0.9990763511276455 0.993821241288552\n",
      "threat 0.095 0.48 3\n",
      "this class avg train 0.0045760947271759125 avg val 0.00684046608465144\n",
      "this class auc train 0.9988547533553623 auc val 0.995377916070629\n",
      "========================\n",
      "0 fold: ls 0.04925275935534324 0.04960843113715681 auc 0.9919459429586764 0.9915904854960197\n",
      "1 fold: ls 0.050977205651953995 0.05338435398833609 auc 0.9913152312971167 0.9898527349932573\n",
      "2 fold: ls 0.05133983645459956 0.05511076411756965 auc 0.9912227341115181 0.9888415637997785\n",
      "3 fold: ls 0.050383551597495115 0.051108509283326116 auc 0.9915423094828688 0.9909038092816104\n",
      "4 fold: ls 0.05098172983170385 0.055383828749394494 auc 0.9913065020047902 0.9891048166963547\n",
      "5 fold: ls 0.04537576889222667 0.05205735990036866 auc 0.9933914088716747 0.9907011293738599\n",
      "6 fold: ls 0.049303920521404594 0.05589660497594918 auc 0.9919381766794843 0.9893197387270926\n",
      "7 fold: ls 0.048919194353113274 0.055193021174239756 auc 0.9920708188774924 0.9896521218833669\n",
      "8 fold: ls 0.04668930431673057 0.052156194674645204 auc 0.9929259922068243 0.9903487208036386\n",
      "9 fold: ls 0.04916121005493522 0.05434665504386241 auc 0.9919981099469529 0.9895633298132025\n",
      "insult 0.095 0.48 3\n",
      "this class avg train 0.04923844810295061 avg val 0.05342457230448483\n",
      "this class auc train 0.9919657226437399 auc val 0.9899878450868181\n",
      "========================\n",
      "0 fold: ls 0.014624005257450954 0.01708093743926495 auc 0.9956071928228456 0.9930651866180431\n",
      "1 fold: ls 0.014814675652557363 0.017510659053069106 auc 0.9955375932000252 0.9921060785213145\n",
      "2 fold: ls 0.013927658969467155 0.017125465700188792 auc 0.9961436040019667 0.9929270822263684\n",
      "3 fold: ls 0.015111241972300748 0.018226847570588022 auc 0.9952513749635301 0.9875365270422299\n",
      "4 fold: ls 0.014636282811438755 0.018598698607871778 auc 0.9955930622597999 0.9900950454152705\n",
      "5 fold: ls 0.014767265326495577 0.01633141437493889 auc 0.9954586507686101 0.9936871720300942\n",
      "6 fold: ls 0.013805828983614469 0.016298767854920855 auc 0.9962498989991545 0.9917402810896742\n",
      "7 fold: ls 0.01503660332087648 0.018533855244056513 auc 0.9953319005790714 0.9884407290989233\n",
      "8 fold: ls 0.01396588558387468 0.016783642709427254 auc 0.9961634617120024 0.9927207529445768\n",
      "9 fold: ls 0.01475568872510276 0.01658028700335539 auc 0.9955655283767255 0.992253775561818\n",
      "identity_hate 0.095 0.48 3\n",
      "this class avg train 0.014544513660317896 avg val 0.017307057555768158\n",
      "this class auc train 0.995690226768373 auc val 0.9914572630548312\n",
      "========================\n",
      "all loss avg 0.03169785887559739 0.035031630599498445\n",
      "all auc avg 0.9946659919332465 0.9922007425511588\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.095 3 0.48 toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.48 insult\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.56\n",
      "0 fold: ls 0.07125140084612051 0.07840555064043322 auc 0.9902175161165504 0.9870534508970393\n",
      "1 fold: ls 0.06931289501674336 0.07045513168110573 auc 0.9908858703423804 0.9900478100860527\n",
      "2 fold: ls 0.07007872512182346 0.07534036840810046 auc 0.9905746379444801 0.9887068717145854\n",
      "3 fold: ls 0.06684402938535874 0.07304311921755523 auc 0.9916728758438571 0.989113126074753\n",
      "4 fold: ls 0.06632060568713306 0.07715221313693976 auc 0.9917641329049176 0.9881446003818969\n",
      "5 fold: ls 0.06627237183307667 0.0748568473753132 auc 0.9918564819221114 0.9881982258536242\n",
      "6 fold: ls 0.0683433486104414 0.07710633433701242 auc 0.9911632667676804 0.9881066591140728\n",
      "7 fold: ls 0.06932067730984964 0.07020274186166733 auc 0.9908246571353322 0.9903377247161609\n",
      "8 fold: ls 0.06806122595354303 0.07371108131479513 auc 0.9912161460435013 0.9894713617185422\n",
      "9 fold: ls 0.07006442437649732 0.07640516073507628 auc 0.9905539110832431 0.9881772798740535\n",
      "toxic 0.095 0.56 3\n",
      "this class avg train 0.06858697041405871 avg val 0.07466785487079988\n",
      "this class auc train 0.9910729496104054 auc val 0.9887357110430781\n",
      "========================\n",
      "0 fold: ls 0.016679659160490187 0.020794382642953614 auc 0.9952792698439601 0.9916583586529941\n",
      "1 fold: ls 0.017199389706924216 0.02059601057058624 auc 0.9948473089849633 0.9917901000126599\n",
      "2 fold: ls 0.017955800416849545 0.02079971193710927 auc 0.9942408453691286 0.991411096341309\n",
      "3 fold: ls 0.018097157048849552 0.019599441621026652 auc 0.9940963560277756 0.9927122895303203\n",
      "4 fold: ls 0.01798450180012033 0.021056457833932205 auc 0.9942150471144701 0.9913418628940371\n",
      "5 fold: ls 0.018034124899134446 0.01970939708978789 auc 0.9941586791769632 0.9925354773830937\n",
      "6 fold: ls 0.01771228290911033 0.019106261646379897 auc 0.9944488891273842 0.9929852933623652\n",
      "7 fold: ls 0.016801382361102502 0.02032876107196263 auc 0.9952039913553202 0.9920512731698519\n",
      "8 fold: ls 0.017984530936840278 0.020770326497286964 auc 0.99421719631503 0.9914773643431223\n",
      "9 fold: ls 0.01811578974707309 0.018696744646896805 auc 0.994101948677391 0.9934630530516304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_toxic 0.095 0.56 3\n",
      "this class avg train 0.01765646189864945 avg val 0.020145749555792215\n",
      "this class auc train 0.9944809531992387 auc val 0.9921426168741385\n",
      "========================\n",
      "0 fold: ls 0.03546032253236468 0.036789235477717584 auc 0.9961959904338261 0.9957806222708064\n",
      "1 fold: ls 0.03567423895938666 0.036618632286133504 auc 0.9961456410102575 0.9957162159463795\n",
      "2 fold: ls 0.0344042868464071 0.038957553457418546 auc 0.9964192552292838 0.995349281577241\n",
      "3 fold: ls 0.035077750555038645 0.03364969878765119 auc 0.9962712678628814 0.9964768779699349\n",
      "4 fold: ls 0.034458421082097984 0.04209645811697498 auc 0.9964134933993349 0.9945190310768354\n",
      "5 fold: ls 0.034588086569177584 0.037392540130312686 auc 0.9963802499901278 0.9956211764779588\n",
      "6 fold: ls 0.030969352557429462 0.03728326995636316 auc 0.997173883325641 0.9954494410179144\n",
      "7 fold: ls 0.035132440276023615 0.03807799117253782 auc 0.9962577688013085 0.9955749339840434\n",
      "8 fold: ls 0.03346531172800258 0.03886290368261612 auc 0.996639396003195 0.995223122969794\n",
      "9 fold: ls 0.03349854607292828 0.03844094170078595 auc 0.9966294425406028 0.9954239780570477\n",
      "obscene 0.095 0.56 3\n",
      "this class avg train 0.03427287571788566 avg val 0.03781692247685116\n",
      "this class auc train 0.9964526388596457 auc val 0.9955134681347955\n",
      "========================\n",
      "0 fold: ls 0.0051139390762320125 0.006892353897326672 auc 0.9983679366898988 0.9966910224177666\n",
      "1 fold: ls 0.004803099346786954 0.006334386992153647 auc 0.9987434441109596 0.9966687617850409\n",
      "2 fold: ls 0.004632610747381895 0.007259178922502522 auc 0.9988543527562467 0.9941336685522733\n",
      "3 fold: ls 0.004486921648120602 0.006401034922600064 auc 0.9989726262126208 0.9932532947807321\n",
      "4 fold: ls 0.004766425742009902 0.006568395587047218 auc 0.9987592319006671 0.9968387914597607\n",
      "5 fold: ls 0.004314767285369763 0.006914764287989072 auc 0.9990638407695286 0.9959692626815011\n",
      "6 fold: ls 0.003935022938101973 0.006317375500559509 auc 0.9992989992385793 0.995038185932491\n",
      "7 fold: ls 0.004896553853325761 0.007447609665912093 auc 0.9986148007312756 0.995597355794414\n",
      "8 fold: ls 0.004573729946643317 0.007088532772919022 auc 0.9989249064497738 0.9948323108958799\n",
      "9 fold: ls 0.004563326245453291 0.007210247069206831 auc 0.9989045215359322 0.9946129783355601\n",
      "threat 0.095 0.56 3\n",
      "this class avg train 0.004608639682942547 avg val 0.006843387961821665\n",
      "this class auc train 0.9988504660395483 auc val 0.995363563263542\n",
      "========================\n",
      "0 fold: ls 0.049292075972644046 0.04979954773516128 auc 0.9919505057970386 0.9914949523003255\n",
      "1 fold: ls 0.04945851375990819 0.05327630210997892 auc 0.9918829492347159 0.9900345994130816\n",
      "2 fold: ls 0.05066543698845716 0.055059841157379 auc 0.9914322754371925 0.9890071992879347\n",
      "3 fold: ls 0.049376937384411096 0.051159475613373674 auc 0.9919067568977871 0.9908817663769997\n",
      "4 fold: ls 0.049920708711624004 0.055480090666818443 auc 0.9917087732918031 0.9890685920021899\n",
      "5 fold: ls 0.04722757914349187 0.05231479533816461 auc 0.9927332903865518 0.9906444916880641\n",
      "6 fold: ls 0.04911535635073413 0.05583296305746721 auc 0.9920051412148608 0.9894064102817227\n",
      "7 fold: ls 0.049048158351011305 0.055016432923961175 auc 0.9920197306248256 0.9896997847965023\n",
      "8 fold: ls 0.05008445270597792 0.05222660019302162 auc 0.9916455378296003 0.9904326544397751\n",
      "9 fold: ls 0.05090363782287997 0.05456497057328239 auc 0.9913354684028224 0.9894988299131773\n",
      "insult 0.095 0.56 3\n",
      "this class avg train 0.04950928571911396 avg val 0.053473101936860834\n",
      "this class auc train 0.9918620429117198 auc val 0.9900169280499773\n",
      "========================\n",
      "0 fold: ls 0.014795610661323868 0.01715564603173885 auc 0.9955111633504427 0.9929208047540194\n",
      "1 fold: ls 0.015016603734639936 0.01743391806101592 auc 0.9953281440755654 0.9922899187829596\n",
      "2 fold: ls 0.014898435074750435 0.01729282024117518 auc 0.995382812820265 0.993056667191284\n",
      "3 fold: ls 0.015068300236184147 0.018203158964572735 auc 0.9953008388813694 0.9872087533074432\n",
      "4 fold: ls 0.014428613890133972 0.01864820230289708 auc 0.9957625206403481 0.9898605369839525\n",
      "5 fold: ls 0.015036464009673185 0.01632181686416442 auc 0.9952849364239985 0.9936600764096497\n",
      "6 fold: ls 0.013793902927085128 0.016367484194162348 auc 0.9962671364767851 0.9917678300455236\n",
      "7 fold: ls 0.015015705419997047 0.018487423847468272 auc 0.9953442122697633 0.9861083261073778\n",
      "8 fold: ls 0.013913456167627456 0.01676794457317766 auc 0.9962133359989226 0.9927266240335285\n",
      "9 fold: ls 0.015209300937428564 0.016463394754236547 auc 0.9952083727271339 0.9924177144302335\n",
      "identity_hate 0.095 0.56 3\n",
      "this class avg train 0.014717639305884373 avg val 0.0173141809834609\n",
      "this class auc train 0.9955603473664594 auc val 0.9912017252045973\n",
      "========================\n",
      "all loss avg 0.03155864545642245 0.03504353296426444\n",
      "all auc avg 0.9947132329978362 0.9921623354283549\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.095 3 0.56 severe_toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.56 insult\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.62\n",
      "0 fold: ls 0.06996259609465623 0.07853014437086923 auc 0.9906367319544698 0.9869351261436097\n",
      "1 fold: ls 0.0696749868812875 0.0705097580050801 auc 0.9907114412144324 0.9901385921709964\n",
      "2 fold: ls 0.06986005987706473 0.07511046039087957 auc 0.9906393197410942 0.9887553884875271\n",
      "3 fold: ls 0.0667919248558969 0.07320282394014764 auc 0.9916595251029615 0.9890899322486596\n",
      "4 fold: ls 0.06842255142126624 0.07726798136953893 auc 0.9911137348148976 0.9881173116803077\n",
      "5 fold: ls 0.0655664523556384 0.07503816971895455 auc 0.9920617615761056 0.9882252425747987\n",
      "6 fold: ls 0.06716877421777116 0.07709294900268131 auc 0.9915443867562874 0.988093377403831\n",
      "7 fold: ls 0.07112475830165785 0.0702479609006435 auc 0.990232892992711 0.9902994181527687\n",
      "8 fold: ls 0.06905266005295335 0.07391542831239724 auc 0.9908762215644332 0.9894169618652042\n",
      "9 fold: ls 0.06919492620568721 0.07634634110306468 auc 0.9908369342871101 0.988287484910274\n",
      "toxic 0.095 0.62 3\n",
      "this class avg train 0.06868196902638797 avg val 0.07472620171142567\n",
      "this class auc train 0.9910312950004503 auc val 0.9887358835637976\n",
      "========================\n",
      "0 fold: ls 0.017700012474516042 0.020901706878908858 auc 0.9943966151670407 0.9914791429294847\n",
      "1 fold: ls 0.017355721576742176 0.020572306966410762 auc 0.9947135879091151 0.9917968255475377\n",
      "2 fold: ls 0.017894206908000386 0.02087519925200767 auc 0.9942941205907518 0.9912912235726042\n",
      "3 fold: ls 0.01790170498286413 0.019575646762582655 auc 0.994266222107705 0.9927589726547665\n",
      "4 fold: ls 0.017980633431747568 0.02101366473299023 auc 0.9942257124516293 0.9913881503987847\n",
      "5 fold: ls 0.018059402959304124 0.01977439501899914 auc 0.9941396800808872 0.9924674009368275\n",
      "6 fold: ls 0.017875508873340586 0.018886863423423796 auc 0.994308000971979 0.9931568887174264\n",
      "7 fold: ls 0.01724614662895425 0.020249870570259336 auc 0.9948181640111906 0.9920580414321165\n",
      "8 fold: ls 0.01802844157201082 0.02086169788501207 auc 0.9941900594881563 0.9914088854543275\n",
      "9 fold: ls 0.017061492350019986 0.01862332169048112 auc 0.9950109674804569 0.9935092364882592\n",
      "severe_toxic 0.095 0.62 3\n",
      "this class avg train 0.017710327175750003 avg val 0.020133467318107563\n",
      "this class auc train 0.9944363130258914 auc val 0.9921314768132137\n",
      "========================\n",
      "0 fold: ls 0.035023967627501484 0.036695277348049414 auc 0.9962894929838186 0.9958065805644813\n",
      "1 fold: ls 0.035841602824422565 0.036812429915914296 auc 0.9961188370393952 0.9956296099952351\n",
      "2 fold: ls 0.03303443204532541 0.038982919340873244 auc 0.9967199857198666 0.9953203066022221\n",
      "3 fold: ls 0.03563509030993732 0.03357272617974798 auc 0.9961551615263613 0.9965059704110688\n",
      "4 fold: ls 0.03454370921322424 0.04219284772645947 auc 0.9963939245462696 0.9944973390009428\n",
      "5 fold: ls 0.03365937288068428 0.03741706273788013 auc 0.9965885011278645 0.995608490137545\n",
      "6 fold: ls 0.03374594167139081 0.03725139677266719 auc 0.996573814698079 0.995552889509806\n",
      "7 fold: ls 0.03505466162389811 0.03793621142819386 auc 0.9962779985388796 0.9956157730366714\n",
      "8 fold: ls 0.03501833181586512 0.03872550847199984 auc 0.9963047887758973 0.9952520196340695\n",
      "9 fold: ls 0.033271488319472214 0.03857515085681895 auc 0.9966751178184727 0.9953396158603438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obscene 0.095 0.62 3\n",
      "this class avg train 0.034482859833172154 avg val 0.037816153077860445\n",
      "this class auc train 0.9964097622774905 auc val 0.9955128594752386\n",
      "========================\n",
      "0 fold: ls 0.003853004235446458 0.006753863671145623 auc 0.9993611850438916 0.9969686256023466\n",
      "1 fold: ls 0.0049664287630285615 0.006373117100323511 auc 0.9986183074546494 0.9965574586214121\n",
      "2 fold: ls 0.004238837363409342 0.007247360762065442 auc 0.999134072854238 0.9941533102870312\n",
      "3 fold: ls 0.0036071039895356895 0.006393554573797135 auc 0.9994985473237233 0.9934117480671318\n",
      "4 fold: ls 0.004340516439479098 0.006680940232611025 auc 0.9990511152343902 0.9966017662958074\n",
      "5 fold: ls 0.0047168221443877 0.0068737882384817695 auc 0.9987889870766384 0.9958003331447608\n",
      "6 fold: ls 0.004885128575826627 0.006268601765275631 auc 0.9986429073535564 0.9956916420474784\n",
      "7 fold: ls 0.005192192179282805 0.007444071123339415 auc 0.9981775117136643 0.9955764032937331\n",
      "8 fold: ls 0.004409209094146621 0.007036177273871708 auc 0.9990075479923743 0.994978086804873\n",
      "9 fold: ls 0.0041813365591853325 0.007229915753315764 auc 0.999156027297247 0.9947320063713434\n",
      "threat 0.095 0.62 3\n",
      "this class avg train 0.004439057934372823 avg val 0.006830139049422703\n",
      "this class auc train 0.9989436209344372 auc val 0.9954471380535918\n",
      "========================\n",
      "0 fold: ls 0.04999076049008611 0.0498486638191776 auc 0.9916921428241644 0.9914745406543104\n",
      "1 fold: ls 0.051171210039664544 0.05345146165976281 auc 0.9912254624119472 0.9896308001699854\n",
      "2 fold: ls 0.051222046491645024 0.0549774972049151 auc 0.9912631709232762 0.9888292666195972\n",
      "3 fold: ls 0.04985893362904846 0.05118000240155541 auc 0.9917250895962288 0.9908637807053061\n",
      "4 fold: ls 0.0498667624078434 0.055384810050843836 auc 0.9917315537360819 0.9890823122096796\n",
      "5 fold: ls 0.04903725013330802 0.05267172463695408 auc 0.9920700408123581 0.9905067039945546\n",
      "6 fold: ls 0.04965211397670843 0.05605198925120693 auc 0.9917934424609348 0.989266949392178\n",
      "7 fold: ls 0.04868772579543763 0.055119427252397546 auc 0.9921591766621064 0.9896670322498663\n",
      "8 fold: ls 0.05002379652231413 0.05242574563309688 auc 0.9916844257097837 0.9904471459757549\n",
      "9 fold: ls 0.04865162946119003 0.05439553515500744 auc 0.9921968284403653 0.9895056149675955\n",
      "insult 0.095 0.62 3\n",
      "this class avg train 0.04981622289472457 avg val 0.053550685706491755\n",
      "this class auc train 0.9917541333577246 auc val 0.9899274146938829\n",
      "========================\n",
      "0 fold: ls 0.014327539519613288 0.017109662451761234 auc 0.9958497188582065 0.9928535461217104\n",
      "1 fold: ls 0.014338319017740915 0.01745990033695573 auc 0.9958843797260273 0.9922392506132865\n",
      "2 fold: ls 0.015058511502664573 0.017422213257606737 auc 0.9952440859828283 0.9928210377827609\n",
      "3 fold: ls 0.01529469495579459 0.01822800201645398 auc 0.995098895489611 0.9879898502239935\n",
      "4 fold: ls 0.01444676265291993 0.018385385658261713 auc 0.9957860521266549 0.9899614249324162\n",
      "5 fold: ls 0.014578794430709264 0.016304443851217452 auc 0.995659920836439 0.9936718178451757\n",
      "6 fold: ls 0.015240449348650355 0.016455667750198542 auc 0.9950833232679569 0.992373681263097\n",
      "7 fold: ls 0.015247846521161014 0.018531636459713088 auc 0.9951306930115746 0.986231844786473\n",
      "8 fold: ls 0.013758564499253206 0.016899069508634578 auc 0.9963177604607028 0.9925360394537178\n",
      "9 fold: ls 0.015226439277585244 0.016687679227469023 auc 0.9951910463965259 0.9920564166486018\n",
      "identity_hate 0.095 0.62 3\n",
      "this class avg train 0.014751792172609237 avg val 0.017348366051827206\n",
      "this class auc train 0.9955245876156527 auc val 0.9912734909671232\n",
      "========================\n",
      "all loss avg 0.03164703817283613 0.03506750215252256\n",
      "all auc avg 0.9946832853686077 0.9921713772611412\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.06912962091204919 0.07798384222633009 auc 0.9909440040596726 0.9871119790675719\n",
      "1 fold: ls 0.06894564932411025 0.07058629494707462 auc 0.990963202359057 0.9900183194985784\n",
      "2 fold: ls 0.06938087867410038 0.07510520388658766 auc 0.9908023038581448 0.9887659887908586\n",
      "3 fold: ls 0.0669044433950994 0.07304076286664023 auc 0.9916273263230982 0.9891877359020496\n",
      "4 fold: ls 0.06901943632680967 0.07705237970723355 auc 0.9909170756408257 0.9881678093772681\n",
      "5 fold: ls 0.06755200290383222 0.07518385834526098 auc 0.9914469576535736 0.9880644114896856\n",
      "6 fold: ls 0.06992031340437828 0.0773916071199414 auc 0.9906253399807371 0.9880189907604627\n",
      "7 fold: ls 0.07079708138753416 0.0699798054709485 auc 0.9903200703373826 0.9904169218359788\n",
      "8 fold: ls 0.0685758399243408 0.07366847971341671 auc 0.9910784935704262 0.9894956603196997\n",
      "9 fold: ls 0.06695519402924777 0.07613121115159062 auc 0.9915907060327368 0.9882856262486183\n",
      "toxic 0.075 0.4 3\n",
      "this class avg train 0.06871804602815021 avg val 0.07461234454350243\n",
      "this class auc train 0.9910315479815655 auc val 0.9887533443290772\n",
      "========================\n",
      "0 fold: ls 0.017903172293940586 0.020897251503560876 auc 0.994269295254028 0.9915028801114064\n",
      "1 fold: ls 0.01760054057424087 0.0206891966702014 auc 0.9945086918821834 0.9915274085327257\n",
      "2 fold: ls 0.018017799968658453 0.020793217735618753 auc 0.9941960837921036 0.9913268293454867\n",
      "3 fold: ls 0.01798642404135441 0.019576293068831813 auc 0.9941885112066666 0.9927384004304343\n",
      "4 fold: ls 0.01792519970296767 0.021000862292151073 auc 0.994253326656388 0.9913153563742246\n",
      "5 fold: ls 0.017641262697699483 0.01972614199980236 auc 0.9944807258553963 0.9924849176832351\n",
      "6 fold: ls 0.017158030665086425 0.019028426955487173 auc 0.9949301912781645 0.9930330693312918\n",
      "7 fold: ls 0.017838619781890094 0.0204018478943776 auc 0.9943311509065007 0.9919676652242304\n",
      "8 fold: ls 0.01648712438242492 0.020730850669799017 auc 0.9954543108895588 0.9915826705412978\n",
      "9 fold: ls 0.018214675562324957 0.018669960058548547 auc 0.9940226540904995 0.9934861447699449\n",
      "severe_toxic 0.075 0.4 3\n",
      "this class avg train 0.01767728496705879 avg val 0.02015140488483786\n",
      "this class auc train 0.994463494181149 auc val 0.9920965342344277\n",
      "========================\n",
      "0 fold: ls 0.03488821663931895 0.03683901489693751 auc 0.9963223049079639 0.9957953828691706\n",
      "1 fold: ls 0.03497138347832159 0.03654431872034443 auc 0.9963055946080308 0.9957211491967611\n",
      "2 fold: ls 0.034419076014209744 0.03890705494789964 auc 0.9964163201672247 0.995372148314283\n",
      "3 fold: ls 0.035524502741727526 0.03357829397233658 auc 0.9961659566178973 0.9965121961151606\n",
      "4 fold: ls 0.03402992431772836 0.04184579470156114 auc 0.9965099511125312 0.994579095416942\n",
      "5 fold: ls 0.035040315062247314 0.03720048972220234 auc 0.9962782567044437 0.9956634642793376\n",
      "6 fold: ls 0.035789123819127196 0.03752810480170758 auc 0.9961324081478832 0.9954458387237228\n",
      "7 fold: ls 0.035221480345810054 0.03796993262971776 auc 0.9962387955661738 0.9955933761640892\n",
      "8 fold: ls 0.03462207794358659 0.03862827441650953 auc 0.9963852068656492 0.9952877293330117\n",
      "9 fold: ls 0.035211359411148314 0.03860946245516956 auc 0.996254707457843 0.9953452609143985\n",
      "obscene 0.075 0.4 3\n",
      "this class avg train 0.03497174597732257 avg val 0.037765074126438605\n",
      "this class auc train 0.996300950215564 auc val 0.9955315641326875\n",
      "========================\n",
      "0 fold: ls 0.004990249490075659 0.0066559773832055745 auc 0.9985446661281895 0.9968926775612821\n",
      "1 fold: ls 0.004868098195896126 0.006198683924712834 auc 0.9986881968740929 0.9970485019903625\n",
      "2 fold: ls 0.0051348001146550885 0.007187488162243416 auc 0.9983729392325872 0.9943038969201761\n",
      "3 fold: ls 0.003430585722959837 0.00639487126027406 auc 0.999587845335454 0.9936671066691809\n",
      "4 fold: ls 0.004862817518103965 0.006709317953882006 auc 0.9986650450745439 0.9966253378590735\n",
      "5 fold: ls 0.004587357610717635 0.0066527989761644615 auc 0.9988556925939497 0.9965506945753976\n",
      "6 fold: ls 0.004671121844114426 0.006188906313853121 auc 0.9987961172743739 0.9956130701699247\n",
      "7 fold: ls 0.005086123767523597 0.007321607826033143 auc 0.9983899071482587 0.9959470006495275\n",
      "8 fold: ls 0.004488928319099388 0.007124657305362967 auc 0.9989294193261687 0.9943067151873086\n",
      "9 fold: ls 0.0045310602481752815 0.007325319345283598 auc 0.9989123643803673 0.9940847078396678\n",
      "threat 0.075 0.4 3\n",
      "this class avg train 0.004665114283132101 avg val 0.006775962845101518\n",
      "this class auc train 0.9987742193367986 auc val 0.9955039709421902\n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.04964303462319662 0.049855262350365 auc 0.9918298576759376 0.991470525248537\n",
      "1 fold: ls 0.048738175748239236 0.05327538463273099 auc 0.9921642219751595 0.9900521668133405\n",
      "2 fold: ls 0.05097845663910874 0.054979824420283326 auc 0.9913190548440731 0.9891233114382181\n",
      "3 fold: ls 0.04764560117465539 0.05097872414886621 auc 0.9925681406283645 0.9909050222687712\n",
      "4 fold: ls 0.049573945956825974 0.05518398631959181 auc 0.9918353399833425 0.9891773497444861\n",
      "5 fold: ls 0.04865458548399649 0.05237936184561952 auc 0.9921926661951458 0.9905999010137224\n",
      "6 fold: ls 0.04835672286624806 0.05552788819068433 auc 0.9922873764282304 0.989531649004967\n",
      "7 fold: ls 0.049454954408126015 0.055150017757222745 auc 0.9918819328956269 0.9896573153818103\n",
      "8 fold: ls 0.050641004089096345 0.052421299267702046 auc 0.9914398696850006 0.9904412823484798\n",
      "9 fold: ls 0.04984429784189032 0.054342190610639086 auc 0.9917436795701485 0.9895776538169742\n",
      "insult 0.075 0.4 3\n",
      "this class avg train 0.049353077883138315 avg val 0.0534093939543705\n",
      "this class auc train 0.9919262139881029 auc val 0.9900536177079309\n",
      "========================\n",
      "0 fold: ls 0.01512022290476167 0.01708767886167267 auc 0.9952177200472968 0.9928889690013931\n",
      "1 fold: ls 0.015044922759565156 0.0174703840517532 auc 0.9952980766828115 0.9919294125137824\n",
      "2 fold: ls 0.014832615125737149 0.01728120482824934 auc 0.9954088286179511 0.9930450090283505\n",
      "3 fold: ls 0.015409189165066086 0.018138548736489422 auc 0.9950351481707778 0.9878779766989194\n",
      "4 fold: ls 0.014258160006348457 0.018666741720044486 auc 0.9959311648114622 0.9896233382073423\n",
      "5 fold: ls 0.015155549185992932 0.016227552099450204 auc 0.9952041101899061 0.9938497457527615\n",
      "6 fold: ls 0.014663562079140513 0.016420265366207755 auc 0.9956022885195012 0.9915126634872462\n",
      "7 fold: ls 0.014817751060426158 0.01861509398180717 auc 0.9954827812647944 0.9864879145892044\n",
      "8 fold: ls 0.015018896209229872 0.01694736972058495 auc 0.9953565989301547 0.9925288134980852\n",
      "9 fold: ls 0.01486364579002763 0.016499090874757556 auc 0.9954642720789237 0.9924561023195319\n",
      "identity_hate 0.075 0.4 3\n",
      "this class avg train 0.014918451428629562 avg val 0.017335393024101677\n",
      "this class auc train 0.995400098931358 auc val 0.9912199945096617\n",
      "========================\n",
      "all loss avg 0.03171728676123859 0.03500826222972543\n",
      "all auc avg 0.9946494207724229 0.9921931709759959\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.075 3 0.4 threat\n",
      "FIND BETTER PARAMS 0.075 3 0.4 insult\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.48\n",
      "0 fold: ls 0.06900051020253198 0.07821500418548313 auc 0.9909702713528323 0.9871220357656046\n",
      "1 fold: ls 0.06744115530865395 0.07043493605523644 auc 0.9914926237763634 0.9900937447338237\n",
      "2 fold: ls 0.07087328292256354 0.07519636090092015 auc 0.9902948288297375 0.9886817752699453\n",
      "3 fold: ls 0.06575886915417056 0.07332348732217413 auc 0.991994186062035 0.9890896151455684\n",
      "4 fold: ls 0.06702297846436119 0.07700641755739725 auc 0.9915661936388117 0.9882176271231925\n",
      "5 fold: ls 0.06780317043281896 0.07495502739778477 auc 0.9913541185312127 0.9881754248288745\n",
      "6 fold: ls 0.06794844841693994 0.07705020017916606 auc 0.9912922483486973 0.9881358970086324\n",
      "7 fold: ls 0.07156912256689753 0.07010187473541563 auc 0.9900622216167442 0.9903793406039645\n",
      "8 fold: ls 0.0663188820415637 0.07373059014188268 auc 0.9918068748749045 0.9895090789501897\n",
      "9 fold: ls 0.06708701555921963 0.0761400242973494 auc 0.9915436456690017 0.9883082475209648\n",
      "toxic 0.075 0.48 3\n",
      "this class avg train 0.06808234350697209 avg val 0.07461539227728095\n",
      "this class auc train 0.9912377212700341 auc val 0.9887712786950761\n",
      "========================\n",
      "0 fold: ls 0.01775203573263548 0.020983077297795933 auc 0.9943621660299895 0.9913822161033042\n",
      "1 fold: ls 0.017847859505212574 0.020685363089890704 auc 0.9942827028963988 0.9915808171920497\n",
      "2 fold: ls 0.018018159256769906 0.020838544671958163 auc 0.9942049576859121 0.9912864761362198\n",
      "3 fold: ls 0.017854339560826635 0.01951519720221519 auc 0.994283492013716 0.9927878528927712\n",
      "4 fold: ls 0.01790164597090271 0.021022110659911324 auc 0.9942741010275041 0.9913181257121156\n",
      "5 fold: ls 0.01796636625090785 0.01969030929843206 auc 0.9942078936439319 0.9925219417154151\n",
      "6 fold: ls 0.018034516124115968 0.01913531345265392 auc 0.9941591855619403 0.992957424047158\n",
      "7 fold: ls 0.017822379197437434 0.02028457314121108 auc 0.9943294268449431 0.9920703835574225\n",
      "8 fold: ls 0.017979363094951184 0.02081010705036082 auc 0.9942102339471208 0.9914556660905681\n",
      "9 fold: ls 0.017828299612887403 0.0186912764014704 auc 0.9943417915989262 0.993443146397911\n",
      "severe_toxic 0.075 0.48 3\n",
      "this class avg train 0.01790049643066471 avg val 0.020165587226589957\n",
      "this class auc train 0.9942655951250382 auc val 0.9920804049844936\n",
      "========================\n",
      "0 fold: ls 0.035218855939635106 0.03673729208638222 auc 0.9962475760637632 0.9958071287034126\n",
      "1 fold: ls 0.03570600455630999 0.03661778815599352 auc 0.9961499360659398 0.9957053314733153\n",
      "2 fold: ls 0.03517088396306734 0.038979122220116694 auc 0.9962537348435059 0.9953563295441374\n",
      "3 fold: ls 0.034282324722139185 0.033477795762850454 auc 0.9964470322002269 0.9965233945514517\n",
      "4 fold: ls 0.0339881419098254 0.04191372220121818 auc 0.9965214573680085 0.9945798002136317\n",
      "5 fold: ls 0.03435498355953051 0.03744254240953294 auc 0.9964371140980792 0.9956286159985717\n",
      "6 fold: ls 0.035822627894332125 0.0375266793867095 auc 0.9961211199311124 0.9954714463367801\n",
      "7 fold: ls 0.035256811143519085 0.03804794880071115 auc 0.9962337777751822 0.995575756246848\n",
      "8 fold: ls 0.03435864103472897 0.03858485731433472 auc 0.9964424335656848 0.9952891389263911\n",
      "9 fold: ls 0.03518059543632225 0.03875838336661736 auc 0.9962615556799808 0.9953091168877437\n",
      "obscene 0.075 0.48 3\n",
      "this class avg train 0.03493398701594099 avg val 0.037808613170446675\n",
      "this class auc train 0.9963115737591485 auc val 0.9955246058882284\n",
      "========================\n",
      "0 fold: ls 0.00445215760242965 0.006774251678543465 auc 0.9989897462492705 0.996806253928347\n",
      "1 fold: ls 0.004487652139964782 0.006307799960245797 auc 0.9989848411587123 0.9967394720301698\n",
      "2 fold: ls 0.005174437520676423 0.007190921284007753 auc 0.9983209322790527 0.9940223653886444\n",
      "3 fold: ls 0.00395021371453625 0.0064232945988731745 auc 0.9993025399745846 0.9941385379345025\n",
      "4 fold: ls 0.003954933022088513 0.0067246437592770585 auc 0.9992982358688902 0.996483908479477\n",
      "5 fold: ls 0.004744841435906045 0.0068346525295885485 auc 0.9987471641628141 0.9963136694114443\n",
      "6 fold: ls 0.0049309135310533 0.006363269816254135 auc 0.9985847044752305 0.9952699729712742\n",
      "7 fold: ls 0.00495426815024161 0.007308540082984708 auc 0.9985569551749318 0.9958645001780962\n",
      "8 fold: ls 0.004416093139667448 0.00714131496917221 auc 0.9990136812911733 0.9945006372680792\n",
      "9 fold: ls 0.005194616263489519 0.007343025660622192 auc 0.9983386781345905 0.9936085956965346\n",
      "threat 0.075 0.48 3\n",
      "this class avg train 0.004626012652005354 avg val 0.006841171433956905\n",
      "this class auc train 0.998813747876925 auc val 0.995374791328657\n",
      "========================\n",
      "0 fold: ls 0.049006760196562735 0.04984477888650496 auc 0.9920779382470994 0.9914867541802048\n",
      "1 fold: ls 0.0493062744559082 0.053182154076254165 auc 0.9919479918962834 0.9901233566115328\n",
      "2 fold: ls 0.05110285767427941 0.05503341457546144 auc 0.9912957782722425 0.9887743057530725\n",
      "3 fold: ls 0.047647706091884866 0.05103712099545973 auc 0.992572063884085 0.9908350036305961\n",
      "4 fold: ls 0.05058166328138552 0.05534433391967904 auc 0.9914635617894947 0.9891505786079209\n",
      "5 fold: ls 0.047218950389785776 0.05233132510692162 auc 0.9927312972591218 0.9906231584386137\n",
      "6 fold: ls 0.049157991521635275 0.05580081064151362 auc 0.9919699628773558 0.9893918534762154\n",
      "7 fold: ls 0.05066158779935533 0.05509440380073164 auc 0.9914140463638949 0.9896992403168269\n",
      "8 fold: ls 0.05000149098111001 0.05233283821260413 auc 0.991679607856608 0.9903039897041407\n",
      "9 fold: ls 0.049581344013697216 0.05443573216572771 auc 0.9918416281213447 0.9894871864247312\n",
      "insult 0.075 0.48 3\n",
      "this class avg train 0.049426662640560434 avg val 0.05344369123808581\n",
      "this class auc train 0.9918993876567528 auc val 0.9899875427143854\n",
      "========================\n",
      "0 fold: ls 0.014385904726138257 0.01708206325278433 auc 0.9958255260004139 0.992969230969282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold: ls 0.015159778320858072 0.017388537586225473 auc 0.9951875804038763 0.9922939543008982\n",
      "2 fold: ls 0.014772610609073979 0.01718445722987947 auc 0.995476788706416 0.9931952199738409\n",
      "3 fold: ls 0.014045528419067846 0.018196264636919892 auc 0.9961336723141937 0.9895072049688884\n",
      "4 fold: ls 0.014641194824773072 0.018612900800294582 auc 0.9956428513056383 0.9893529585054593\n",
      "5 fold: ls 0.014671101650000726 0.016259740318928895 auc 0.9956214777703422 0.993722396336672\n",
      "6 fold: ls 0.014050255678070675 0.01631542972450636 auc 0.9960837217180278 0.9920911915600839\n",
      "7 fold: ls 0.01510486453413881 0.018617819463198848 auc 0.9952407485308022 0.9876366157236794\n",
      "8 fold: ls 0.013839593832324831 0.01681140004173444 auc 0.996239580947145 0.992537845942626\n",
      "9 fold: ls 0.015527366560260719 0.016654931559284318 auc 0.9949141860720181 0.9923851976298865\n",
      "identity_hate 0.075 0.48 3\n",
      "this class avg train 0.0146198199154707 avg val 0.017312354461375662\n",
      "this class auc train 0.9956366133768875 auc val 0.9915691815911316\n",
      "========================\n",
      "all loss avg 0.03159822036026905 0.03503113496795599\n",
      "all auc avg 0.9946941065107976 0.9922179675336621\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.56\n",
      "0 fold: ls 0.06985858655375787 0.07814637306925676 auc 0.9906837127310523 0.9871371208126536\n",
      "1 fold: ls 0.06888766447322596 0.07043931351463824 auc 0.9910004446651107 0.990069373096249\n",
      "2 fold: ls 0.06902072078454252 0.07509342225148484 auc 0.9909294288061224 0.9887335989751229\n",
      "3 fold: ls 0.0688849640813572 0.07308750951164844 auc 0.9910026088940109 0.9891859238843862\n",
      "4 fold: ls 0.07031305933406876 0.0775035192958694 auc 0.9905004767377722 0.9880729788727427\n",
      "5 fold: ls 0.06823044954127432 0.07513300216711327 auc 0.9912209179056026 0.9882373003731751\n",
      "6 fold: ls 0.06737745417966547 0.07704432387125287 auc 0.9914556861132872 0.9881241565207395\n",
      "7 fold: ls 0.07030547937521633 0.0701734329033864 auc 0.990497886227941 0.99037997526892\n",
      "8 fold: ls 0.06991204288864428 0.07382308315046006 auc 0.9906072862579571 0.9894566737581408\n",
      "9 fold: ls 0.06700988281293323 0.07627173683453194 auc 0.9915732833758141 0.9882265117413244\n",
      "toxic 0.075 0.56 3\n",
      "this class avg train 0.0689800304024686 avg val 0.07467157165696423\n",
      "this class auc train 0.9909471731714671 auc val 0.9887623613303453\n",
      "========================\n",
      "0 fold: ls 0.017401294501536664 0.020794894016401232 auc 0.9946530929992403 0.9915938726421065\n",
      "1 fold: ls 0.017439534623301854 0.020656322993374585 auc 0.9946464271635158 0.991636203949867\n",
      "2 fold: ls 0.01799524731049345 0.020765471274118246 auc 0.9942039357544792 0.9913236643878973\n",
      "3 fold: ls 0.01784874381246894 0.019551984047090458 auc 0.994310689603742 0.9927656981896442\n",
      "4 fold: ls 0.01804798315406391 0.02107012406893094 auc 0.9941785296078043 0.991284893657425\n",
      "5 fold: ls 0.01810816169312992 0.01972882508724107 auc 0.9941045569775201 0.9925191549603047\n",
      "6 fold: ls 0.01814806509366967 0.019177309266957144 auc 0.9940930859234468 0.9929494613856703\n",
      "7 fold: ls 0.01777762778318259 0.020336820751843756 auc 0.994376216504202 0.9919983214709585\n",
      "8 fold: ls 0.017805645692933095 0.02068004855294813 auc 0.9943428911779594 0.9915848602732069\n",
      "9 fold: ls 0.01823673250313896 0.01866327495766849 auc 0.994022801027564 0.9935000794275484\n",
      "severe_toxic 0.075 0.56 3\n",
      "this class avg train 0.017880903616791906 avg val 0.020142507501657407\n",
      "this class auc train 0.9942932226739473 auc val 0.9921156210344628\n",
      "========================\n",
      "0 fold: ls 0.03512019861897117 0.036836304250817846 auc 0.9962663497695555 0.9957813661736418\n",
      "1 fold: ls 0.03535812904836048 0.036599228571941526 auc 0.9962221301311058 0.9957256126137732\n",
      "2 fold: ls 0.03433159100073266 0.03888596892205146 auc 0.9964370246586984 0.9953719916927963\n",
      "3 fold: ls 0.034383347855730835 0.033632964322120165 auc 0.9964270103487842 0.9964796188459503\n",
      "4 fold: ls 0.03455927663572802 0.042116266165022516 auc 0.9963954957786353 0.994515742025617\n",
      "5 fold: ls 0.034038070826893876 0.037402344940438446 auc 0.9965035627234606 0.9956307303886406\n",
      "6 fold: ls 0.03586094254370102 0.03751119168887229 auc 0.9961195574009569 0.9954960359101744\n",
      "7 fold: ls 0.03525840964042672 0.03798553726674411 auc 0.9962343994997429 0.9955992103144646\n",
      "8 fold: ls 0.03521643372279692 0.03861882093701766 auc 0.9962557595577051 0.995275277924828\n",
      "9 fold: ls 0.03515705682365998 0.03870211468872423 auc 0.9962623092889533 0.9953364013156738\n",
      "obscene 0.075 0.56 3\n",
      "this class avg train 0.03492834567170017 avg val 0.03782907417537503\n",
      "this class auc train 0.9963123599157597 auc val 0.995521198720556\n",
      "========================\n",
      "0 fold: ls 0.004911674103526912 0.006738305036425995 auc 0.9986076689304255 0.9967499476220407\n",
      "1 fold: ls 0.00442535445092741 0.006283316807031964 auc 0.9990348341015539 0.9967041169076053\n",
      "2 fold: ls 0.004821795561345657 0.00719816448885445 auc 0.9987533761072389 0.9936950031426774\n",
      "3 fold: ls 0.004196191977493129 0.006363478240311931 auc 0.9991679595225659 0.9938975841766715\n",
      "4 fold: ls 0.0038260160848951254 0.006673494875771091 auc 0.9993566417710696 0.9968322438032979\n",
      "5 fold: ls 0.0051719393111713755 0.006891453521112672 auc 0.9982881434719221 0.9963490267563433\n",
      "6 fold: ls 0.004444878638474252 0.006259313806421016 auc 0.998954906290686 0.9955986653257066\n",
      "7 fold: ls 0.005005006792584117 0.0072851952416776574 auc 0.9984940746270209 0.9961185492488529\n",
      "8 fold: ls 0.005156912158861513 0.007200034806934409 auc 0.9983612101081172 0.9941475653417106\n",
      "9 fold: ls 0.004256254707257867 0.007163762505960238 auc 0.9991130698993186 0.9944605154582646\n",
      "threat 0.075 0.56 3\n",
      "this class avg train 0.004621602378653736 avg val 0.006805651933050143\n",
      "this class auc train 0.9988131884829918 auc val 0.995455321778317\n",
      "========================\n",
      "0 fold: ls 0.049156792648059194 0.049547187546405344 auc 0.9920117440268014 0.9916057105762444\n",
      "1 fold: ls 0.051255415977296784 0.053272897404022454 auc 0.9912066634352348 0.9898288935214774\n",
      "2 fold: ls 0.05121114420777992 0.055100496306509164 auc 0.9912639034985475 0.9887535594899095\n",
      "3 fold: ls 0.048947422978618156 0.051317649830815895 auc 0.9920800610621844 0.9907111952859136\n",
      "4 fold: ls 0.05066828858649508 0.055364832742236025 auc 0.9914311081160815 0.9891104219030731\n",
      "5 fold: ls 0.045206912099404105 0.05228216766100923 auc 0.9934451251541181 0.9906322773570062\n",
      "6 fold: ls 0.04791302548047268 0.05570526098657966 auc 0.9924471877413674 0.9894171187363489\n",
      "7 fold: ls 0.04857328637333368 0.05511858211288227 auc 0.9922079280467807 0.9896417348864798\n",
      "8 fold: ls 0.050060659224210005 0.052413960358546105 auc 0.9916713781968254 0.9904319843109438\n",
      "9 fold: ls 0.04946853932772587 0.054330776041244803 auc 0.9918837242878015 0.9895293207750073\n",
      "insult 0.075 0.56 3\n",
      "this class avg train 0.04924614869033954 avg val 0.0534453810990251\n",
      "this class auc train 0.9919648823565742 auc val 0.9899662216842404\n",
      "========================\n",
      "0 fold: ls 0.014134166904697074 0.01693263594039022 auc 0.9960194301160876 0.993086709380382\n",
      "1 fold: ls 0.014255106478021065 0.017312646891697314 auc 0.9959520508118264 0.9922612217665076\n",
      "2 fold: ls 0.013296194890390054 0.01714213577022782 auc 0.9966243154749546 0.9930400767286477\n",
      "3 fold: ls 0.01529803487519017 0.018188950583602276 auc 0.9951362351898361 0.988256194407938\n",
      "4 fold: ls 0.014466385181846184 0.018494796495265062 auc 0.9957745253664754 0.9895749119920796\n",
      "5 fold: ls 0.015110155263460536 0.01631069857138313 auc 0.9952312355284902 0.9937056873707314\n",
      "6 fold: ls 0.014080104889561986 0.016151772289440997 auc 0.9960379402213828 0.9924271984970012\n",
      "7 fold: ls 0.01523267695070668 0.01855682043699465 auc 0.9951550415040589 0.9884188254209119\n",
      "8 fold: ls 0.013449853405638559 0.01673053400345092 auc 0.9965409424801921 0.9927731411229135\n",
      "9 fold: ls 0.014852245007156335 0.016454437045759423 auc 0.9954783275092983 0.9924515860972614\n",
      "identity_hate 0.075 0.56 3\n",
      "this class avg train 0.014417492384666867 avg val 0.01722754280282118\n",
      "this class auc train 0.9957950044202603 auc val 0.9915995552784374\n",
      "========================\n",
      "all loss avg 0.03167908719077014 0.035020288194815515\n",
      "all auc avg 0.9946876385035001 0.992236713304393\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.075 3 0.56 identity_hate\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.06977040812238773 0.07813686865585732 auc 0.9907088565051761 0.9870724770825064\n",
      "1 fold: ls 0.06719504734725641 0.07031744553946755 auc 0.9915465666781276 0.9901108683007442\n",
      "2 fold: ls 0.0714291865971861 0.07520362773084804 auc 0.9901083271876058 0.9886753879076814\n",
      "3 fold: ls 0.06363302765107588 0.0729142339628574 auc 0.9926478094873569 0.9892821420223205\n",
      "4 fold: ls 0.06882686256897019 0.07721411668362818 auc 0.9909576827561396 0.9881589700137967\n",
      "5 fold: ls 0.06519716778950178 0.07483233914177997 auc 0.9921943836985853 0.9881977725529333\n",
      "6 fold: ls 0.06884743802393967 0.0769723584790323 auc 0.9910137768590395 0.9881713904527258\n",
      "7 fold: ls 0.07181699704085817 0.07030531122273023 auc 0.9899668266816009 0.99031410611317\n",
      "8 fold: ls 0.0671012284265956 0.07369377328258528 auc 0.9915394960319831 0.9895506495047822\n",
      "9 fold: ls 0.06869002389920277 0.0763340558031077 auc 0.9910364889158618 0.9882269197402244\n",
      "toxic 0.075 0.62 3\n",
      "this class avg train 0.06825073874669743 avg val 0.0745924130501894\n",
      "this class auc train 0.9911720214801477 auc val 0.9887760683690884\n",
      "========================\n",
      "0 fold: ls 0.01769752688181794 0.020915461472195186 auc 0.9944199235360023 0.9914356247626283\n",
      "1 fold: ls 0.017039052999567046 0.02065035135215348 auc 0.9949976578899993 0.9916852607925054\n",
      "2 fold: ls 0.017943367733111847 0.020825982288057186 auc 0.9942504863241801 0.9912682776300796\n",
      "3 fold: ls 0.01795234206315762 0.019526142145900937 auc 0.9942131184333246 0.9927929959488543\n",
      "4 fold: ls 0.018003669489845095 0.021036171693193318 auc 0.9941963656197171 0.9913782599063172\n",
      "5 fold: ls 0.017156110857169327 0.01965183881823083 auc 0.994881171356036 0.9925454300799162\n",
      "6 fold: ls 0.01780705675019418 0.019069455377800675 auc 0.9943653382635225 0.9929697661724641\n",
      "7 fold: ls 0.01787862580818082 0.020349212875795452 auc 0.9942819686220404 0.9920114598624131\n",
      "8 fold: ls 0.01753255197497127 0.02072373048557913 auc 0.9945714297413275 0.9915313113747017\n",
      "9 fold: ls 0.018035857084828726 0.01871423878643549 auc 0.9941776628978094 0.9934156752157783\n",
      "severe_toxic 0.075 0.62 3\n",
      "this class avg train 0.017704616164284384 avg val 0.020146258529534167\n",
      "this class auc train 0.9944355122683961 auc val 0.9921034061745659\n",
      "========================\n",
      "0 fold: ls 0.03551073105732471 0.03691400748415436 auc 0.9961870449796061 0.9957712647561937\n",
      "1 fold: ls 0.035263031041669425 0.036672289489931696 auc 0.9962435797861553 0.995697266000469\n",
      "2 fold: ls 0.035122222215786175 0.03902171066899426 auc 0.9962625724212422 0.9953416854351415\n",
      "3 fold: ls 0.03576167473175479 0.033613670214590774 auc 0.9961226022075533 0.9964883504938277\n",
      "4 fold: ls 0.03459814783285086 0.042147190889446406 auc 0.9963876275304043 0.9945002364984448\n",
      "5 fold: ls 0.03406836052620391 0.037425109984245086 auc 0.9964913564235325 0.99565876563474\n",
      "6 fold: ls 0.035924758168543146 0.03753200084179352 auc 0.9961075522187716 0.9955047284026801\n",
      "7 fold: ls 0.03496033588144503 0.037911071964926206 auc 0.9962980252248835 0.9956184356019434\n",
      "8 fold: ls 0.03434675097644332 0.03850127092600837 auc 0.9964431234575575 0.9953152947146514\n",
      "9 fold: ls 0.03462881502771717 0.0388401513039361 auc 0.9963785294241766 0.9953121746253566\n",
      "obscene 0.075 0.62 3\n",
      "this class avg train 0.035018482745973856 avg val 0.03785784737680268\n",
      "this class auc train 0.9962922013673884 auc val 0.9955208202163448\n",
      "========================\n",
      "0 fold: ls 0.005143544926671865 0.006809150186197405 auc 0.9984190016061734 0.9967237586423633\n",
      "1 fold: ls 0.004624218444396237 0.006291466369883559 auc 0.9988823052756197 0.9968075633773308\n",
      "2 fold: ls 0.004841120662553969 0.007130061522655927 auc 0.9987400576494319 0.9940433165723863\n",
      "3 fold: ls 0.004197924304801309 0.0063996097291747645 auc 0.9991723285958936 0.9932113897793701\n",
      "4 fold: ls 0.004207426278580202 0.0066845673606603815 auc 0.9991462278492855 0.9967995055209838\n",
      "5 fold: ls 0.004913776683177708 0.006808418801680129 auc 0.998618520128272 0.9963254551930774\n",
      "6 fold: ls 0.0049208068882653265 0.006340058339314099 auc 0.9985885456865389 0.9954153309447482\n",
      "7 fold: ls 0.005032743168182167 0.007371955453648884 auc 0.9984247866462929 0.9958160475202715\n",
      "8 fold: ls 0.004778331832711188 0.007083581386907082 auc 0.9987627021671399 0.9947346811586644\n",
      "9 fold: ls 0.004475487584856116 0.007306364100239969 auc 0.9989571042429399 0.9943508491781047\n",
      "threat 0.075 0.62 3\n",
      "this class avg train 0.004713538077419609 avg val 0.00682252332503622\n",
      "this class auc train 0.9987711579847588 auc val 0.99542278978873\n",
      "========================\n",
      "0 fold: ls 0.050113705582891695 0.049761433692578946 auc 0.9916399501941394 0.9914897657345347\n",
      "1 fold: ls 0.05114695557275142 0.05332316700713937 auc 0.9912333461204399 0.989683460543619\n",
      "2 fold: ls 0.0511389676660549 0.05504750414199245 auc 0.991289606661387 0.9887373723853853\n",
      "3 fold: ls 0.04770596763140062 0.05121423666720006 auc 0.992548704652139 0.9907672436581685\n",
      "4 fold: ls 0.05071350725072441 0.05548953920042936 auc 0.9914170740811834 0.989153088401974\n",
      "5 fold: ls 0.04744344769346994 0.05230432865595969 auc 0.9926464059425574 0.9906374642647158\n",
      "6 fold: ls 0.04946028357942747 0.05580026517107614 auc 0.9918781813740278 0.9893869175479112\n",
      "7 fold: ls 0.04998817153035762 0.05516142309646006 auc 0.9916794497925927 0.989642656313623\n",
      "8 fold: ls 0.0496530721407652 0.05229388956303774 auc 0.9918092575407769 0.9905310796118915\n",
      "9 fold: ls 0.04886271249986139 0.05445274014863209 auc 0.9921153908636416 0.9894083625209342\n",
      "insult 0.075 0.62 3\n",
      "this class avg train 0.04962267911477046 avg val 0.0534848527344506\n",
      "this class auc train 0.9918257367222886 auc val 0.9899437410982757\n",
      "========================\n",
      "0 fold: ls 0.015133814005434813 0.01715671661652796 auc 0.9952280324405047 0.9928096038152684\n",
      "1 fold: ls 0.014733154893428788 0.017334222733093683 auc 0.9955934068399016 0.9924414749010961\n",
      "2 fold: ls 0.015015779520824198 0.017274385074160247 auc 0.9952921187816963 0.9930261766113038\n",
      "3 fold: ls 0.01522629565198611 0.018179513666089747 auc 0.9951692676387888 0.9882257038279579\n",
      "4 fold: ls 0.014997675673644085 0.01864657119238814 auc 0.9953344576722695 0.9906902843112066\n",
      "5 fold: ls 0.015238398695884907 0.016341466987885495 auc 0.9951275655887439 0.9936140138548939\n",
      "6 fold: ls 0.013230237989832297 0.016235597705378295 auc 0.9966805471677419 0.992287647228846\n",
      "7 fold: ls 0.014779398163867195 0.018500927422821976 auc 0.995506632735936 0.987339222487174\n",
      "8 fold: ls 0.013253690445128417 0.01658697846596904 auc 0.9966852230556815 0.9929592094804538\n",
      "9 fold: ls 0.015480370767204757 0.01659378106422569 auc 0.9949419276375798 0.9922858407399378\n",
      "identity_hate 0.075 0.62 3\n",
      "this class avg train 0.014708881580723556 avg val 0.017285016092854027\n",
      "this class auc train 0.9955559179558844 auc val 0.9915679177258138\n",
      "========================\n",
      "all loss avg 0.031669822738311544 0.03503148518481119\n",
      "all auc avg 0.9946754246298107 0.992222457228803\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.05 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.07054908247034279 0.07799381380216308 auc 0.9904564854821903 0.9871499408376233\n",
      "1 fold: ls 0.07026490907419657 0.0708493844601407 auc 0.9905569248947377 0.9899895084177281\n",
      "2 fold: ls 0.07010288855777674 0.07511858865400087 auc 0.9905634430396628 0.9887420248572584\n",
      "3 fold: ls 0.06708963346279326 0.07305350836513544 auc 0.9915805911764245 0.9891753688814958\n",
      "4 fold: ls 0.06853288965246013 0.07729230316955127 auc 0.9910896922471606 0.9880766959384076\n",
      "5 fold: ls 0.06850396201558229 0.07522622483776802 auc 0.9911423516975264 0.9881758327994962\n",
      "6 fold: ls 0.06852703429135769 0.07702614104935028 auc 0.9911058203997253 0.9881049365714475\n",
      "7 fold: ls 0.0691254261083487 0.06993777341552614 auc 0.9909102763380195 0.9904143831761563\n",
      "8 fold: ls 0.06903230689714812 0.07367716090582431 auc 0.9908818403866766 0.9894666017313751\n",
      "9 fold: ls 0.07106641394238877 0.0766025842487476 auc 0.990227325082827 0.9880566935324875\n",
      "toxic 0.05 0.4 3\n",
      "this class avg train 0.0692794546472395 avg val 0.07467774829082077\n",
      "this class auc train 0.9908514750744949 auc val 0.9887351986743476\n",
      "========================\n",
      "0 fold: ls 0.017707374587083977 0.020862676935186803 auc 0.9944106232247617 0.9914969458159261\n",
      "1 fold: ls 0.017680734962496992 0.020635268762648998 auc 0.994440619485522 0.991599806937587\n",
      "2 fold: ls 0.01803320960861925 0.020901571976233383 auc 0.994181171435345 0.9912492878845424\n",
      "3 fold: ls 0.01790974252298061 0.019584128333517727 auc 0.9942450090657817 0.992722971262185\n",
      "4 fold: ls 0.017996885607474606 0.02093705438882468 auc 0.994208464601692 0.9914209868337764\n",
      "5 fold: ls 0.01813067308532558 0.019795222237375433 auc 0.9940900296975611 0.9924662066132088\n",
      "6 fold: ls 0.018211288467261354 0.019112941838697696 auc 0.9940508831495531 0.9930366525289611\n",
      "7 fold: ls 0.01791436455479075 0.020299089103250335 auc 0.9942687222456703 0.992051273169852\n",
      "8 fold: ls 0.01789708033994749 0.02079115212157893 auc 0.994273745044327 0.9914479024956177\n",
      "9 fold: ls 0.01826349127682501 0.018811833804515114 auc 0.9939902373251059 0.993356751520769\n",
      "severe_toxic 0.05 0.4 3\n",
      "this class avg train 0.01797448450128056 avg val 0.020173093950182908\n",
      "this class auc train 0.9942159505275319 auc val 0.9920848785062425\n",
      "========================\n",
      "0 fold: ls 0.03545082075652091 0.03678377319540139 auc 0.996201312841301 0.9957765112288218\n",
      "1 fold: ls 0.03487944692203939 0.036565881701585264 auc 0.996326771591828 0.9957079155568485\n",
      "2 fold: ls 0.03395390803664268 0.03887959212924744 auc 0.9965214525334473 0.9953734795969189\n",
      "3 fold: ls 0.035569849861638324 0.03364486941910705 auc 0.9961630360595217 0.9965125093581338\n",
      "4 fold: ls 0.034564967379243174 0.041866169731449084 auc 0.9963935484174141 0.9945752581905207\n",
      "5 fold: ls 0.03467752949047243 0.037212506692146034 auc 0.9963641523519501 0.9956494466562879\n",
      "6 fold: ls 0.03568966277625004 0.037554386815689816 auc 0.9961482340837255 0.9953890634348346\n",
      "7 fold: ls 0.03538305467070441 0.03805387204588856 auc 0.9962141644441544 0.9955849969145567\n",
      "8 fold: ls 0.035029497665975 0.038744201228392526 auc 0.9963021998684148 0.9952532726059622\n",
      "9 fold: ls 0.03528873288871837 0.038662798545416155 auc 0.9962387294007706 0.9953447120896987\n",
      "obscene 0.05 0.4 3\n",
      "this class avg train 0.03504874704482047 avg val 0.037796805150432336\n",
      "this class auc train 0.9962873601592529 auc val 0.9955167165632585\n",
      "========================\n",
      "0 fold: ls 0.0050225554545652714 0.006662884791130871 auc 0.9985038986536825 0.9968743452755081\n",
      "1 fold: ls 0.004925974628449812 0.006299224582185927 auc 0.9986382526573165 0.9967159019484602\n",
      "2 fold: ls 0.004898929559413961 0.007063769947375628 auc 0.9986640937138666 0.9943994866959983\n",
      "3 fold: ls 0.003803808904195181 0.006345560498942313 auc 0.9994002837786214 0.9938006788610221\n",
      "4 fold: ls 0.004010196714250638 0.006677172892105563 auc 0.999266694083008 0.9965428373876422\n",
      "5 fold: ls 0.0048473626028545675 0.006841934209392931 auc 0.9986988769694938 0.9963830745699499\n",
      "6 fold: ls 0.0045346512049722456 0.006170665380914092 auc 0.9988934875145202 0.9952568776583485\n",
      "7 fold: ls 0.005072051717684168 0.007292209752773499 auc 0.998336926043445 0.996046525027762\n",
      "8 fold: ls 0.004598135835415044 0.0070467282731716705 auc 0.9988872559352183 0.9947600916382136\n",
      "9 fold: ls 0.005057213182634593 0.007285694195779784 auc 0.9984672019375587 0.993917533632107\n",
      "threat 0.05 0.4 3\n",
      "this class avg train 0.004677087980443548 avg val 0.006768584452377229\n",
      "this class auc train 0.998775697128673 auc val 0.995469735269501\n",
      "========================\n",
      "0 fold: ls 0.04978169581875111 0.04988761376791332 auc 0.9917829077637228 0.9914588973026512\n",
      "1 fold: ls 0.05107482880116802 0.053364922335387456 auc 0.9912735339186007 0.9896712470177247\n",
      "2 fold: ls 0.05112544454700879 0.05494836633555058 auc 0.9912806344225016 0.9891369889141339\n",
      "3 fold: ls 0.049547172732450614 0.051127409022393586 auc 0.991850581600084 0.9908449584907429\n",
      "4 fold: ls 0.05060872233551314 0.05519387620609777 auc 0.9914523242291374 0.9891451407208062\n",
      "5 fold: ls 0.04658700714746986 0.05229207336242348 auc 0.9929787560668528 0.9905839219915851\n",
      "6 fold: ls 0.04917283572280507 0.05565361806389703 auc 0.991969352746849 0.9894454794091476\n",
      "7 fold: ls 0.04900834150355503 0.054948816996027966 auc 0.9920512308202102 0.9897425892756101\n",
      "8 fold: ls 0.04991542831565033 0.052198622965890694 auc 0.991716779244232 0.9904431252027663\n",
      "9 fold: ls 0.04906751755351007 0.054243210554475244 auc 0.99204374964003 0.9895764810915193\n",
      "insult 0.05 0.4 3\n",
      "this class avg train 0.04958889944778821 avg val 0.05338585296100571\n",
      "this class auc train 0.991839985045222 auc val 0.9900048829416688\n",
      "========================\n",
      "0 fold: ls 0.014682966410413039 0.01709608293116958 auc 0.9956022075268567 0.9928907625649215\n",
      "1 fold: ls 0.014925855414692777 0.0175533979301414 auc 0.9954222561264341 0.9918594635361808\n",
      "2 fold: ls 0.014456908844716822 0.017307207856897464 auc 0.9957482038928926 0.992997927985734\n",
      "3 fold: ls 0.015079443083969165 0.018183772550931534 auc 0.9953095589810423 0.9888252024372736\n",
      "4 fold: ls 0.0141665454122534 0.018663304620074036 auc 0.9960208834660329 0.9901762041649236\n",
      "5 fold: ls 0.015177818076116862 0.01632500726004535 auc 0.9951902018245699 0.9937413632709833\n",
      "6 fold: ls 0.01446941502933701 0.01638469978139838 auc 0.9957618184872503 0.9917416359563552\n",
      "7 fold: ls 0.015243123891913365 0.01857919128668633 auc 0.9951615000048591 0.9883978249873544\n",
      "8 fold: ls 0.0136157760557169 0.016886570665538066 auc 0.9964484631905716 0.9925830081653298\n",
      "9 fold: ls 0.015079643488830034 0.01649752127206618 auc 0.9952964454644025 0.9924014560300599\n",
      "identity_hate 0.05 0.4 3\n",
      "this class avg train 0.014689749570795937 avg val 0.017347675615494832\n",
      "this class auc train 0.9955961538964913 auc val 0.9915614849099115\n",
      "========================\n",
      "all loss avg 0.031876403865394705 0.0350249600700523\n",
      "all auc avg 0.9945944369719444 0.9922288161441549\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.05 max depth 3 feature fraction 0.48\n",
      "0 fold: ls 0.07018118058794508 0.07810500630177115 auc 0.990591599277178 0.9871541084782496\n",
      "1 fold: ls 0.07078245067742589 0.07056840282591778 auc 0.9904074179704261 0.9900570513761369\n",
      "2 fold: ls 0.06825131549910922 0.0749228870493857 auc 0.9911848069770461 0.9888331240452931\n",
      "3 fold: ls 0.0676322970879674 0.07330525677436579 auc 0.991418012712412 0.9891244964855918\n",
      "4 fold: ls 0.0690067206163232 0.0772386964313421 auc 0.9909351720467892 0.9881263776941248\n",
      "5 fold: ls 0.0637801029527547 0.07479847165626277 auc 0.9926191913037037 0.9882910165050407\n",
      "6 fold: ls 0.0674049480133374 0.07703537973963935 auc 0.9914852004030935 0.9881279189164736\n",
      "7 fold: ls 0.07185889849108434 0.07014099969621722 auc 0.9899722471694337 0.9903656499742077\n",
      "8 fold: ls 0.06810158583731533 0.07371059090198269 auc 0.9912163860589093 0.9894466097852733\n",
      "9 fold: ls 0.068768827612783 0.07623991800339548 auc 0.9910218795863959 0.9882300477317914\n",
      "toxic 0.05 0.48 3\n",
      "this class avg train 0.06857683273760456 avg val 0.074606560938028\n",
      "this class auc train 0.9910851913505387 auc val 0.9887756400992183\n",
      "========================\n",
      "0 fold: ls 0.017763095616992894 0.020802906753490034 auc 0.9943611833093551 0.9915495632358526\n",
      "1 fold: ls 0.017934947559570077 0.020680023685370807 auc 0.9942293835626275 0.9915812128117484\n",
      "2 fold: ls 0.018032894519692692 0.02083846086786065 auc 0.9941851562327304 0.9913256424863905\n",
      "3 fold: ls 0.01795281451236976 0.019597871373624252 auc 0.9942169708942655 0.99272455374098\n",
      "4 fold: ls 0.017967682389297728 0.021032717734600442 auc 0.9942238229812823 0.9913193125712116\n",
      "5 fold: ls 0.016171105176017508 0.019687733309613927 auc 0.9957408522903398 0.9924900930855829\n",
      "6 fold: ls 0.017947104128463592 0.01914770949630395 auc 0.9942471665782987 0.9929363229942156\n",
      "7 fold: ls 0.017248677795961734 0.02035281428476148 auc 0.994842006998867 0.9920265889192399\n",
      "8 fold: ls 0.017743278809113965 0.020624755304669982 auc 0.9944004243855891 0.9916079519915213\n",
      "9 fold: ls 0.018079946889669227 0.01874729544408326 auc 0.9941401645589327 0.9933599365853638\n",
      "severe_toxic 0.05 0.48 3\n",
      "this class avg train 0.01768415473971492 avg val 0.02015122882543788\n",
      "this class auc train 0.994458713179229 auc val 0.9920921178422105\n",
      "========================\n",
      "0 fold: ls 0.03545548938743491 0.036769047269220104 auc 0.9961964178121717 0.995794286591308\n",
      "1 fold: ls 0.03437221717297121 0.036617850386478946 auc 0.9964414090646267 0.9956866947496512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 fold: ls 0.03408746089991449 0.03887395324466806 auc 0.9964901178089721 0.9953713652068501\n",
      "3 fold: ls 0.035884110936046903 0.03364337157223497 auc 0.996090500721684 0.99650338615654\n",
      "4 fold: ls 0.034644141352200404 0.04207084663045394 auc 0.9963759815561106 0.9945074410868279\n",
      "5 fold: ls 0.034644973771366536 0.03737271456663951 auc 0.9963709835868196 0.9956301822134376\n",
      "6 fold: ls 0.03357973791042103 0.037449149324098166 auc 0.9966103949213746 0.9954679223533318\n",
      "7 fold: ls 0.03527977207618144 0.03802374898528234 auc 0.9962342051503856 0.995583861408779\n",
      "8 fold: ls 0.03463678935379216 0.03867325940987267 auc 0.9963832711073748 0.995274259885165\n",
      "9 fold: ls 0.03518953163301317 0.03862357182912659 auc 0.9962596472462263 0.9953482402484827\n",
      "obscene 0.05 0.48 3\n",
      "this class avg train 0.034777422449334225 avg val 0.03781175132180752\n",
      "this class auc train 0.9963452928975747 auc val 0.9955167639900372\n",
      "========================\n",
      "0 fold: ls 0.00467762539499971 0.006701278000037373 auc 0.9988255394097227 0.9968992248062015\n",
      "1 fold: ls 0.004931295216522102 0.006224628473504225 auc 0.9986362061625804 0.9969136287450242\n",
      "2 fold: ls 0.004828559974720627 0.007085642721335938 auc 0.9987222239095878 0.9942096165933376\n",
      "3 fold: ls 0.004498950047326107 0.006351491064581108 auc 0.9989739499281458 0.993678892450814\n",
      "4 fold: ls 0.004334449356612078 0.006678176597973495 auc 0.9990660253061924 0.9966868858298238\n",
      "5 fold: ls 0.004540820317247336 0.006796376944438216 auc 0.9989274818285531 0.99643938441553\n",
      "6 fold: ls 0.004882843671425605 0.006165678435379954 auc 0.998633860610644 0.9955855700127811\n",
      "7 fold: ls 0.004700949772697978 0.007322566990877824 auc 0.9988046605181299 0.9958317618957823\n",
      "8 fold: ls 0.0050731338746179844 0.0070859381378534795 auc 0.998435222901417 0.9943374752414998\n",
      "9 fold: ls 0.005098204955194032 0.007349068210195395 auc 0.9984450507632967 0.9937797820850771\n",
      "threat 0.05 0.48 3\n",
      "this class avg train 0.0047566832581363554 avg val 0.006776084557617701\n",
      "this class auc train 0.9987470221338268 auc val 0.9954362222075872\n",
      "========================\n",
      "0 fold: ls 0.04816202060160167 0.04972842266594707 auc 0.9924023719667245 0.991522892832166\n",
      "1 fold: ls 0.05050407700306056 0.053335730898850436 auc 0.9914946440440313 0.9899308262701227\n",
      "2 fold: ls 0.05115040109771483 0.05508116123045599 auc 0.9912784015661381 0.9887359502625072\n",
      "3 fold: ls 0.049462580068174895 0.051243532025923216 auc 0.9918831243708353 0.9908334978534311\n",
      "4 fold: ls 0.05071441824765875 0.05534320278610287 auc 0.9914127158585292 0.9890645763317051\n",
      "5 fold: ls 0.04764458154551396 0.05232380400917773 auc 0.9925871178925317 0.9906157127162564\n",
      "6 fold: ls 0.04863431189821115 0.055682160918920295 auc 0.9921725184562541 0.9894412127592576\n",
      "7 fold: ls 0.048922304587034116 0.054965049837593156 auc 0.9920867037944919 0.9897266737158634\n",
      "8 fold: ls 0.04967685548354698 0.05222957275126819 auc 0.9918105468472556 0.990455271287836\n",
      "9 fold: ls 0.050730197164369564 0.054497309062879785 auc 0.991408776530283 0.9894810714991444\n",
      "insult 0.05 0.48 3\n",
      "this class avg train 0.04956017476968865 avg val 0.05344299461871187\n",
      "this class auc train 0.9918536921327075 auc val 0.9899807685528291\n",
      "========================\n",
      "0 fold: ls 0.014510790466087135 0.017050241798014818 auc 0.995731138841238 0.99295353728841\n",
      "1 fold: ls 0.01354563946196336 0.01751946724131503 auc 0.996485994352799 0.9917585755877172\n",
      "2 fold: ls 0.014420073258577288 0.017227923794823848 auc 0.9957862577631674 0.9930086893669035\n",
      "3 fold: ls 0.014508390228859367 0.01819625941973763 auc 0.9957652939543933 0.988447208923696\n",
      "4 fold: ls 0.014040559543823064 0.018670478188118327 auc 0.9961097462280928 0.9900941486335064\n",
      "5 fold: ls 0.01522026809011084 0.016290715795345415 auc 0.9951342962490702 0.9937305250228055\n",
      "6 fold: ls 0.01447961704139097 0.01638984412198677 auc 0.995734374023832 0.9920207384926657\n",
      "7 fold: ls 0.015038330682228906 0.018502259467674382 auc 0.9953405831809644 0.9875282263891899\n",
      "8 fold: ls 0.01366692691358648 0.01680945473011738 auc 0.9963863660659372 0.9926855264108678\n",
      "9 fold: ls 0.015174757031999365 0.016572012307314896 auc 0.9952278592957569 0.9922962280511597\n",
      "identity_hate 0.05 0.48 3\n",
      "this class avg train 0.014460535271862677 avg val 0.01732286568644485\n",
      "this class auc train 0.9957701909955251 auc val 0.9914523404166923\n",
      "========================\n",
      "all loss avg 0.031635967204390235 0.03501858099134131\n",
      "all auc avg 0.9947100171149001 0.9922089755180957\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.05 max depth 3 feature fraction 0.56\n",
      "0 fold: ls 0.0707643145280119 0.07826653270535054 auc 0.9904091379393087 0.9870732471900137\n",
      "1 fold: ls 0.06980423969686078 0.07061731611909916 auc 0.9907159959986519 0.9900107090243916\n",
      "2 fold: ls 0.07084610384526914 0.07507000311500986 auc 0.9903093959702267 0.9887485934212887\n",
      "3 fold: ls 0.06750087904693612 0.07325193837613289 auc 0.9914554122439884 0.9891198758405497\n",
      "4 fold: ls 0.06601532567442135 0.07705881308450103 auc 0.9918779027628942 0.9881760594498417\n",
      "5 fold: ls 0.06785576053416251 0.0751834226737988 auc 0.9913618651021502 0.9881468215552819\n",
      "6 fold: ls 0.06735978588849174 0.07702861104636292 auc 0.9914934063077547 0.9881282815570261\n",
      "7 fold: ls 0.070917616916117 0.07011556800376129 auc 0.9902931639947236 0.9903938018983101\n",
      "8 fold: ls 0.06745178520566973 0.0736866580074989 auc 0.9914262233058038 0.989521364250402\n",
      "9 fold: ls 0.06837664081019731 0.07617642499276453 auc 0.9911295804863558 0.9882988182130528\n",
      "toxic 0.05 0.56 3\n",
      "this class avg train 0.06868924521461377 avg val 0.07464552881242799\n",
      "this class auc train 0.9910472084111859 auc val 0.988761757240016\n",
      "========================\n",
      "0 fold: ls 0.017936963693863297 0.02084361210754873 auc 0.9942375222539661 0.9915325515888087\n",
      "1 fold: ls 0.017863989278147634 0.020697434404270923 auc 0.9942784779328732 0.9916243353589062\n",
      "2 fold: ls 0.01814760581695933 0.02076651187230708 auc 0.9941134935973422 0.9913133782757312\n",
      "3 fold: ls 0.018129822463120004 0.019711167752770143 auc 0.9940811544913789 0.9926248575769085\n",
      "4 fold: ls 0.017943568135051265 0.021044876024740918 auc 0.9942364929705088 0.9913117957969363\n",
      "5 fold: ls 0.01808754716658547 0.01985620935467275 auc 0.9941121928606814 0.992384196391391\n",
      "6 fold: ls 0.018167083421393393 0.019105911631358607 auc 0.9940730069735741 0.9930203290729113\n",
      "7 fold: ls 0.0176144048534416 0.02041072626981494 auc 0.9945123781840307 0.9919389996428746\n",
      "8 fold: ls 0.017704717107529733 0.02075685891519483 auc 0.9944293513956994 0.9914926924664862\n",
      "9 fold: ls 0.01811363018482909 0.01870713136001169 auc 0.9941026784648119 0.9934069162881417\n",
      "severe_toxic 0.05 0.56 3\n",
      "this class avg train 0.01797093321209208 avg val 0.020190043969269062\n",
      "this class auc train 0.9942176749124867 auc val 0.9920650052459097\n",
      "========================\n",
      "0 fold: ls 0.0351354933314433 0.03678064601078485 auc 0.9962673432791713 0.9957934252301303\n",
      "1 fold: ls 0.03574313760723968 0.03681961193505487 auc 0.9961403858036764 0.9956513789413637\n",
      "2 fold: ls 0.03514841173406249 0.039056742156628174 auc 0.9962522762564144 0.9953196801162758\n",
      "3 fold: ls 0.034178809390591654 0.03354945474685152 auc 0.9964744925074165 0.9964985700458274\n",
      "4 fold: ls 0.03450190791152078 0.04210605714541848 auc 0.996408843518445 0.9945100253413566\n",
      "5 fold: ls 0.03496955712394275 0.037383666099134426 auc 0.9962981074124226 0.995642790243108\n",
      "6 fold: ls 0.03596175912811999 0.037636695977214286 auc 0.996102250639042 0.9954470916956154\n",
      "7 fold: ls 0.03526906008537521 0.03795567722629252 auc 0.9962327557489603 0.9955953339326716\n",
      "8 fold: ls 0.03490983574684719 0.03863399227533714 auc 0.9963262580949408 0.9952809162983451\n",
      "9 fold: ls 0.03523032719811765 0.03870029468212428 auc 0.996251651933587 0.995348867476711\n",
      "obscene 0.05 0.56 3\n",
      "this class avg train 0.03510482992572607 avg val 0.03786228382548405\n",
      "this class auc train 0.9962754365194076 auc val 0.9955088079321405\n",
      "========================\n",
      "0 fold: ls 0.004913507227324583 0.0067079930134706645 auc 0.9986247961423249 0.9968010161324115\n",
      "1 fold: ls 0.005115214564632036 0.0062871637661887966 auc 0.998471390247218 0.9968245862141212\n",
      "2 fold: ls 0.004272454806843552 0.007049372494500319 auc 0.9991161741463072 0.9942763984915147\n",
      "3 fold: ls 0.00439686772426662 0.006515460344553343 auc 0.9990450326397389 0.9931053177446728\n",
      "4 fold: ls 0.004717933680897914 0.0066992442445583476 auc 0.9988030688111182 0.9966161711400255\n",
      "5 fold: ls 0.0049709060772206975 0.006813291394128506 auc 0.9985563704662337 0.9964223605087268\n",
      "6 fold: ls 0.0046151644265062155 0.006265014190909559 auc 0.9988377615272072 0.995246401408008\n",
      "7 fold: ls 0.004590478203173685 0.007333436136952576 auc 0.9988867796064002 0.9957859283005427\n",
      "8 fold: ls 0.004588028977695319 0.007005588115125271 auc 0.9988841771326509 0.9947052584981337\n",
      "9 fold: ls 0.004673453175840835 0.007251464043721226 auc 0.9988187039654199 0.9944377797660364\n",
      "threat 0.05 0.56 3\n",
      "this class avg train 0.004685400886440147 avg val 0.00679280277441086\n",
      "this class auc train 0.9988044254684618 auc val 0.9954221218204193\n",
      "========================\n",
      "0 fold: ls 0.049616995336390184 0.049637736548136156 auc 0.9918388810607693 0.9915397073438426\n",
      "1 fold: ls 0.05104914940167078 0.053303575247972336 auc 0.9912878806148553 0.9897045832510734\n",
      "2 fold: ls 0.05076656861407945 0.054987296233327355 auc 0.9913965270038465 0.9890636241044809\n",
      "3 fold: ls 0.048145083330288246 0.05101325368072045 auc 0.9923936818054824 0.9908428671335691\n",
      "4 fold: ls 0.05040098888161344 0.05539901223862584 auc 0.9915378783716459 0.9890683410227846\n",
      "5 fold: ls 0.049076642546555894 0.052520902431079354 auc 0.9920416167900475 0.9905695325056812\n",
      "6 fold: ls 0.04991096831700472 0.055815620548087176 auc 0.9917012822737276 0.9893914351772063\n",
      "7 fold: ls 0.048929991687679704 0.054989898460668096 auc 0.9920857920853835 0.9897113445188446\n",
      "8 fold: ls 0.04906444059921006 0.052326042172359546 auc 0.9920363283795417 0.9903508149562368\n",
      "9 fold: ls 0.04889046876058147 0.05428959359449806 auc 0.9921086612623582 0.9895371110226727\n",
      "insult 0.05 0.56 3\n",
      "this class avg train 0.049585129747507395 avg val 0.05342829311554744\n",
      "this class auc train 0.9918428529647658 auc val 0.989977936103639\n",
      "========================\n",
      "0 fold: ls 0.01423438159449469 0.017023244591318005 auc 0.9959395431099053 0.9929956860313237\n",
      "1 fold: ls 0.015219904220666502 0.017522198290550187 auc 0.9951774680894372 0.9921217722021867\n",
      "2 fold: ls 0.015097417457818441 0.017263629870869376 auc 0.9952241086735392 0.9930970223706694\n",
      "3 fold: ls 0.015078559411846564 0.01824413039863666 auc 0.995301672542906 0.9877064671865311\n",
      "4 fold: ls 0.014681945491959612 0.018674131201223815 auc 0.9956165465052864 0.9904802131829609\n",
      "5 fold: ls 0.015317736329209396 0.0163533426233039 auc 0.9950384426495955 0.9936140138548939\n",
      "6 fold: ls 0.014465041304520209 0.01632550024645592 auc 0.9957479851893193 0.9924181660524604\n",
      "7 fold: ls 0.015178002697021091 0.01862340799099096 auc 0.9952055544217546 0.9873972559433485\n",
      "8 fold: ls 0.012900379657302287 0.01670105093479372 auc 0.9969318733678472 0.992858046101597\n",
      "9 fold: ls 0.015434720544386316 0.01661115972114195 auc 0.994991985183766 0.9923861008743406\n",
      "identity_hate 0.05 0.56 3\n",
      "this class avg train 0.014760808870922511 avg val 0.01733417958692845\n",
      "this class auc train 0.9955175179733358 auc val 0.9915074743800311\n",
      "========================\n",
      "all loss avg 0.031799391309550326 0.035042188680677974\n",
      "all auc avg 0.9946175193749406 0.9922071837870259\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.05 max depth 3 feature fraction 0.62\n",
      "0 fold: ls 0.06837289844962591 0.07816413291535003 auc 0.9911643855638107 0.9870993855448102\n",
      "1 fold: ls 0.07088276731083241 0.07076818794572352 auc 0.9903511099715675 0.9899976624972139\n",
      "2 fold: ls 0.07047165641649192 0.07502632525650652 auc 0.990433125182632 0.9887616852489078\n",
      "3 fold: ls 0.06614977233803197 0.07309245909925528 auc 0.9918804560571463 0.9892089818091546\n",
      "4 fold: ls 0.06628788608505103 0.07708508899385727 auc 0.9917930698105547 0.9881247911417068\n",
      "5 fold: ls 0.0675903271234019 0.0751477397493289 auc 0.9914258490030683 0.9881687613087189\n",
      "6 fold: ls 0.06965946812167795 0.0772451223009216 auc 0.9907159618807502 0.9880803223439345\n",
      "7 fold: ls 0.07123750925213757 0.07011350188224294 auc 0.9901621513884598 0.9903348687238606\n",
      "8 fold: ls 0.06706537549972469 0.07374558281801054 auc 0.991556201999541 0.989475260374698\n",
      "9 fold: ls 0.06778916070789338 0.07600741828333168 auc 0.9913202825188359 0.9883646420355918\n",
      "toxic 0.05 0.62 3\n",
      "this class avg train 0.06855068213048687 avg val 0.07463955592445284\n",
      "this class auc train 0.9910802593376367 auc val 0.9887616361028597\n",
      "========================\n",
      "0 fold: ls 0.017680494288732394 0.020923326853786162 auc 0.9944327111576238 0.9914277123686542\n",
      "1 fold: ls 0.01729316582482553 0.02062183524016142 auc 0.9947770971491152 0.991715723509305\n",
      "2 fold: ls 0.017975497388137873 0.020781810768551866 auc 0.9942224898141374 0.9913181257121154\n",
      "3 fold: ls 0.01784792039996046 0.019655880387787046 auc 0.9942989092095038 0.9926663976452715\n",
      "4 fold: ls 0.017878293127453312 0.0210385516855111 auc 0.9942951988877072 0.9913062571211546\n",
      "5 fold: ls 0.018036602995426216 0.01981691749858366 auc 0.9941520939012028 0.9924172393448418\n",
      "6 fold: ls 0.017899527716543934 0.01907416176255694 auc 0.9942780429534656 0.9930223197382831\n",
      "7 fold: ls 0.017808715255314824 0.020399689290018744 auc 0.9943466331752034 0.9919346201790562\n",
      "8 fold: ls 0.018018071056271314 0.020734881427774892 auc 0.9942082209093364 0.9915430563003962\n",
      "9 fold: ls 0.017893565266544526 0.018765916561486264 auc 0.9942835188080716 0.9933241046086692\n",
      "severe_toxic 0.05 0.62 3\n",
      "this class avg train 0.01783318533192104 avg val 0.020181297147621808\n",
      "this class auc train 0.9943294915965366 auc val 0.9920675556527747\n",
      "========================\n",
      "0 fold: ls 0.03514723515719677 0.036852289458453584 auc 0.9962616689130588 0.9957747102009047\n",
      "1 fold: ls 0.03591171849960056 0.03683328191090037 auc 0.9961133217315132 0.9956520053858564\n",
      "2 fold: ls 0.03310255699816885 0.03899617515342213 auc 0.9967054936394387 0.9953357338186511\n",
      "3 fold: ls 0.03516987627149197 0.03360610091653622 auc 0.9962495660014478 0.9964633302113451\n",
      "4 fold: ls 0.034922571894900896 0.04206047333653045 auc 0.9963180006645393 0.9945319523494789\n",
      "5 fold: ls 0.03526839121801023 0.03757253006102841 auc 0.9962330255174711 0.9955905569773306\n",
      "6 fold: ls 0.03462314683551311 0.03747240815488638 auc 0.9963796471203555 0.9953865574910491\n",
      "7 fold: ls 0.0351678763927762 0.03794863774807825 auc 0.9962535482294463 0.9956049661540968\n",
      "8 fold: ls 0.03506185277363545 0.038625977014115784 auc 0.9962961948600416 0.9952853800107129\n",
      "9 fold: ls 0.03513126672265153 0.03875165229510793 auc 0.9962717968343686 0.995335225262746\n",
      "obscene 0.05 0.62 3\n",
      "this class avg train 0.03495064927639455 avg val 0.03787195260490596\n",
      "this class auc train 0.9963082263511682 auc val 0.9954960417862171\n",
      "========================\n",
      "0 fold: ls 0.004905431762869463 0.006788010216081468 auc 0.9986356214497986 0.9966792373769118\n",
      "1 fold: ls 0.004858200172666296 0.006259174311408822 auc 0.9987040815713312 0.9968664885816049\n",
      "2 fold: ls 0.004032403024314361 0.007050322832969131 auc 0.9992699860919568 0.9946220930232558\n",
      "3 fold: ls 0.004581853541959779 0.006475898925050221 auc 0.9989059613000804 0.9931236511827687\n",
      "4 fold: ls 0.003949123254935396 0.006682872941569937 auc 0.9993047326322027 0.9965585517631529\n",
      "5 fold: ls 0.004615440479184208 0.0067633264898291105 auc 0.9988659249961669 0.9963215265991996\n",
      "6 fold: ls 0.004813246538830284 0.006297747305875235 auc 0.9986844216711234 0.9953577115678758\n",
      "7 fold: ls 0.004626951361341618 0.007363370588024023 auc 0.9988603540216264 0.9958487858025855\n",
      "8 fold: ls 0.004844039083933259 0.007018624147588053 auc 0.9986823130116809 0.9948617335564106\n",
      "9 fold: ls 0.004283454849189657 0.007252075155467154 auc 0.9990940623445205 0.9941248296494825\n",
      "threat 0.05 0.62 3\n",
      "this class avg train 0.0045510144069224314 avg val 0.006795142291386315\n",
      "this class auc train 0.9989007459090488 auc val 0.995436460910325\n",
      "========================\n",
      "0 fold: ls 0.049725226646905984 0.049732688497379746 auc 0.991799898550414 0.9914987167432383\n",
      "1 fold: ls 0.05123326082498841 0.053286571644362464 auc 0.9912346351016121 0.9897474142459903\n",
      "2 fold: ls 0.05113556437440813 0.05496742100112154 auc 0.9912868504631689 0.9887762298016725\n",
      "3 fold: ls 0.047907359847531285 0.05109454605424236 auc 0.9924730287999952 0.9908181054646327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 fold: ls 0.05066557238758293 0.05548348131566155 auc 0.991446947228025 0.989030945091395\n",
      "5 fold: ls 0.04783885517132042 0.052316756205815294 auc 0.9925141713917555 0.9906306878207727\n",
      "6 fold: ls 0.049806255746093 0.05569035584073277 auc 0.9917492056743997 0.9894456048988504\n",
      "7 fold: ls 0.04925033099421955 0.055085217453466645 auc 0.9919615842456666 0.9896598283649283\n",
      "8 fold: ls 0.048579188139600535 0.0523002826980512 auc 0.9922256126207402 0.9904226862734077\n",
      "9 fold: ls 0.050719803656115746 0.05452334923615614 auc 0.9914181735909638 0.9895052799031798\n",
      "insult 0.05 0.62 3\n",
      "this class avg train 0.0496861417788766 avg val 0.053448066994698964\n",
      "this class auc train 0.991811010766674 auc val 0.9899535498608067\n",
      "========================\n",
      "0 fold: ls 0.014587980409584951 0.017034385269734092 auc 0.9956708817853712 0.9929409823437122\n",
      "1 fold: ls 0.014631530705768914 0.017360155534442517 auc 0.9956465972248096 0.9922589798120973\n",
      "2 fold: ls 0.014313852706702521 0.017212041128671134 auc 0.9958768767721977 0.9931006094977259\n",
      "3 fold: ls 0.015233186323153514 0.018189385211304986 auc 0.9951789909111773 0.9880234795401482\n",
      "4 fold: ls 0.014544298039221628 0.018600052308365082 auc 0.9957115116697943 0.9897959686969358\n",
      "5 fold: ls 0.015265916565177598 0.016279646181307478 auc 0.9950979634560131 0.9937350409595462\n",
      "6 fold: ls 0.01441002951437856 0.01638752287394176 auc 0.9957790393049477 0.9922375171616447\n",
      "7 fold: ls 0.015275158049941472 0.018649446865383133 auc 0.9951322923651691 0.9875555495339259\n",
      "8 fold: ls 0.013956230791844146 0.016797128606969917 auc 0.9961711974743541 0.9926688163884675\n",
      "9 fold: ls 0.015181528247048348 0.016475317951686608 auc 0.9952134678900612 0.992537845942626\n",
      "identity_hate 0.05 0.62 3\n",
      "this class avg train 0.014739971135282168 avg val 0.017298508193180673\n",
      "this class auc train 0.9955478818853895 auc val 0.991485478987683\n",
      "========================\n",
      "all loss avg 0.03171860734331394 0.035039087192707756\n",
      "all auc avg 0.994662935974409 0.992200120550111\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "[0.9887825645527965, 0.9921426168741385, 0.9955318446611215, 0.9955039709421902, 0.9900536177079309, 0.9915995552784374]\n",
      "0.9922690283361025\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999031      0.404624  0.982360  0.146232  0.926281   \n",
      "1  0000247867823ef7  0.000179      0.000032  0.000069  0.000030  0.000052   \n",
      "2  00013b17ad220c46  0.000386      0.000033  0.000120  0.000031  0.000165   \n",
      "3  00017563c3f7919a  0.000060      0.000032  0.000057  0.000048  0.000047   \n",
      "4  00017695ad8997eb  0.001359      0.000036  0.000170  0.000076  0.000192   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.391064  \n",
      "1       0.000038  \n",
      "2       0.000049  \n",
      "3       0.000040  \n",
      "4       0.000051  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "# find best params for each column, early stopping = 30\n",
    "\n",
    "best_pred = np.zeros((153164,6))\n",
    "val_auc_res = [0,0,0,0,0,0]\n",
    "for lr in [0.095,0.075,0.05]:\n",
    "    for max_d in [3]:\n",
    "        for s_rate in [0.4,0.48,0.56,0.62]:\n",
    "            print('learning rate',lr,'max depth',max_d,'feature fraction',s_rate)\n",
    "            lgb_res,tmp_auc_res = simple_ens('lgb',k=10,rnd=666,lr=lr,\n",
    "                                 feature_fraction=s_rate,bagging_fraction=0.9,\n",
    "                                 bag_frec=3,met='binary_logloss',max_d=max_d)\n",
    "            # check for each cls\n",
    "            for i in range(6):\n",
    "                # find better params for this class\n",
    "                if tmp_auc_res[i] > val_auc_res[i]:\n",
    "                    val_auc_res[i] = tmp_auc_res[i]\n",
    "                    best_pred[:,i] = lgb_res[:,i]\n",
    "                    print('FIND BETTER PARAMS',lr,max_d,s_rate,list_classes[i])\n",
    "            print('TEST PARAM DONE ------------------------------------------')\n",
    "\n",
    "print(val_auc_res)\n",
    "print(np.mean(val_auc_res))\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = best_pred\n",
    "sample_submission.to_csv(\"../results/lgb_grid_search_fold10_stratified.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "                \n",
    "            \n",
    "# best auc params\n",
    "# toxic 0.05,4,0.5\n",
    "# severe toxic 0.075 3 0.5\n",
    "# obs 0.075 3 0.6\n",
    "# threat 0.095 3 0.5\n",
    "# insult 0.075 0.5 4\n",
    "# hate 0.05 0.5 3\n",
    "\n",
    "# TEST PARAM DONE ------------------------------------------\n",
    "# [0.9884873039634368, 0.9920148292218363, 0.9954573112511824, \n",
    "#  0.994769628570376, 0.9898761527824232, 0.9912880359235366]\n",
    "# 0.9919822102854652 PUB 9870\n",
    "\n",
    "# updated pool gru v2 10 fold\n",
    "# TEST PARAM DONE ------------------------------------------\n",
    "# [0.9887825645527965, 0.9921426168741385, 0.9955318446611215, \n",
    "#  0.9955039709421902, 0.9900536177079309, 0.9915995552784374]\n",
    "# 0.9922690283361025 PUB 9869"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
