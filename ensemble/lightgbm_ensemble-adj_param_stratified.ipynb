{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping\n",
      "sleep done =======================\n",
      "file path ../features/fasttext_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/glove_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_fasttext_adj2_4_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lstm_attention_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/muse_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/no_pretrained_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/pool_gru_fasttext_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj1_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_10_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_fasttext_adj2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/pool_gru_glove_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 313)\n",
      "(159571, 313)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "import lightgbm as lgb\n",
    "\n",
    "# import time\n",
    "# print('sleeping')\n",
    "# time.sleep(24400)\n",
    "# print('sleep done =======================')\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in sorted(glob.glob('../features/*.pkl')):\n",
    "    if '3_feat' in feat or 'tfidf' in feat:\n",
    "        continue\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.hstack(train_x)\n",
    "test_x = np.hstack(test_x)\n",
    "print(train_x.shape)\n",
    "    \n",
    "# load y\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values.astype('int')\n",
    "print(train_x.shape)\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "def simple_ens(model_name,k=3,rnd=233,lr=0.05,feature_fraction=0.9,bagging_fraction=0.9,\n",
    "               bag_frec=3,met='binary_logloss',max_d=3):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    cls_auc_res = [0,0,0,0,0,0]\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "\n",
    "            # share params\n",
    "            params = {\n",
    "                    'application': 'binary',\n",
    "                    #'num_leaves': 8,\n",
    "                    #'lambda_l1': 1,\n",
    "                    'lambda_l2': 1.0,\n",
    "                    'max_depth': max_d,\n",
    "                    'metric': met, # or auc\n",
    "                    'data_random_seed': 2,\n",
    "                    'learning_rate':lr,\n",
    "                    # 'bagging_fraction': bagging_fraction,\n",
    "                    # 'bagging_freq':bag_frec,\n",
    "                    'feature_fraction': feature_fraction,\n",
    "\n",
    "                    }\n",
    "            if met == 'auc':\n",
    "                s_round = 60\n",
    "            else:\n",
    "                s_round = 30\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i], lr, feature_fraction, max_d)\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        cls_auc_res[i] = val_auc_l\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred, cls_auc_res\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.06960238770416617 0.0781605646273722 auc 0.9907893554131592 0.9871128397759622\n",
      "1 fold: ls 0.06965586749041318 0.07066149030897897 auc 0.9907712887458717 0.9900424193335037\n",
      "2 fold: ls 0.07141288345672146 0.07499926491809923 auc 0.9901234160818309 0.9888034975564943\n",
      "3 fold: ls 0.06657558553422858 0.07283725191739757 auc 0.9917452868695367 0.9892583592904864\n",
      "4 fold: ls 0.06970514972282499 0.07728551992825755 auc 0.99067586389295 0.9880644114896857\n",
      "5 fold: ls 0.06693818737383217 0.0751218141768801 auc 0.9916379375382057 0.9882031215010854\n",
      "6 fold: ls 0.06974700654827182 0.07725293056797954 auc 0.9906617045532646 0.9880672219539689\n",
      "7 fold: ls 0.07197855625583804 0.07002825637527361 auc 0.9899108479831085 0.9904254444796684\n",
      "8 fold: ls 0.0680760847028484 0.073680745182608 auc 0.991238367889631 0.9895219989153576\n",
      "9 fold: ls 0.0674869994506814 0.07622478882141936 auc 0.9914183749698059 0.9882837222537514\n",
      "toxic 0.05 0.45 3\n",
      "this class avg train 0.06911787082398262 avg val 0.07462526268242661\n",
      "this class auc train 0.9908972443937365 auc val 0.9887783036549964\n",
      "========================\n",
      "0 fold: ls 0.017987346888853478 0.0208894518142621 auc 0.9941943952770985 0.991499715153817\n",
      "1 fold: ls 0.01732651399758421 0.020675681379565554 auc 0.9947515121031725 0.9916116755285479\n",
      "2 fold: ls 0.018031773551823026 0.020756386964467074 auc 0.9941869525774313 0.9913790511457147\n",
      "3 fold: ls 0.017813870567409563 0.019562596811564616 auc 0.9943366863630676 0.9927641157108495\n",
      "4 fold: ls 0.017999989679332792 0.020954259006361966 auc 0.9941945790777159 0.9913758861881251\n",
      "5 fold: ls 0.018003144576727192 0.019831581074355398 auc 0.9941891492405593 0.9924216185314438\n",
      "6 fold: ls 0.01816665872272521 0.019148299043951735 auc 0.9940861431471462 0.9929677755070923\n",
      "7 fold: ls 0.01793293762550574 0.020397097630104832 auc 0.9942495322650381 0.9919541286997015\n",
      "8 fold: ls 0.018001148953483637 0.020756400952782184 auc 0.9941997965176347 0.9914855260711473\n",
      "9 fold: ls 0.018168249630771557 0.018795093021345438 auc 0.9940602944685385 0.9933460019277601\n",
      "severe_toxic 0.05 0.45 3\n",
      "this class avg train 0.01794316341942164 avg val 0.02017668476987609\n",
      "this class auc train 0.9942449041037401 auc val 0.9920805494464199\n",
      "========================\n",
      "0 fold: ls 0.03546660993797372 0.03677708365427342 auc 0.9962001370673906 0.9958018039252229\n",
      "1 fold: ls 0.035825538902254234 0.03667182058450849 auc 0.9961239003124888 0.9956999283895639\n",
      "2 fold: ls 0.03432863515892918 0.03891699262571719 auc 0.9964395270275367 0.9953614980531948\n",
      "3 fold: ls 0.03563514259123 0.03366700835860073 auc 0.9961480087931771 0.9964922660309923\n",
      "4 fold: ls 0.03514487297991607 0.042049304817378896 auc 0.9962732079722609 0.9945296813379234\n",
      "5 fold: ls 0.03442872954883285 0.03739362447087665 auc 0.9964176719105153 0.9956128755391694\n",
      "6 fold: ls 0.035756368829304094 0.03765403436837075 auc 0.9961370681813514 0.9953853045191564\n",
      "7 fold: ls 0.03522239034061242 0.03798185046808135 auc 0.9962473135794196 0.9955828825244878\n",
      "8 fold: ls 0.035051696580429996 0.0387120313238479 auc 0.9962966981378546 0.9952589109794795\n",
      "9 fold: ls 0.03521448738882167 0.038670444237857066 auc 0.9962521817318128 0.9953347548415747\n",
      "obscene 0.05 0.45 3\n",
      "this class avg train 0.03520744722583043 avg val 0.03784941949095124\n",
      "this class auc train 0.9962535714713809 auc val 0.9955059906140764\n",
      "========================\n",
      "0 fold: ls 0.005026954380770551 0.006708523174214938 auc 0.9984916765323413 0.9968782736224595\n",
      "1 fold: ls 0.0048936064463851025 0.006251861075130591 auc 0.9986787440174543 0.9968298240100566\n",
      "2 fold: ls 0.004555700582547964 0.007033539915598485 auc 0.9989183788058508 0.9944662685941755\n",
      "3 fold: ls 0.004577237876584939 0.00645182541116019 auc 0.9989042558997107 0.9930476983678003\n",
      "4 fold: ls 0.004372817905140133 0.006601461295906563 auc 0.9990491662053964 0.9967314098937708\n",
      "5 fold: ls 0.004916475194918716 0.0068033726180631035 auc 0.9986241966752164 0.9963542648815137\n",
      "6 fold: ls 0.004875262156339776 0.006184043707302638 auc 0.9986309208252448 0.9955842604814884\n",
      "7 fold: ls 0.005107530536504377 0.007332803328671676 auc 0.9983459240606329 0.9959797389318416\n",
      "8 fold: ls 0.005041162558673279 0.007045911917249528 auc 0.998483867981983 0.9944551658836226\n",
      "9 fold: ls 0.004594896450804474 0.007304062108877982 auc 0.9988653721306533 0.9941850123642043\n",
      "threat 0.05 0.45 3\n",
      "this class avg train 0.00479616440886693 avg val 0.0067717404552175686\n",
      "this class auc train 0.9986992503134484 auc val 0.9954511917030932\n",
      "========================\n",
      "0 fold: ls 0.049343552902700046 0.049787348021110865 auc 0.9919487456532095 0.991491104203126\n",
      "1 fold: ls 0.051056192441345044 0.053282837056848824 auc 0.9912843970076228 0.989697932735261\n",
      "2 fold: ls 0.051225604984930895 0.05509250240041487 auc 0.9912539992668626 0.9887495022569925\n",
      "3 fold: ls 0.05026100987643154 0.0513817379918849 auc 0.991581412367017 0.9907705061753596\n",
      "4 fold: ls 0.04896399469082867 0.055308563831465825 auc 0.9920766241256916 0.9890156353476718\n",
      "5 fold: ls 0.047578863927422506 0.05231915143230791 auc 0.9926064209545579 0.9905923716315636\n",
      "6 fold: ls 0.05002391541471699 0.05581932618655993 auc 0.9916470602184086 0.9893900966203784\n",
      "7 fold: ls 0.05061762785330415 0.05514777326018388 auc 0.9914354092837815 0.989662676412462\n",
      "8 fold: ls 0.0501075071406736 0.052309035895404254 auc 0.9916387870498049 0.9904407797518563\n",
      "9 fold: ls 0.050895238868401464 0.05451825489892317 auc 0.9913454724086606 0.9894613026986171\n",
      "insult 0.05 0.45 3\n",
      "this class avg train 0.05000735081007549 avg val 0.05349665309751045\n",
      "this class auc train 0.9916818328335616 auc val 0.9899271907833288\n",
      "========================\n",
      "0 fold: ls 0.014344278481195627 0.016965033099169285 auc 0.9958536064998388 0.9931176483512443\n",
      "1 fold: ls 0.01458246542721523 0.01755891384373002 auc 0.995679988148223 0.9918729152626428\n",
      "2 fold: ls 0.01513798332470182 0.01735028368491172 auc 0.9951977066126745 0.9929203563631374\n",
      "3 fold: ls 0.015418292035814343 0.01823698072523437 auc 0.9949974805636814 0.9886467428662132\n",
      "4 fold: ls 0.014981015845049928 0.018624797733989416 auc 0.9953717945936226 0.9904255094953496\n",
      "5 fold: ls 0.01500685113588815 0.016352529701609074 auc 0.9953155159463476 0.9936451738184052\n",
      "6 fold: ls 0.01503539287168468 0.016418319034395116 auc 0.9952836756255458 0.9922908085844353\n",
      "7 fold: ls 0.015243012312602645 0.018504550526959252 auc 0.9951716070310472 0.9876621323795072\n",
      "8 fold: ls 0.01361340959263402 0.016702207855076995 auc 0.9964290710282372 0.9927555278560589\n",
      "9 fold: ls 0.014881093743440764 0.016561971710651536 auc 0.9954606568734026 0.9922664209841752\n",
      "identity_hate 0.05 0.45 3\n",
      "this class avg train 0.01482437947702272 avg val 0.017327558791572682\n",
      "this class auc train 0.9954761102922621 auc val 0.9915603235961168\n",
      "========================\n",
      "all loss avg 0.031982729360866644 0.035041219881259106\n",
      "all auc avg 0.9945421522346883 0.9922172582996719\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999080      0.404933  0.982493  0.144690  0.924196   \n",
      "1  0000247867823ef7  0.000196      0.000038  0.000077  0.000029  0.000093   \n",
      "2  00013b17ad220c46  0.000370      0.000040  0.000137  0.000030  0.000193   \n",
      "3  00017563c3f7919a  0.000064      0.000038  0.000069  0.000056  0.000087   \n",
      "4  00017695ad8997eb  0.001307      0.000045  0.000188  0.000080  0.000236   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.398105  \n",
      "1       0.000045  \n",
      "2       0.000063  \n",
      "3       0.000046  \n",
      "4       0.000062  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "lgb_res,tmp_auc_res = simple_ens('lgb',10,666,0.05,0.45)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_log_csv_fold10_stratified.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# add 7 base fasttext NN models\n",
    "# all loss avg 0.0319116484218 0.0358161833583 all auc avg 0.994546891137 0.991249510282 PUB 9866\n",
    "\n",
    "# rm tfidf test\n",
    "# all loss avg 0.0319555637279 0.0357756335619 all auc avg 0.994512718368 0.991251471241\n",
    "\n",
    "# change to stratified\n",
    "# all loss avg 0.0316412744167 0.0357232681944 all auc avg 0.994584962017 0.991293307537 pub 9866\n",
    "\n",
    "# change to base model 5 fold, add word batch, tilli, lgb feat\n",
    "# feat dim 217\n",
    "# all loss avg 0.031983129767200906 0.035387924581 all auc avg 0.9945427088311004 0.99188514127 PUB 9870\n",
    "\n",
    "# add more base models\n",
    "# feat dim 247, all loss avg 0.031885221777487156 0.0353958  all auc avg 0.9945986685511962 0.9919487331094685\n",
    "\n",
    "# change early stopping to 30, and test later part\n",
    "# all loss avg 0.03208696729295824 0.035406264613808025 all auc avg 0.9945261773637045 0.991946174432887\n",
    "\n",
    "# add fasttext lstm v1\n",
    "# all loss avg 0.031977558955160815 0.03537909655655411 all auc avg 0.9945299073418443 0.9919307178575658\n",
    "\n",
    "# add muse base model, feat dim 295, lower loss, but lower auc\n",
    "# all loss avg 0.03196610276261484 0.03530962013098028 all auc avg 0.9945635742334386 0.9918682952070021\n",
    "\n",
    "# fix pool gru fold to 5, and adj params\n",
    "# all loss avg 0.03172365669814678 0.03528054768190897 all auc avg 0.9946437910139179 0.991903618166854\n",
    "\n",
    "# updated pool gru v2\n",
    "# all loss avg 0.03204678384090053 0.035272185628381025 all auc avg 0.9944980867573662 0.9919033708443966\n",
    "\n",
    "# updated other feat, change some cnt to ratio, a bit worse\n",
    "# all loss avg 0.03197308230179816 0.03527747086566205 all auc avg 0.9945654974291834 0.991902658184264\n",
    "\n",
    "# updated pool gru v2 10 fold PUB 9870\n",
    "# all loss avg 0.03188938973578164 0.035151701851945265 all auc avg 0.9945816630869728 0.9919824289801534\n",
    "\n",
    "# rm lr, mnb feat1\n",
    "# worse all loss avg 0.032097370918022436 0.03520186226431002\n",
    "# all auc avg 0.9944811076726177 0.9919214297276806\n",
    "\n",
    "# add ridge , change lr,mnb,ridge to fold 6\n",
    "# all loss avg 0.03187703661408994 0.0351472518210873 all auc avg 0.9946115814252721 0.9920990121249438\n",
    "\n",
    "# ridge, lr, mnb fold 10, feat frac 0.6\n",
    "# all loss avg 0.031754244562510095 0.03510006533423048 all auc avg 0.9946626820706096 0.9921195758845225\n",
    "\n",
    "# lgb v1 feat fold 10, 5 fold is better, change feat file back\n",
    "# all loss avg 0.031846113030824186 0.03510235514884163 all auc avg 0.9946147807624219 0.9920710159886731\n",
    "\n",
    "# feat frac 0.45 PUB 9871\n",
    "# all loss avg 0.03178085998781136 0.035091249281527216 all auc avg 0.9946391992503673 0.9921624859169091\n",
    "\n",
    "# tilli feat 10 fold\n",
    "# all loss avg 0.031769047743715494 0.035076842284761586 all auc avg 0.9946504692097619 0.9921581333743735\n",
    "\n",
    "# update pool gru v2\n",
    "# all loss avg 0.031982729360866644 0.035041219881259106 all auc avg 0.9945421522346883 0.9922172582996719"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.05, 'feature_fraction': 0.4, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.07054908247034279 0.07799381380216308 auc 0.9904564854821903 0.9871499408376233\n",
      "1 fold: ls 0.07026490907419657 0.0708493844601407 auc 0.9905569248947377 0.9899895084177281\n",
      "2 fold: ls 0.07010288855777674 0.07511858865400087 auc 0.9905634430396628 0.9887420248572584\n",
      "3 fold: ls 0.06708963346279326 0.07305350836513544 auc 0.9915805911764245 0.9891753688814958\n",
      "4 fold: ls 0.06853288965246013 0.07729230316955127 auc 0.9910896922471606 0.9880766959384076\n",
      "5 fold: ls 0.06850396201558229 0.07522622483776802 auc 0.9911423516975264 0.9881758327994962\n",
      "6 fold: ls 0.06852703429135769 0.07702614104935028 auc 0.9911058203997253 0.9881049365714475\n",
      "7 fold: ls 0.0691254261083487 0.06993777341552614 auc 0.9909102763380195 0.9904143831761563\n",
      "8 fold: ls 0.06903230689714812 0.07367716090582431 auc 0.9908818403866766 0.9894666017313751\n",
      "9 fold: ls 0.07106641394238877 0.0766025842487476 auc 0.990227325082827 0.9880566935324875\n",
      "toxic\n",
      "this class avg train 0.0692794546472395 avg val 0.07467774829082077\n",
      "this class auc train 0.9908514750744949 auc val 0.9887351986743476\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.075, 'feature_fraction': 0.6, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.01784498841526735 0.02087871998450636 auc 0.9942924712865449 0.9915289910115205\n",
      "1 fold: ls 0.01781594032345846 0.020625638955607917 auc 0.9943121085445084 0.9917220534244842\n",
      "2 fold: ls 0.017835493542469882 0.020871663603681374 auc 0.9943292020019268 0.9913208950500065\n",
      "3 fold: ls 0.017939395948453903 0.01952206131785635 auc 0.9942188334071885 0.9927720281048235\n",
      "4 fold: ls 0.01800695175910923 0.02108096945110194 auc 0.9942052958790479 0.9912959710089885\n",
      "5 fold: ls 0.0180387032681827 0.019810200315302152 auc 0.9941364351978887 0.9924335617676308\n",
      "6 fold: ls 0.01743437647960038 0.019027871471517934 auc 0.9946804545942822 0.9930024130845638\n",
      "7 fold: ls 0.017888996377383302 0.02034744811700291 auc 0.9942910615773858 0.991997923337884\n",
      "8 fold: ls 0.017952101531367375 0.020722160816503136 auc 0.9942331561291927 0.9915569909579998\n",
      "9 fold: ls 0.01812928775527521 0.018665045609395237 auc 0.9940927528661005 0.9934773858423084\n",
      "severe_toxic\n",
      "this class avg train 0.01788862354005678 avg val 0.020155177964247532\n",
      "this class auc train 0.9942791771484065 auc val 0.9921108213590208\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.095, 'feature_fraction': 0.6, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.03501276186469422 0.036758738524518456 auc 0.9962926209678646 0.9957846550072295\n",
      "1 fold: ls 0.03514333622973654 0.03672021284038774 auc 0.9962708309572097 0.9956772197766961\n",
      "2 fold: ls 0.033778530438696434 0.03902448108665873 auc 0.9965548516155227 0.995319993359249\n",
      "3 fold: ls 0.034179909886822804 0.033580316387400444 auc 0.9964724600579196 0.9964720227038507\n",
      "4 fold: ls 0.034312788842217466 0.0421432376910739 auc 0.9964489882636581 0.9945054833182455\n",
      "5 fold: ls 0.03264592151472135 0.037358939252314295 auc 0.9968035158166149 0.9956358205869548\n",
      "6 fold: ls 0.03567122614237551 0.03747633400777585 auc 0.9961500746011458 0.9954713680260368\n",
      "7 fold: ls 0.03528975841651287 0.037961461803730986 auc 0.9962287304933677 0.9956100955077825\n",
      "8 fold: ls 0.034570252492217735 0.0386850759951574 auc 0.9964005686836236 0.9952729286025291\n",
      "9 fold: ls 0.03521429468407392 0.038755308139161886 auc 0.9962501824202603 0.9953141347135699\n",
      "obscene\n",
      "this class avg train 0.03458187805120689 avg val 0.03784641057281797\n",
      "this class auc train 0.9963872823877187 auc val 0.9955063721602144\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.05, 'feature_fraction': 0.6, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.0050021257405958655 0.006775744429503689 auc 0.9985310715560132 0.9966857846218311\n",
      "1 fold: ls 0.004914462829221575 0.0063051638525380305 auc 0.9986726532593109 0.9967420909281374\n",
      "2 fold: ls 0.00451360091728444 0.007045295420647893 auc 0.9989609004187031 0.9948027969830295\n",
      "3 fold: ls 0.004570055494448908 0.006456267698677444 auc 0.9989180452798415 0.9932126993106628\n",
      "4 fold: ls 0.004302584117033868 0.006676175831440495 auc 0.999082510843098 0.996680338173361\n",
      "5 fold: ls 0.005004129426191659 0.0068025925696379875 auc 0.998513654247454 0.9963123598801517\n",
      "6 fold: ls 0.0048657538264647165 0.00630932922710763 auc 0.9986680823113925 0.9953262828168542\n",
      "7 fold: ls 0.004943575434431532 0.00737169652842123 auc 0.9985501904201328 0.9958487858025855\n",
      "8 fold: ls 0.005146334496338494 0.007065649006775819 auc 0.9983973131192769 0.9945889052496713\n",
      "9 fold: ls 0.004710745035633763 0.007273805184082782 auc 0.9987906544536078 0.9942344959296424\n",
      "threat\n",
      "this class avg train 0.004797336731764482 avg val 0.006808171974883299\n",
      "this class auc train 0.998708507590883 auc val 0.9954434539695928\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.075, 'feature_fraction': 0.4, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.04964303462319662 0.049855262350365 auc 0.9918298576759376 0.991470525248537\n",
      "1 fold: ls 0.048738175748239236 0.05327538463273099 auc 0.9921642219751595 0.9900521668133405\n",
      "2 fold: ls 0.05097845663910874 0.054979824420283326 auc 0.9913190548440731 0.9891233114382181\n",
      "3 fold: ls 0.04764560117465539 0.05097872414886621 auc 0.9925681406283645 0.9909050222687712\n",
      "4 fold: ls 0.049573945956825974 0.05518398631959181 auc 0.9918353399833425 0.9891773497444861\n",
      "5 fold: ls 0.04865458548399649 0.05237936184561952 auc 0.9921926661951458 0.9905999010137224\n",
      "6 fold: ls 0.04835672286624806 0.05552788819068433 auc 0.9922873764282304 0.989531649004967\n",
      "7 fold: ls 0.049454954408126015 0.055150017757222745 auc 0.9918819328956269 0.9896573153818103\n",
      "8 fold: ls 0.050641004089096345 0.052421299267702046 auc 0.9914398696850006 0.9904412823484798\n",
      "9 fold: ls 0.04984429784189032 0.054342190610639086 auc 0.9917436795701485 0.9895776538169742\n",
      "insult\n",
      "this class avg train 0.049353077883138315 avg val 0.0534093939543705\n",
      "this class auc train 0.9919262139881029 auc val 0.9900536177079309\n",
      "========================\n",
      "{'application': 'binary', 'max_depth': 3, 'data_random_seed': 2, 'learning_rate': 0.095, 'feature_fraction': 0.4, 'lambda_l2': 1.0, 'metric': 'binary_logloss'}\n",
      "0 fold: ls 0.015044865604511298 0.017004252117725827 auc 0.9952924716984135 0.9929302209625428\n",
      "1 fold: ls 0.014510681199286076 0.017388951508259847 auc 0.995733664835694 0.9922132439421271\n",
      "2 fold: ls 0.015065767853567325 0.017345314941278073 auc 0.9952274266464548 0.9928914351512443\n",
      "3 fold: ls 0.015241910474482218 0.01820355859395391 auc 0.9951307274659509 0.9888996353236955\n",
      "4 fold: ls 0.01493036399125096 0.018774530235074843 auc 0.9953810482366792 0.9899452828606621\n",
      "5 fold: ls 0.015145607047772703 0.016354230878900953 auc 0.9951839598590526 0.9935512423341973\n",
      "6 fold: ls 0.013549668696278721 0.01624126921154971 auc 0.9964081628119746 0.9917858949346052\n",
      "7 fold: ls 0.015250248427493186 0.01856875381947178 auc 0.9951079383193737 0.9887128314907146\n",
      "8 fold: ls 0.014167483434083361 0.016853594003453102 auc 0.9960029043817014 0.9925509429872101\n",
      "9 fold: ls 0.01490069408566706 0.016524212920252166 auc 0.9954359529690084 0.9923653262518969\n",
      "identity_hate\n",
      "this class avg train 0.014780729081439289 avg val 0.01732586682299202\n",
      "this class auc train 0.9954904257224303 auc val 0.9915846056238896\n",
      "========================\n",
      "all loss avg 0.03178018332247421 0.035037128263355345\n",
      "all auc avg 0.9946071803186727 0.9922390115824994\n",
      "=======================================================\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999070      0.403654  0.980888  0.147801  0.926281   \n",
      "1  0000247867823ef7  0.000191      0.000033  0.000055  0.000030  0.000052   \n",
      "2  00013b17ad220c46  0.000351      0.000035  0.000115  0.000031  0.000165   \n",
      "3  00017563c3f7919a  0.000062      0.000033  0.000040  0.000059  0.000047   \n",
      "4  00017695ad8997eb  0.001256      0.000039  0.000177  0.000078  0.000192   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.396194  \n",
      "1       0.000038  \n",
      "2       0.000056  \n",
      "3       0.000038  \n",
      "4       0.000055  \n",
      "save done\n"
     ]
    }
   ],
   "source": [
    "def special_ens(model_name,k=3,rnd=233):\n",
    "    kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=rnd)\n",
    "    test_pred = np.zeros((153164,6))\n",
    "    all_train_loss_l,all_val_loss_l = 0,0\n",
    "    all_train_auc_l,all_val_auc_l = 0,0\n",
    "    \n",
    "    params_list = [\n",
    "        [0.05, 3,0.4], # depth should be 3\n",
    "        [0.075,3,0.6],\n",
    "        [0.095,3,0.6],\n",
    "        [0.05, 3,0.6],\n",
    "        [0.075,3,0.4],\n",
    "        [0.095,3,0.4],\n",
    "    ]\n",
    "    \n",
    "    for i in range(6):\n",
    "        val_loss_l,train_loss_l = 0,0\n",
    "        val_auc_l,train_auc_l = 0,0\n",
    "        fold_cnt = 0\n",
    "        \n",
    "        # special params\n",
    "        params = {\n",
    "                'application': 'binary',\n",
    "                #'num_leaves': 8,\n",
    "                #'lambda_l1': 1,\n",
    "                'lambda_l2': 1.0,\n",
    "                'max_depth': params_list[i][1],\n",
    "                'metric': 'binary_logloss', # or auc\n",
    "                'data_random_seed': 2,\n",
    "                'learning_rate':params_list[i][0],\n",
    "                'feature_fraction': params_list[i][2],\n",
    "\n",
    "                }\n",
    "        print(params)\n",
    "            \n",
    "        for train_index, test_index in kf.split(train_x,train_y[:,i]):\n",
    "            # x,y\n",
    "            curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "            hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "            d_test = test_x\n",
    "            s_round = 30\n",
    "            # train for each class\n",
    "            d_train = lgb.Dataset(curr_x, curr_y[:,i])\n",
    "            d_valid = lgb.Dataset(hold_out_x, hold_out_y[:,i])\n",
    "            watchlist = [d_train, d_valid]\n",
    "            model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=2000,\n",
    "                      valid_sets=watchlist,\n",
    "                      early_stopping_rounds=s_round,\n",
    "                      verbose_eval=None)\n",
    "            print(fold_cnt,'fold: ',end='')\n",
    "            fold_cnt += 1\n",
    "            try:\n",
    "                train_pred = model.predict(curr_x)\n",
    "                tmp_test_pred = model.predict(hold_out_x)\n",
    "                \n",
    "                curr_train_loss = log_loss(curr_y[:,i],train_pred)\n",
    "                curr_val_loss = log_loss(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                curr_train_auc = roc_auc_score(curr_y[:,i],train_pred)\n",
    "                curr_val_auc = roc_auc_score(hold_out_y[:,i],tmp_test_pred)\n",
    "                \n",
    "                print('ls',curr_train_loss,curr_val_loss,'auc',curr_train_auc,curr_val_auc)\n",
    "                val_loss_l += curr_val_loss\n",
    "                train_loss_l += curr_train_loss\n",
    "                val_auc_l += curr_val_auc\n",
    "                train_auc_l += curr_train_auc\n",
    "            except:\n",
    "                pass\n",
    "            curr_test_pred = model.predict(d_test)\n",
    "            test_pred[:,i] += curr_test_pred\n",
    "            \n",
    "        # avg k fold\n",
    "        train_loss_l = train_loss_l/k\n",
    "        val_loss_l = val_loss_l/k\n",
    "        train_auc_l = train_auc_l/k\n",
    "        val_auc_l = val_auc_l/k\n",
    "        print(list_classes[i])\n",
    "        print('this class avg train',train_loss_l,'avg val',val_loss_l)\n",
    "        print('this class auc train',train_auc_l,'auc val',val_auc_l)\n",
    "        \n",
    "        \n",
    "        # avg 6 class\n",
    "        all_train_loss_l += train_loss_l/6\n",
    "        all_val_loss_l += val_loss_l/6\n",
    "        all_train_auc_l += train_auc_l/6\n",
    "        all_val_auc_l += val_auc_l/6\n",
    "        print('========================')\n",
    "    test_pred = test_pred/k\n",
    "    print('all loss avg',all_train_loss_l,all_val_loss_l)\n",
    "    print('all auc avg',all_train_auc_l,all_val_auc_l)\n",
    "    print('=======================================================')\n",
    "    return test_pred\n",
    "\n",
    "print('done')\n",
    "\n",
    "lgb_res = special_ens('lgb',10,666)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = lgb_res\n",
    "sample_submission.to_csv(\"../results/lgb_log_csv_fold10_stratified_special.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "\n",
    "# best params changed when base models changed\n",
    "# all loss avg 0.03111512578966158 0.03534320053008906 all auc avg 0.9948524214184179 0.9918450388634261\n",
    "\n",
    "# change lr, ridge, mnb fold to 10, train loss too low ?\n",
    "# all loss avg 0.030612393454975732 0.035095753229703264 all auc avg 0.9950485697696142 0.9921210772750391\n",
    "\n",
    "# tilli feat fold 10\n",
    "# all loss avg 0.03172720686364922 0.0350882288604753 all auc avg 0.9946451061433877 0.9921962852188195\n",
    "\n",
    "# update pool gru v2\n",
    "# all loss avg 0.03178018332247421 0.035037128263355345 all auc avg 0.9946071803186727 0.9922390115824994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate 0.095 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.07085827353248232 0.07798084842168852 auc 0.9903233115480082 0.9872524557369384\n",
      "1 fold: ls 0.06625255482436719 0.0705600833910215 auc 0.9918243707478177 0.9900669268724033\n",
      "2 fold: ls 0.06818108014013745 0.0748847775176703 auc 0.9912062114694379 0.9887604168365433\n",
      "3 fold: ls 0.06601199204171145 0.07305479782103391 auc 0.9919402453983566 0.9891495476297903\n",
      "4 fold: ls 0.0693323629687326 0.07722464066960674 auc 0.990792059100126 0.9880921988220346\n",
      "5 fold: ls 0.06440595724399693 0.07492308373241514 auc 0.9923908487328252 0.9881962313305844\n",
      "6 fold: ls 0.06877299277394358 0.07731179436612658 auc 0.9910170358346071 0.9880255182904107\n",
      "7 fold: ls 0.07217164787214733 0.07017723982894845 auc 0.98989274835965 0.9903317860655048\n",
      "8 fold: ls 0.06644510648395821 0.07360303958204517 auc 0.9917515689468738 0.9895266229028914\n",
      "9 fold: ls 0.06653699594977339 0.07624970565715444 auc 0.991711174745344 0.9883336341191891\n",
      "toxic 0.095 0.4 3\n",
      "this class avg train 0.06789689638312504 avg val 0.07459700109877107\n",
      "this class auc train 0.9912849574883046 auc val 0.988773533860629\n",
      "========================\n",
      "0 fold: ls 0.01801499655757402 0.020911261210646275 auc 0.9941860752358176 0.9915032757311051\n",
      "1 fold: ls 0.01767961315412588 0.020745269863634546 auc 0.9944350221440534 0.9915064406886946\n",
      "2 fold: ls 0.01819458630197277 0.020776290106638617 auc 0.9940752532662227 0.9913865679199899\n",
      "3 fold: ls 0.01666294388112957 0.019484670381444334 auc 0.9953347947851402 0.9928606469173313\n",
      "4 fold: ls 0.017977517746377274 0.020945829389014747 auc 0.9942210047051488 0.991400810229143\n",
      "5 fold: ls 0.01776134254002069 0.019697937101721116 auc 0.9943808299894468 0.9924681971525733\n",
      "6 fold: ls 0.01800103309945274 0.018962232992595758 auc 0.9942047874799278 0.9930852247640365\n",
      "7 fold: ls 0.017997373158664825 0.020404243462768243 auc 0.994184189352427 0.9919644801596355\n",
      "8 fold: ls 0.01797567320708851 0.020728300068826915 auc 0.99421191637651 0.9915225524470651\n",
      "9 fold: ls 0.01799474017040712 0.01870263748291824 auc 0.994192692110563 0.9934694231808205\n",
      "severe_toxic 0.095 0.4 3\n",
      "this class avg train 0.017825981981681338 avg val 0.02013586720602088\n",
      "this class auc train 0.9943426565445256 auc val 0.9921167619190395\n",
      "========================\n",
      "0 fold: ls 0.03558778112577781 0.03690549950501394 auc 0.9961635449721086 0.9957740054508502\n",
      "1 fold: ls 0.034114952352247256 0.0365495615125795 auc 0.996495358812336 0.9957128488072301\n",
      "2 fold: ls 0.0352532912363591 0.03890722992283616 auc 0.996230910880435 0.9953659617655628\n",
      "3 fold: ls 0.03556224211942488 0.03367905772394987 auc 0.996151979418229 0.9965092203069155\n",
      "4 fold: ls 0.03488165323187822 0.041938994834294384 auc 0.9963224881041755 0.9945578732055094\n",
      "5 fold: ls 0.03532857592797993 0.03730744925442132 auc 0.9962162331528599 0.9956600186066326\n",
      "6 fold: ls 0.035841312057100144 0.03751669906497995 auc 0.9961186475365534 0.995522504941408\n",
      "7 fold: ls 0.03537278992242763 0.03802061519435101 auc 0.9962105346556618 0.995579515162526\n",
      "8 fold: ls 0.03456420895857297 0.03854725840086725 auc 0.9963993479569396 0.9953046444535633\n",
      "9 fold: ls 0.03361461463899195 0.03860366786426793 auc 0.9965963403027419 0.9953318539110189\n",
      "obscene 0.095 0.4 3\n",
      "this class avg train 0.035012142157075994 avg val 0.03779760332775613\n",
      "this class auc train 0.9962905385792041 auc val 0.9955318446611215\n",
      "========================\n",
      "0 fold: ls 0.004881216587819931 0.006765270596417183 auc 0.9986558914929 0.9967473287240729\n",
      "1 fold: ls 0.004670906935882958 0.006273687487112723 auc 0.9988456957586722 0.9968049444793631\n",
      "2 fold: ls 0.004907307727325069 0.007165680132466428 auc 0.9986236348377723 0.9942763984915148\n",
      "3 fold: ls 0.004406989311642278 0.0064552591080812415 auc 0.999028587707604 0.9936238921365265\n",
      "4 fold: ls 0.004225969699796507 0.00660002201609406 auc 0.999131999937631 0.9965755756699562\n",
      "5 fold: ls 0.004863099794036336 0.006903486185929679 auc 0.9986799145415753 0.996132954093071\n",
      "6 fold: ls 0.004569503380827582 0.006188719902135316 auc 0.9988580963963754 0.9954794979780837\n",
      "7 fold: ls 0.00498709779549681 0.0073243269307647785 auc 0.9985023011535654 0.9959417625243572\n",
      "8 fold: ls 0.005092622139933012 0.007118847270581577 auc 0.9984292111342984 0.994659787113677\n",
      "9 fold: ls 0.00388445760690332 0.007313659634954247 auc 0.9993480797605612 0.9944511537026411\n",
      "threat 0.095 0.4 3\n",
      "this class avg train 0.0046489170979663805 avg val 0.006810895926453722\n",
      "this class auc train 0.9988103412720957 auc val 0.9954693294913264\n",
      "========================\n",
      "0 fold: ls 0.05018096249458346 0.04984493632321642 auc 0.9916112320035961 0.991482989737292\n",
      "1 fold: ls 0.05066879284324924 0.053310952491023925 auc 0.9914261451563948 0.9898102386154879\n",
      "2 fold: ls 0.051260659163783404 0.055056485445885116 auc 0.9912489229025343 0.9888377993568657\n",
      "3 fold: ls 0.050303175330295 0.051257370904934305 auc 0.9915731070919366 0.990785731255584\n",
      "4 fold: ls 0.048406752814799635 0.05523427556396291 auc 0.9922737825552219 0.9891604504645295\n",
      "5 fold: ls 0.04345910969555771 0.05190878569321837 auc 0.9940351921313106 0.9907523291725411\n",
      "6 fold: ls 0.049056235787947315 0.05555562067844542 auc 0.9920480291015203 0.9894918269393262\n",
      "7 fold: ls 0.04936790799802893 0.05514170144326197 auc 0.9919040835528452 0.989654383568173\n",
      "8 fold: ls 0.049367436923989545 0.05228824970179308 auc 0.9919220165869072 0.9902202236002119\n",
      "9 fold: ls 0.04991022207271313 0.05417257972189585 auc 0.9917088610635384 0.9896387611897903\n",
      "insult 0.095 0.4 3\n",
      "this class avg train 0.049198125512494736 avg val 0.05337709579676374\n",
      "this class auc train 0.9919751372145805 auc val 0.9899834733899799\n",
      "========================\n",
      "0 fold: ls 0.015044865604511298 0.017004252117725827 auc 0.9952924716984135 0.9929302209625428\n",
      "1 fold: ls 0.014510681199286076 0.017388951508259847 auc 0.995733664835694 0.9922132439421271\n",
      "2 fold: ls 0.015065767853567325 0.017345314941278073 auc 0.9952274266464548 0.9928914351512443\n",
      "3 fold: ls 0.015241910474482218 0.01820355859395391 auc 0.9951307274659509 0.9888996353236955\n",
      "4 fold: ls 0.01493036399125096 0.018774530235074843 auc 0.9953810482366792 0.9899452828606621\n",
      "5 fold: ls 0.015145607047772703 0.016354230878900953 auc 0.9951839598590526 0.9935512423341973\n",
      "6 fold: ls 0.013549668696278721 0.01624126921154971 auc 0.9964081628119746 0.9917858949346052\n",
      "7 fold: ls 0.015250248427493186 0.01856875381947178 auc 0.9951079383193737 0.9887128314907146\n",
      "8 fold: ls 0.014167483434083361 0.016853594003453102 auc 0.9960029043817014 0.9925509429872101\n",
      "9 fold: ls 0.01490069408566706 0.016524212920252166 auc 0.9954359529690084 0.9923653262518969\n",
      "identity_hate 0.095 0.4 3\n",
      "this class avg train 0.014780729081439289 avg val 0.01732586682299202\n",
      "this class auc train 0.9954904257224303 auc val 0.9915846056238896\n",
      "========================\n",
      "all loss avg 0.03156046536896379 0.03500738836312626\n",
      "all auc avg 0.9946990094701901 0.9922432581576642\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.095 3 0.4 toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.4 severe_toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.4 obscene\n",
      "FIND BETTER PARAMS 0.095 3 0.4 threat\n",
      "FIND BETTER PARAMS 0.095 3 0.4 insult\n",
      "FIND BETTER PARAMS 0.095 3 0.4 identity_hate\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.48\n",
      "0 fold: ls 0.07057631307231102 0.07809440281598456 auc 0.9904510329458328 0.9871768492999269\n",
      "1 fold: ls 0.06920794057767342 0.07032593588374933 auc 0.9909131735894722 0.9901183428736063\n",
      "2 fold: ls 0.07002299503635256 0.07514293804059685 auc 0.9905902222949332 0.9887618211502325\n",
      "3 fold: ls 0.06833952224135462 0.07306878721248639 auc 0.9911923677901603 0.9891841118667224\n",
      "4 fold: ls 0.06843315701509747 0.07709138220250707 auc 0.9911115321390505 0.9881631403801524\n",
      "5 fold: ls 0.06391007900536469 0.07452495340744476 auc 0.9925522276081291 0.9883846230977009\n",
      "6 fold: ls 0.07002578174279787 0.07763772335013883 auc 0.9905827634930632 0.9879246135566281\n",
      "7 fold: ls 0.07156677935160481 0.0700904634555228 auc 0.9900729741951266 0.9904280738059131\n",
      "8 fold: ls 0.0701030434290233 0.07375622703225056 auc 0.9905358388741319 0.9894631790739359\n",
      "9 fold: ls 0.06777189345684807 0.07633387492861321 auc 0.9913414035949999 0.988220890423146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic 0.095 0.48 3\n",
      "this class avg train 0.06899575049284278 avg val 0.07460666883292945\n",
      "this class auc train 0.99093435365249 auc val 0.9887825645527965\n",
      "========================\n",
      "0 fold: ls 0.017338010487293602 0.02082985361076282 auc 0.9947028294463096 0.9915349253070009\n",
      "1 fold: ls 0.018022351308761387 0.020722802597329087 auc 0.9941522314154662 0.9915456070388657\n",
      "2 fold: ls 0.01818077125666185 0.02086609192852913 auc 0.9940943562770582 0.991387754779086\n",
      "3 fold: ls 0.017991023999446413 0.01950313932475885 auc 0.9941754221520326 0.992819502468667\n",
      "4 fold: ls 0.01813337086122452 0.020977748020218647 auc 0.9941094107729609 0.9914779560703886\n",
      "5 fold: ls 0.017902591874846427 0.019777176281426098 auc 0.9942654370520911 0.9924216185314437\n",
      "6 fold: ls 0.018022509887937056 0.019134665205145934 auc 0.9941715013369021 0.9929566277810091\n",
      "7 fold: ls 0.018005706358819408 0.020392079524569354 auc 0.9941866897314758 0.9919194911222295\n",
      "8 fold: ls 0.017943241774021 0.020913292443824172 auc 0.9942410490985106 0.9913308513717476\n",
      "9 fold: ls 0.01802601292966598 0.018703255187087146 auc 0.9941816938712804 0.9934156752157782\n",
      "severe_toxic 0.095 0.48 3\n",
      "this class avg train 0.017956559073867766 avg val 0.020182010412365125\n",
      "this class auc train 0.9942280621154087 auc val 0.9920810009686217\n",
      "========================\n",
      "0 fold: ls 0.03550823280129518 0.036694595291195446 auc 0.9961860843452476 0.9958084598979601\n",
      "1 fold: ls 0.035571531164850616 0.03673256011562219 auc 0.9961737295334554 0.9956655522480156\n",
      "2 fold: ls 0.034553696223880694 0.03900534039928079 auc 0.9963831028646477 0.9953326796996627\n",
      "3 fold: ls 0.03538146673332712 0.03351507437516874 auc 0.996206434947828 0.9965472793281565\n",
      "4 fold: ls 0.034444701384834374 0.042089385015586074 auc 0.9964165377224755 0.9945048568322991\n",
      "5 fold: ls 0.03336920019612725 0.03752901032715062 auc 0.9966483965056411 0.9955554737643347\n",
      "6 fold: ls 0.035853872556871995 0.03748491042117706 auc 0.9961205073922181 0.9955295529083045\n",
      "7 fold: ls 0.03533213249715676 0.03795979850995175 auc 0.9962191580623404 0.9956002675094991\n",
      "8 fold: ls 0.03344546706578972 0.038558890807231364 auc 0.9966479304539486 0.9953086383014712\n",
      "9 fold: ls 0.035297571340160074 0.038720518805552766 auc 0.9962264488132494 0.9953258952428502\n",
      "obscene 0.095 0.48 3\n",
      "this class avg train 0.03487578719642938 avg val 0.037829008406791674\n",
      "this class auc train 0.9963228330641052 auc val 0.9955178655732555\n",
      "========================\n",
      "0 fold: ls 0.004720921465404068 0.006743409189791411 auc 0.9987919671508358 0.9968193484181856\n",
      "1 fold: ls 0.004851117067997533 0.00632199781183387 auc 0.9987270477900374 0.9966569767441861\n",
      "2 fold: ls 0.004673914568119069 0.0071968583072768945 auc 0.9988329944976904 0.9943169914100146\n",
      "3 fold: ls 0.0037109069136083686 0.006358237425749418 auc 0.999439865309103 0.9932454375929768\n",
      "4 fold: ls 0.004132674502764672 0.006624804568681951 auc 0.9991871737000626 0.9966109330148554\n",
      "5 fold: ls 0.005118062510960585 0.006980587027941964 auc 0.9983916937581695 0.996076644247491\n",
      "6 fold: ls 0.004017483937043904 0.006268604303612544 auc 0.9992611880761004 0.9957086659542816\n",
      "7 fold: ls 0.004963968343507584 0.007373811679452376 auc 0.998547096336605 0.9959286672114316\n",
      "8 fold: ls 0.0052450674336632684 0.0071572570346669015 auc 0.9982921558073736 0.9945942548243133\n",
      "9 fold: ls 0.004326830528690064 0.007379093497507071 auc 0.9990763511276455 0.993821241288552\n",
      "threat 0.095 0.48 3\n",
      "this class avg train 0.0045760947271759125 avg val 0.00684046608465144\n",
      "this class auc train 0.9988547533553623 auc val 0.995377916070629\n",
      "========================\n",
      "0 fold: ls 0.04925275935534324 0.04960843113715681 auc 0.9919459429586764 0.9915904854960197\n",
      "1 fold: ls 0.050977205651953995 0.05338435398833609 auc 0.9913152312971167 0.9898527349932573\n",
      "2 fold: ls 0.05133983645459956 0.05511076411756965 auc 0.9912227341115181 0.9888415637997785\n",
      "3 fold: ls 0.050383551597495115 0.051108509283326116 auc 0.9915423094828688 0.9909038092816104\n",
      "4 fold: ls 0.05098172983170385 0.055383828749394494 auc 0.9913065020047902 0.9891048166963547\n",
      "5 fold: ls 0.04537576889222667 0.05205735990036866 auc 0.9933914088716747 0.9907011293738599\n",
      "6 fold: ls 0.049303920521404594 0.05589660497594918 auc 0.9919381766794843 0.9893197387270926\n",
      "7 fold: ls 0.048919194353113274 0.055193021174239756 auc 0.9920708188774924 0.9896521218833669\n",
      "8 fold: ls 0.04668930431673057 0.052156194674645204 auc 0.9929259922068243 0.9903487208036386\n",
      "9 fold: ls 0.04916121005493522 0.05434665504386241 auc 0.9919981099469529 0.9895633298132025\n",
      "insult 0.095 0.48 3\n",
      "this class avg train 0.04923844810295061 avg val 0.05342457230448483\n",
      "this class auc train 0.9919657226437399 auc val 0.9899878450868181\n",
      "========================\n",
      "0 fold: ls 0.014624005257450954 0.01708093743926495 auc 0.9956071928228456 0.9930651866180431\n",
      "1 fold: ls 0.014814675652557363 0.017510659053069106 auc 0.9955375932000252 0.9921060785213145\n",
      "2 fold: ls 0.013927658969467155 0.017125465700188792 auc 0.9961436040019667 0.9929270822263684\n",
      "3 fold: ls 0.015111241972300748 0.018226847570588022 auc 0.9952513749635301 0.9875365270422299\n",
      "4 fold: ls 0.014636282811438755 0.018598698607871778 auc 0.9955930622597999 0.9900950454152705\n",
      "5 fold: ls 0.014767265326495577 0.01633141437493889 auc 0.9954586507686101 0.9936871720300942\n",
      "6 fold: ls 0.013805828983614469 0.016298767854920855 auc 0.9962498989991545 0.9917402810896742\n",
      "7 fold: ls 0.01503660332087648 0.018533855244056513 auc 0.9953319005790714 0.9884407290989233\n",
      "8 fold: ls 0.01396588558387468 0.016783642709427254 auc 0.9961634617120024 0.9927207529445768\n",
      "9 fold: ls 0.01475568872510276 0.01658028700335539 auc 0.9955655283767255 0.992253775561818\n",
      "identity_hate 0.095 0.48 3\n",
      "this class avg train 0.014544513660317896 avg val 0.017307057555768158\n",
      "this class auc train 0.995690226768373 auc val 0.9914572630548312\n",
      "========================\n",
      "all loss avg 0.03169785887559739 0.035031630599498445\n",
      "all auc avg 0.9946659919332465 0.9922007425511588\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.095 3 0.48 toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.48 insult\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.56\n",
      "0 fold: ls 0.07125140084612051 0.07840555064043322 auc 0.9902175161165504 0.9870534508970393\n",
      "1 fold: ls 0.06931289501674336 0.07045513168110573 auc 0.9908858703423804 0.9900478100860527\n",
      "2 fold: ls 0.07007872512182346 0.07534036840810046 auc 0.9905746379444801 0.9887068717145854\n",
      "3 fold: ls 0.06684402938535874 0.07304311921755523 auc 0.9916728758438571 0.989113126074753\n",
      "4 fold: ls 0.06632060568713306 0.07715221313693976 auc 0.9917641329049176 0.9881446003818969\n",
      "5 fold: ls 0.06627237183307667 0.0748568473753132 auc 0.9918564819221114 0.9881982258536242\n",
      "6 fold: ls 0.0683433486104414 0.07710633433701242 auc 0.9911632667676804 0.9881066591140728\n",
      "7 fold: ls 0.06932067730984964 0.07020274186166733 auc 0.9908246571353322 0.9903377247161609\n",
      "8 fold: ls 0.06806122595354303 0.07371108131479513 auc 0.9912161460435013 0.9894713617185422\n",
      "9 fold: ls 0.07006442437649732 0.07640516073507628 auc 0.9905539110832431 0.9881772798740535\n",
      "toxic 0.095 0.56 3\n",
      "this class avg train 0.06858697041405871 avg val 0.07466785487079988\n",
      "this class auc train 0.9910729496104054 auc val 0.9887357110430781\n",
      "========================\n",
      "0 fold: ls 0.016679659160490187 0.020794382642953614 auc 0.9952792698439601 0.9916583586529941\n",
      "1 fold: ls 0.017199389706924216 0.02059601057058624 auc 0.9948473089849633 0.9917901000126599\n",
      "2 fold: ls 0.017955800416849545 0.02079971193710927 auc 0.9942408453691286 0.991411096341309\n",
      "3 fold: ls 0.018097157048849552 0.019599441621026652 auc 0.9940963560277756 0.9927122895303203\n",
      "4 fold: ls 0.01798450180012033 0.021056457833932205 auc 0.9942150471144701 0.9913418628940371\n",
      "5 fold: ls 0.018034124899134446 0.01970939708978789 auc 0.9941586791769632 0.9925354773830937\n",
      "6 fold: ls 0.01771228290911033 0.019106261646379897 auc 0.9944488891273842 0.9929852933623652\n",
      "7 fold: ls 0.016801382361102502 0.02032876107196263 auc 0.9952039913553202 0.9920512731698519\n",
      "8 fold: ls 0.017984530936840278 0.020770326497286964 auc 0.99421719631503 0.9914773643431223\n",
      "9 fold: ls 0.01811578974707309 0.018696744646896805 auc 0.994101948677391 0.9934630530516304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe_toxic 0.095 0.56 3\n",
      "this class avg train 0.01765646189864945 avg val 0.020145749555792215\n",
      "this class auc train 0.9944809531992387 auc val 0.9921426168741385\n",
      "========================\n",
      "0 fold: ls 0.03546032253236468 0.036789235477717584 auc 0.9961959904338261 0.9957806222708064\n",
      "1 fold: ls 0.03567423895938666 0.036618632286133504 auc 0.9961456410102575 0.9957162159463795\n",
      "2 fold: ls 0.0344042868464071 0.038957553457418546 auc 0.9964192552292838 0.995349281577241\n",
      "3 fold: ls 0.035077750555038645 0.03364969878765119 auc 0.9962712678628814 0.9964768779699349\n",
      "4 fold: ls 0.034458421082097984 0.04209645811697498 auc 0.9964134933993349 0.9945190310768354\n",
      "5 fold: ls 0.034588086569177584 0.037392540130312686 auc 0.9963802499901278 0.9956211764779588\n",
      "6 fold: ls 0.030969352557429462 0.03728326995636316 auc 0.997173883325641 0.9954494410179144\n",
      "7 fold: ls 0.035132440276023615 0.03807799117253782 auc 0.9962577688013085 0.9955749339840434\n",
      "8 fold: ls 0.03346531172800258 0.03886290368261612 auc 0.996639396003195 0.995223122969794\n",
      "9 fold: ls 0.03349854607292828 0.03844094170078595 auc 0.9966294425406028 0.9954239780570477\n",
      "obscene 0.095 0.56 3\n",
      "this class avg train 0.03427287571788566 avg val 0.03781692247685116\n",
      "this class auc train 0.9964526388596457 auc val 0.9955134681347955\n",
      "========================\n",
      "0 fold: ls 0.0051139390762320125 0.006892353897326672 auc 0.9983679366898988 0.9966910224177666\n",
      "1 fold: ls 0.004803099346786954 0.006334386992153647 auc 0.9987434441109596 0.9966687617850409\n",
      "2 fold: ls 0.004632610747381895 0.007259178922502522 auc 0.9988543527562467 0.9941336685522733\n",
      "3 fold: ls 0.004486921648120602 0.006401034922600064 auc 0.9989726262126208 0.9932532947807321\n",
      "4 fold: ls 0.004766425742009902 0.006568395587047218 auc 0.9987592319006671 0.9968387914597607\n",
      "5 fold: ls 0.004314767285369763 0.006914764287989072 auc 0.9990638407695286 0.9959692626815011\n",
      "6 fold: ls 0.003935022938101973 0.006317375500559509 auc 0.9992989992385793 0.995038185932491\n",
      "7 fold: ls 0.004896553853325761 0.007447609665912093 auc 0.9986148007312756 0.995597355794414\n",
      "8 fold: ls 0.004573729946643317 0.007088532772919022 auc 0.9989249064497738 0.9948323108958799\n",
      "9 fold: ls 0.004563326245453291 0.007210247069206831 auc 0.9989045215359322 0.9946129783355601\n",
      "threat 0.095 0.56 3\n",
      "this class avg train 0.004608639682942547 avg val 0.006843387961821665\n",
      "this class auc train 0.9988504660395483 auc val 0.995363563263542\n",
      "========================\n",
      "0 fold: ls 0.049292075972644046 0.04979954773516128 auc 0.9919505057970386 0.9914949523003255\n",
      "1 fold: ls 0.04945851375990819 0.05327630210997892 auc 0.9918829492347159 0.9900345994130816\n",
      "2 fold: ls 0.05066543698845716 0.055059841157379 auc 0.9914322754371925 0.9890071992879347\n",
      "3 fold: ls 0.049376937384411096 0.051159475613373674 auc 0.9919067568977871 0.9908817663769997\n",
      "4 fold: ls 0.049920708711624004 0.055480090666818443 auc 0.9917087732918031 0.9890685920021899\n",
      "5 fold: ls 0.04722757914349187 0.05231479533816461 auc 0.9927332903865518 0.9906444916880641\n",
      "6 fold: ls 0.04911535635073413 0.05583296305746721 auc 0.9920051412148608 0.9894064102817227\n",
      "7 fold: ls 0.049048158351011305 0.055016432923961175 auc 0.9920197306248256 0.9896997847965023\n",
      "8 fold: ls 0.05008445270597792 0.05222660019302162 auc 0.9916455378296003 0.9904326544397751\n",
      "9 fold: ls 0.05090363782287997 0.05456497057328239 auc 0.9913354684028224 0.9894988299131773\n",
      "insult 0.095 0.56 3\n",
      "this class avg train 0.04950928571911396 avg val 0.053473101936860834\n",
      "this class auc train 0.9918620429117198 auc val 0.9900169280499773\n",
      "========================\n",
      "0 fold: ls 0.014795610661323868 0.01715564603173885 auc 0.9955111633504427 0.9929208047540194\n",
      "1 fold: ls 0.015016603734639936 0.01743391806101592 auc 0.9953281440755654 0.9922899187829596\n",
      "2 fold: ls 0.014898435074750435 0.01729282024117518 auc 0.995382812820265 0.993056667191284\n",
      "3 fold: ls 0.015068300236184147 0.018203158964572735 auc 0.9953008388813694 0.9872087533074432\n",
      "4 fold: ls 0.014428613890133972 0.01864820230289708 auc 0.9957625206403481 0.9898605369839525\n",
      "5 fold: ls 0.015036464009673185 0.01632181686416442 auc 0.9952849364239985 0.9936600764096497\n",
      "6 fold: ls 0.013793902927085128 0.016367484194162348 auc 0.9962671364767851 0.9917678300455236\n",
      "7 fold: ls 0.015015705419997047 0.018487423847468272 auc 0.9953442122697633 0.9861083261073778\n",
      "8 fold: ls 0.013913456167627456 0.01676794457317766 auc 0.9962133359989226 0.9927266240335285\n",
      "9 fold: ls 0.015209300937428564 0.016463394754236547 auc 0.9952083727271339 0.9924177144302335\n",
      "identity_hate 0.095 0.56 3\n",
      "this class avg train 0.014717639305884373 avg val 0.0173141809834609\n",
      "this class auc train 0.9955603473664594 auc val 0.9912017252045973\n",
      "========================\n",
      "all loss avg 0.03155864545642245 0.03504353296426444\n",
      "all auc avg 0.9947132329978362 0.9921623354283549\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.095 3 0.56 severe_toxic\n",
      "FIND BETTER PARAMS 0.095 3 0.56 insult\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.095 max depth 3 feature fraction 0.62\n",
      "0 fold: ls 0.06996259609465623 0.07853014437086923 auc 0.9906367319544698 0.9869351261436097\n",
      "1 fold: ls 0.0696749868812875 0.0705097580050801 auc 0.9907114412144324 0.9901385921709964\n",
      "2 fold: ls 0.06986005987706473 0.07511046039087957 auc 0.9906393197410942 0.9887553884875271\n",
      "3 fold: ls 0.0667919248558969 0.07320282394014764 auc 0.9916595251029615 0.9890899322486596\n",
      "4 fold: ls 0.06842255142126624 0.07726798136953893 auc 0.9911137348148976 0.9881173116803077\n",
      "5 fold: ls 0.0655664523556384 0.07503816971895455 auc 0.9920617615761056 0.9882252425747987\n",
      "6 fold: ls 0.06716877421777116 0.07709294900268131 auc 0.9915443867562874 0.988093377403831\n",
      "7 fold: ls 0.07112475830165785 0.0702479609006435 auc 0.990232892992711 0.9902994181527687\n",
      "8 fold: ls 0.06905266005295335 0.07391542831239724 auc 0.9908762215644332 0.9894169618652042\n",
      "9 fold: ls 0.06919492620568721 0.07634634110306468 auc 0.9908369342871101 0.988287484910274\n",
      "toxic 0.095 0.62 3\n",
      "this class avg train 0.06868196902638797 avg val 0.07472620171142567\n",
      "this class auc train 0.9910312950004503 auc val 0.9887358835637976\n",
      "========================\n",
      "0 fold: ls 0.017700012474516042 0.020901706878908858 auc 0.9943966151670407 0.9914791429294847\n",
      "1 fold: ls 0.017355721576742176 0.020572306966410762 auc 0.9947135879091151 0.9917968255475377\n",
      "2 fold: ls 0.017894206908000386 0.02087519925200767 auc 0.9942941205907518 0.9912912235726042\n",
      "3 fold: ls 0.01790170498286413 0.019575646762582655 auc 0.994266222107705 0.9927589726547665\n",
      "4 fold: ls 0.017980633431747568 0.02101366473299023 auc 0.9942257124516293 0.9913881503987847\n",
      "5 fold: ls 0.018059402959304124 0.01977439501899914 auc 0.9941396800808872 0.9924674009368275\n",
      "6 fold: ls 0.017875508873340586 0.018886863423423796 auc 0.994308000971979 0.9931568887174264\n",
      "7 fold: ls 0.01724614662895425 0.020249870570259336 auc 0.9948181640111906 0.9920580414321165\n",
      "8 fold: ls 0.01802844157201082 0.02086169788501207 auc 0.9941900594881563 0.9914088854543275\n",
      "9 fold: ls 0.017061492350019986 0.01862332169048112 auc 0.9950109674804569 0.9935092364882592\n",
      "severe_toxic 0.095 0.62 3\n",
      "this class avg train 0.017710327175750003 avg val 0.020133467318107563\n",
      "this class auc train 0.9944363130258914 auc val 0.9921314768132137\n",
      "========================\n",
      "0 fold: ls 0.035023967627501484 0.036695277348049414 auc 0.9962894929838186 0.9958065805644813\n",
      "1 fold: ls 0.035841602824422565 0.036812429915914296 auc 0.9961188370393952 0.9956296099952351\n",
      "2 fold: ls 0.03303443204532541 0.038982919340873244 auc 0.9967199857198666 0.9953203066022221\n",
      "3 fold: ls 0.03563509030993732 0.03357272617974798 auc 0.9961551615263613 0.9965059704110688\n",
      "4 fold: ls 0.03454370921322424 0.04219284772645947 auc 0.9963939245462696 0.9944973390009428\n",
      "5 fold: ls 0.03365937288068428 0.03741706273788013 auc 0.9965885011278645 0.995608490137545\n",
      "6 fold: ls 0.03374594167139081 0.03725139677266719 auc 0.996573814698079 0.995552889509806\n",
      "7 fold: ls 0.03505466162389811 0.03793621142819386 auc 0.9962779985388796 0.9956157730366714\n",
      "8 fold: ls 0.03501833181586512 0.03872550847199984 auc 0.9963047887758973 0.9952520196340695\n",
      "9 fold: ls 0.033271488319472214 0.03857515085681895 auc 0.9966751178184727 0.9953396158603438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obscene 0.095 0.62 3\n",
      "this class avg train 0.034482859833172154 avg val 0.037816153077860445\n",
      "this class auc train 0.9964097622774905 auc val 0.9955128594752386\n",
      "========================\n",
      "0 fold: ls 0.003853004235446458 0.006753863671145623 auc 0.9993611850438916 0.9969686256023466\n",
      "1 fold: ls 0.0049664287630285615 0.006373117100323511 auc 0.9986183074546494 0.9965574586214121\n",
      "2 fold: ls 0.004238837363409342 0.007247360762065442 auc 0.999134072854238 0.9941533102870312\n",
      "3 fold: ls 0.0036071039895356895 0.006393554573797135 auc 0.9994985473237233 0.9934117480671318\n",
      "4 fold: ls 0.004340516439479098 0.006680940232611025 auc 0.9990511152343902 0.9966017662958074\n",
      "5 fold: ls 0.0047168221443877 0.0068737882384817695 auc 0.9987889870766384 0.9958003331447608\n",
      "6 fold: ls 0.004885128575826627 0.006268601765275631 auc 0.9986429073535564 0.9956916420474784\n",
      "7 fold: ls 0.005192192179282805 0.007444071123339415 auc 0.9981775117136643 0.9955764032937331\n",
      "8 fold: ls 0.004409209094146621 0.007036177273871708 auc 0.9990075479923743 0.994978086804873\n",
      "9 fold: ls 0.0041813365591853325 0.007229915753315764 auc 0.999156027297247 0.9947320063713434\n",
      "threat 0.095 0.62 3\n",
      "this class avg train 0.004439057934372823 avg val 0.006830139049422703\n",
      "this class auc train 0.9989436209344372 auc val 0.9954471380535918\n",
      "========================\n",
      "0 fold: ls 0.04999076049008611 0.0498486638191776 auc 0.9916921428241644 0.9914745406543104\n",
      "1 fold: ls 0.051171210039664544 0.05345146165976281 auc 0.9912254624119472 0.9896308001699854\n",
      "2 fold: ls 0.051222046491645024 0.0549774972049151 auc 0.9912631709232762 0.9888292666195972\n",
      "3 fold: ls 0.04985893362904846 0.05118000240155541 auc 0.9917250895962288 0.9908637807053061\n",
      "4 fold: ls 0.0498667624078434 0.055384810050843836 auc 0.9917315537360819 0.9890823122096796\n",
      "5 fold: ls 0.04903725013330802 0.05267172463695408 auc 0.9920700408123581 0.9905067039945546\n",
      "6 fold: ls 0.04965211397670843 0.05605198925120693 auc 0.9917934424609348 0.989266949392178\n",
      "7 fold: ls 0.04868772579543763 0.055119427252397546 auc 0.9921591766621064 0.9896670322498663\n",
      "8 fold: ls 0.05002379652231413 0.05242574563309688 auc 0.9916844257097837 0.9904471459757549\n",
      "9 fold: ls 0.04865162946119003 0.05439553515500744 auc 0.9921968284403653 0.9895056149675955\n",
      "insult 0.095 0.62 3\n",
      "this class avg train 0.04981622289472457 avg val 0.053550685706491755\n",
      "this class auc train 0.9917541333577246 auc val 0.9899274146938829\n",
      "========================\n",
      "0 fold: ls 0.014327539519613288 0.017109662451761234 auc 0.9958497188582065 0.9928535461217104\n",
      "1 fold: ls 0.014338319017740915 0.01745990033695573 auc 0.9958843797260273 0.9922392506132865\n",
      "2 fold: ls 0.015058511502664573 0.017422213257606737 auc 0.9952440859828283 0.9928210377827609\n",
      "3 fold: ls 0.01529469495579459 0.01822800201645398 auc 0.995098895489611 0.9879898502239935\n",
      "4 fold: ls 0.01444676265291993 0.018385385658261713 auc 0.9957860521266549 0.9899614249324162\n",
      "5 fold: ls 0.014578794430709264 0.016304443851217452 auc 0.995659920836439 0.9936718178451757\n",
      "6 fold: ls 0.015240449348650355 0.016455667750198542 auc 0.9950833232679569 0.992373681263097\n",
      "7 fold: ls 0.015247846521161014 0.018531636459713088 auc 0.9951306930115746 0.986231844786473\n",
      "8 fold: ls 0.013758564499253206 0.016899069508634578 auc 0.9963177604607028 0.9925360394537178\n",
      "9 fold: ls 0.015226439277585244 0.016687679227469023 auc 0.9951910463965259 0.9920564166486018\n",
      "identity_hate 0.095 0.62 3\n",
      "this class avg train 0.014751792172609237 avg val 0.017348366051827206\n",
      "this class auc train 0.9955245876156527 auc val 0.9912734909671232\n",
      "========================\n",
      "all loss avg 0.03164703817283613 0.03506750215252256\n",
      "all auc avg 0.9946832853686077 0.9921713772611412\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.06912962091204919 0.07798384222633009 auc 0.9909440040596726 0.9871119790675719\n",
      "1 fold: ls 0.06894564932411025 0.07058629494707462 auc 0.990963202359057 0.9900183194985784\n",
      "2 fold: ls 0.06938087867410038 0.07510520388658766 auc 0.9908023038581448 0.9887659887908586\n",
      "3 fold: ls 0.0669044433950994 0.07304076286664023 auc 0.9916273263230982 0.9891877359020496\n",
      "4 fold: ls 0.06901943632680967 0.07705237970723355 auc 0.9909170756408257 0.9881678093772681\n",
      "5 fold: ls 0.06755200290383222 0.07518385834526098 auc 0.9914469576535736 0.9880644114896856\n",
      "6 fold: ls 0.06992031340437828 0.0773916071199414 auc 0.9906253399807371 0.9880189907604627\n",
      "7 fold: ls 0.07079708138753416 0.0699798054709485 auc 0.9903200703373826 0.9904169218359788\n",
      "8 fold: ls 0.0685758399243408 0.07366847971341671 auc 0.9910784935704262 0.9894956603196997\n",
      "9 fold: ls 0.06695519402924777 0.07613121115159062 auc 0.9915907060327368 0.9882856262486183\n",
      "toxic 0.075 0.4 3\n",
      "this class avg train 0.06871804602815021 avg val 0.07461234454350243\n",
      "this class auc train 0.9910315479815655 auc val 0.9887533443290772\n",
      "========================\n",
      "0 fold: ls 0.017903172293940586 0.020897251503560876 auc 0.994269295254028 0.9915028801114064\n",
      "1 fold: ls 0.01760054057424087 0.0206891966702014 auc 0.9945086918821834 0.9915274085327257\n",
      "2 fold: ls 0.018017799968658453 0.020793217735618753 auc 0.9941960837921036 0.9913268293454867\n",
      "3 fold: ls 0.01798642404135441 0.019576293068831813 auc 0.9941885112066666 0.9927384004304343\n",
      "4 fold: ls 0.01792519970296767 0.021000862292151073 auc 0.994253326656388 0.9913153563742246\n",
      "5 fold: ls 0.017641262697699483 0.01972614199980236 auc 0.9944807258553963 0.9924849176832351\n",
      "6 fold: ls 0.017158030665086425 0.019028426955487173 auc 0.9949301912781645 0.9930330693312918\n",
      "7 fold: ls 0.017838619781890094 0.0204018478943776 auc 0.9943311509065007 0.9919676652242304\n",
      "8 fold: ls 0.01648712438242492 0.020730850669799017 auc 0.9954543108895588 0.9915826705412978\n",
      "9 fold: ls 0.018214675562324957 0.018669960058548547 auc 0.9940226540904995 0.9934861447699449\n",
      "severe_toxic 0.075 0.4 3\n",
      "this class avg train 0.01767728496705879 avg val 0.02015140488483786\n",
      "this class auc train 0.994463494181149 auc val 0.9920965342344277\n",
      "========================\n",
      "0 fold: ls 0.03488821663931895 0.03683901489693751 auc 0.9963223049079639 0.9957953828691706\n",
      "1 fold: ls 0.03497138347832159 0.03654431872034443 auc 0.9963055946080308 0.9957211491967611\n",
      "2 fold: ls 0.034419076014209744 0.03890705494789964 auc 0.9964163201672247 0.995372148314283\n",
      "3 fold: ls 0.035524502741727526 0.03357829397233658 auc 0.9961659566178973 0.9965121961151606\n",
      "4 fold: ls 0.03402992431772836 0.04184579470156114 auc 0.9965099511125312 0.994579095416942\n",
      "5 fold: ls 0.035040315062247314 0.03720048972220234 auc 0.9962782567044437 0.9956634642793376\n",
      "6 fold: ls 0.035789123819127196 0.03752810480170758 auc 0.9961324081478832 0.9954458387237228\n",
      "7 fold: ls 0.035221480345810054 0.03796993262971776 auc 0.9962387955661738 0.9955933761640892\n",
      "8 fold: ls 0.03462207794358659 0.03862827441650953 auc 0.9963852068656492 0.9952877293330117\n",
      "9 fold: ls 0.035211359411148314 0.03860946245516956 auc 0.996254707457843 0.9953452609143985\n",
      "obscene 0.075 0.4 3\n",
      "this class avg train 0.03497174597732257 avg val 0.037765074126438605\n",
      "this class auc train 0.996300950215564 auc val 0.9955315641326875\n",
      "========================\n",
      "0 fold: ls 0.004990249490075659 0.0066559773832055745 auc 0.9985446661281895 0.9968926775612821\n",
      "1 fold: ls 0.004868098195896126 0.006198683924712834 auc 0.9986881968740929 0.9970485019903625\n",
      "2 fold: ls 0.0051348001146550885 0.007187488162243416 auc 0.9983729392325872 0.9943038969201761\n",
      "3 fold: ls 0.003430585722959837 0.00639487126027406 auc 0.999587845335454 0.9936671066691809\n",
      "4 fold: ls 0.004862817518103965 0.006709317953882006 auc 0.9986650450745439 0.9966253378590735\n",
      "5 fold: ls 0.004587357610717635 0.0066527989761644615 auc 0.9988556925939497 0.9965506945753976\n",
      "6 fold: ls 0.004671121844114426 0.006188906313853121 auc 0.9987961172743739 0.9956130701699247\n",
      "7 fold: ls 0.005086123767523597 0.007321607826033143 auc 0.9983899071482587 0.9959470006495275\n",
      "8 fold: ls 0.004488928319099388 0.007124657305362967 auc 0.9989294193261687 0.9943067151873086\n",
      "9 fold: ls 0.0045310602481752815 0.007325319345283598 auc 0.9989123643803673 0.9940847078396678\n",
      "threat 0.075 0.4 3\n",
      "this class avg train 0.004665114283132101 avg val 0.006775962845101518\n",
      "this class auc train 0.9987742193367986 auc val 0.9955039709421902\n",
      "========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.04964303462319662 0.049855262350365 auc 0.9918298576759376 0.991470525248537\n",
      "1 fold: ls 0.048738175748239236 0.05327538463273099 auc 0.9921642219751595 0.9900521668133405\n",
      "2 fold: ls 0.05097845663910874 0.054979824420283326 auc 0.9913190548440731 0.9891233114382181\n",
      "3 fold: ls 0.04764560117465539 0.05097872414886621 auc 0.9925681406283645 0.9909050222687712\n",
      "4 fold: ls 0.049573945956825974 0.05518398631959181 auc 0.9918353399833425 0.9891773497444861\n",
      "5 fold: ls 0.04865458548399649 0.05237936184561952 auc 0.9921926661951458 0.9905999010137224\n",
      "6 fold: ls 0.04835672286624806 0.05552788819068433 auc 0.9922873764282304 0.989531649004967\n",
      "7 fold: ls 0.049454954408126015 0.055150017757222745 auc 0.9918819328956269 0.9896573153818103\n",
      "8 fold: ls 0.050641004089096345 0.052421299267702046 auc 0.9914398696850006 0.9904412823484798\n",
      "9 fold: ls 0.04984429784189032 0.054342190610639086 auc 0.9917436795701485 0.9895776538169742\n",
      "insult 0.075 0.4 3\n",
      "this class avg train 0.049353077883138315 avg val 0.0534093939543705\n",
      "this class auc train 0.9919262139881029 auc val 0.9900536177079309\n",
      "========================\n",
      "0 fold: ls 0.01512022290476167 0.01708767886167267 auc 0.9952177200472968 0.9928889690013931\n",
      "1 fold: ls 0.015044922759565156 0.0174703840517532 auc 0.9952980766828115 0.9919294125137824\n",
      "2 fold: ls 0.014832615125737149 0.01728120482824934 auc 0.9954088286179511 0.9930450090283505\n",
      "3 fold: ls 0.015409189165066086 0.018138548736489422 auc 0.9950351481707778 0.9878779766989194\n",
      "4 fold: ls 0.014258160006348457 0.018666741720044486 auc 0.9959311648114622 0.9896233382073423\n",
      "5 fold: ls 0.015155549185992932 0.016227552099450204 auc 0.9952041101899061 0.9938497457527615\n",
      "6 fold: ls 0.014663562079140513 0.016420265366207755 auc 0.9956022885195012 0.9915126634872462\n",
      "7 fold: ls 0.014817751060426158 0.01861509398180717 auc 0.9954827812647944 0.9864879145892044\n",
      "8 fold: ls 0.015018896209229872 0.01694736972058495 auc 0.9953565989301547 0.9925288134980852\n",
      "9 fold: ls 0.01486364579002763 0.016499090874757556 auc 0.9954642720789237 0.9924561023195319\n",
      "identity_hate 0.075 0.4 3\n",
      "this class avg train 0.014918451428629562 avg val 0.017335393024101677\n",
      "this class auc train 0.995400098931358 auc val 0.9912199945096617\n",
      "========================\n",
      "all loss avg 0.03171728676123859 0.03500826222972543\n",
      "all auc avg 0.9946494207724229 0.9921931709759959\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.075 3 0.4 threat\n",
      "FIND BETTER PARAMS 0.075 3 0.4 insult\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.48\n",
      "0 fold: ls 0.06900051020253198 0.07821500418548313 auc 0.9909702713528323 0.9871220357656046\n",
      "1 fold: ls 0.06744115530865395 0.07043493605523644 auc 0.9914926237763634 0.9900937447338237\n",
      "2 fold: ls 0.07087328292256354 0.07519636090092015 auc 0.9902948288297375 0.9886817752699453\n",
      "3 fold: ls 0.06575886915417056 0.07332348732217413 auc 0.991994186062035 0.9890896151455684\n",
      "4 fold: ls 0.06702297846436119 0.07700641755739725 auc 0.9915661936388117 0.9882176271231925\n",
      "5 fold: ls 0.06780317043281896 0.07495502739778477 auc 0.9913541185312127 0.9881754248288745\n",
      "6 fold: ls 0.06794844841693994 0.07705020017916606 auc 0.9912922483486973 0.9881358970086324\n",
      "7 fold: ls 0.07156912256689753 0.07010187473541563 auc 0.9900622216167442 0.9903793406039645\n",
      "8 fold: ls 0.0663188820415637 0.07373059014188268 auc 0.9918068748749045 0.9895090789501897\n",
      "9 fold: ls 0.06708701555921963 0.0761400242973494 auc 0.9915436456690017 0.9883082475209648\n",
      "toxic 0.075 0.48 3\n",
      "this class avg train 0.06808234350697209 avg val 0.07461539227728095\n",
      "this class auc train 0.9912377212700341 auc val 0.9887712786950761\n",
      "========================\n",
      "0 fold: ls 0.01775203573263548 0.020983077297795933 auc 0.9943621660299895 0.9913822161033042\n",
      "1 fold: ls 0.017847859505212574 0.020685363089890704 auc 0.9942827028963988 0.9915808171920497\n",
      "2 fold: ls 0.018018159256769906 0.020838544671958163 auc 0.9942049576859121 0.9912864761362198\n",
      "3 fold: ls 0.017854339560826635 0.01951519720221519 auc 0.994283492013716 0.9927878528927712\n",
      "4 fold: ls 0.01790164597090271 0.021022110659911324 auc 0.9942741010275041 0.9913181257121156\n",
      "5 fold: ls 0.01796636625090785 0.01969030929843206 auc 0.9942078936439319 0.9925219417154151\n",
      "6 fold: ls 0.018034516124115968 0.01913531345265392 auc 0.9941591855619403 0.992957424047158\n",
      "7 fold: ls 0.017822379197437434 0.02028457314121108 auc 0.9943294268449431 0.9920703835574225\n",
      "8 fold: ls 0.017979363094951184 0.02081010705036082 auc 0.9942102339471208 0.9914556660905681\n",
      "9 fold: ls 0.017828299612887403 0.0186912764014704 auc 0.9943417915989262 0.993443146397911\n",
      "severe_toxic 0.075 0.48 3\n",
      "this class avg train 0.01790049643066471 avg val 0.020165587226589957\n",
      "this class auc train 0.9942655951250382 auc val 0.9920804049844936\n",
      "========================\n",
      "0 fold: ls 0.035218855939635106 0.03673729208638222 auc 0.9962475760637632 0.9958071287034126\n",
      "1 fold: ls 0.03570600455630999 0.03661778815599352 auc 0.9961499360659398 0.9957053314733153\n",
      "2 fold: ls 0.03517088396306734 0.038979122220116694 auc 0.9962537348435059 0.9953563295441374\n",
      "3 fold: ls 0.034282324722139185 0.033477795762850454 auc 0.9964470322002269 0.9965233945514517\n",
      "4 fold: ls 0.0339881419098254 0.04191372220121818 auc 0.9965214573680085 0.9945798002136317\n",
      "5 fold: ls 0.03435498355953051 0.03744254240953294 auc 0.9964371140980792 0.9956286159985717\n",
      "6 fold: ls 0.035822627894332125 0.0375266793867095 auc 0.9961211199311124 0.9954714463367801\n",
      "7 fold: ls 0.035256811143519085 0.03804794880071115 auc 0.9962337777751822 0.995575756246848\n",
      "8 fold: ls 0.03435864103472897 0.03858485731433472 auc 0.9964424335656848 0.9952891389263911\n",
      "9 fold: ls 0.03518059543632225 0.03875838336661736 auc 0.9962615556799808 0.9953091168877437\n",
      "obscene 0.075 0.48 3\n",
      "this class avg train 0.03493398701594099 avg val 0.037808613170446675\n",
      "this class auc train 0.9963115737591485 auc val 0.9955246058882284\n",
      "========================\n",
      "0 fold: ls 0.00445215760242965 0.006774251678543465 auc 0.9989897462492705 0.996806253928347\n",
      "1 fold: ls 0.004487652139964782 0.006307799960245797 auc 0.9989848411587123 0.9967394720301698\n",
      "2 fold: ls 0.005174437520676423 0.007190921284007753 auc 0.9983209322790527 0.9940223653886444\n",
      "3 fold: ls 0.00395021371453625 0.0064232945988731745 auc 0.9993025399745846 0.9941385379345025\n",
      "4 fold: ls 0.003954933022088513 0.0067246437592770585 auc 0.9992982358688902 0.996483908479477\n",
      "5 fold: ls 0.004744841435906045 0.0068346525295885485 auc 0.9987471641628141 0.9963136694114443\n",
      "6 fold: ls 0.0049309135310533 0.006363269816254135 auc 0.9985847044752305 0.9952699729712742\n",
      "7 fold: ls 0.00495426815024161 0.007308540082984708 auc 0.9985569551749318 0.9958645001780962\n",
      "8 fold: ls 0.004416093139667448 0.00714131496917221 auc 0.9990136812911733 0.9945006372680792\n",
      "9 fold: ls 0.005194616263489519 0.007343025660622192 auc 0.9983386781345905 0.9936085956965346\n",
      "threat 0.075 0.48 3\n",
      "this class avg train 0.004626012652005354 avg val 0.006841171433956905\n",
      "this class auc train 0.998813747876925 auc val 0.995374791328657\n",
      "========================\n",
      "0 fold: ls 0.049006760196562735 0.04984477888650496 auc 0.9920779382470994 0.9914867541802048\n",
      "1 fold: ls 0.0493062744559082 0.053182154076254165 auc 0.9919479918962834 0.9901233566115328\n",
      "2 fold: ls 0.05110285767427941 0.05503341457546144 auc 0.9912957782722425 0.9887743057530725\n",
      "3 fold: ls 0.047647706091884866 0.05103712099545973 auc 0.992572063884085 0.9908350036305961\n",
      "4 fold: ls 0.05058166328138552 0.05534433391967904 auc 0.9914635617894947 0.9891505786079209\n",
      "5 fold: ls 0.047218950389785776 0.05233132510692162 auc 0.9927312972591218 0.9906231584386137\n",
      "6 fold: ls 0.049157991521635275 0.05580081064151362 auc 0.9919699628773558 0.9893918534762154\n",
      "7 fold: ls 0.05066158779935533 0.05509440380073164 auc 0.9914140463638949 0.9896992403168269\n",
      "8 fold: ls 0.05000149098111001 0.05233283821260413 auc 0.991679607856608 0.9903039897041407\n",
      "9 fold: ls 0.049581344013697216 0.05443573216572771 auc 0.9918416281213447 0.9894871864247312\n",
      "insult 0.075 0.48 3\n",
      "this class avg train 0.049426662640560434 avg val 0.05344369123808581\n",
      "this class auc train 0.9918993876567528 auc val 0.9899875427143854\n",
      "========================\n",
      "0 fold: ls 0.014385904726138257 0.01708206325278433 auc 0.9958255260004139 0.992969230969282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold: ls 0.015159778320858072 0.017388537586225473 auc 0.9951875804038763 0.9922939543008982\n",
      "2 fold: ls 0.014772610609073979 0.01718445722987947 auc 0.995476788706416 0.9931952199738409\n",
      "3 fold: ls 0.014045528419067846 0.018196264636919892 auc 0.9961336723141937 0.9895072049688884\n",
      "4 fold: ls 0.014641194824773072 0.018612900800294582 auc 0.9956428513056383 0.9893529585054593\n",
      "5 fold: ls 0.014671101650000726 0.016259740318928895 auc 0.9956214777703422 0.993722396336672\n",
      "6 fold: ls 0.014050255678070675 0.01631542972450636 auc 0.9960837217180278 0.9920911915600839\n",
      "7 fold: ls 0.01510486453413881 0.018617819463198848 auc 0.9952407485308022 0.9876366157236794\n",
      "8 fold: ls 0.013839593832324831 0.01681140004173444 auc 0.996239580947145 0.992537845942626\n",
      "9 fold: ls 0.015527366560260719 0.016654931559284318 auc 0.9949141860720181 0.9923851976298865\n",
      "identity_hate 0.075 0.48 3\n",
      "this class avg train 0.0146198199154707 avg val 0.017312354461375662\n",
      "this class auc train 0.9956366133768875 auc val 0.9915691815911316\n",
      "========================\n",
      "all loss avg 0.03159822036026905 0.03503113496795599\n",
      "all auc avg 0.9946941065107976 0.9922179675336621\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.56\n",
      "0 fold: ls 0.06985858655375787 0.07814637306925676 auc 0.9906837127310523 0.9871371208126536\n",
      "1 fold: ls 0.06888766447322596 0.07043931351463824 auc 0.9910004446651107 0.990069373096249\n",
      "2 fold: ls 0.06902072078454252 0.07509342225148484 auc 0.9909294288061224 0.9887335989751229\n",
      "3 fold: ls 0.0688849640813572 0.07308750951164844 auc 0.9910026088940109 0.9891859238843862\n",
      "4 fold: ls 0.07031305933406876 0.0775035192958694 auc 0.9905004767377722 0.9880729788727427\n",
      "5 fold: ls 0.06823044954127432 0.07513300216711327 auc 0.9912209179056026 0.9882373003731751\n",
      "6 fold: ls 0.06737745417966547 0.07704432387125287 auc 0.9914556861132872 0.9881241565207395\n",
      "7 fold: ls 0.07030547937521633 0.0701734329033864 auc 0.990497886227941 0.99037997526892\n",
      "8 fold: ls 0.06991204288864428 0.07382308315046006 auc 0.9906072862579571 0.9894566737581408\n",
      "9 fold: ls 0.06700988281293323 0.07627173683453194 auc 0.9915732833758141 0.9882265117413244\n",
      "toxic 0.075 0.56 3\n",
      "this class avg train 0.0689800304024686 avg val 0.07467157165696423\n",
      "this class auc train 0.9909471731714671 auc val 0.9887623613303453\n",
      "========================\n",
      "0 fold: ls 0.017401294501536664 0.020794894016401232 auc 0.9946530929992403 0.9915938726421065\n",
      "1 fold: ls 0.017439534623301854 0.020656322993374585 auc 0.9946464271635158 0.991636203949867\n",
      "2 fold: ls 0.01799524731049345 0.020765471274118246 auc 0.9942039357544792 0.9913236643878973\n",
      "3 fold: ls 0.01784874381246894 0.019551984047090458 auc 0.994310689603742 0.9927656981896442\n",
      "4 fold: ls 0.01804798315406391 0.02107012406893094 auc 0.9941785296078043 0.991284893657425\n",
      "5 fold: ls 0.01810816169312992 0.01972882508724107 auc 0.9941045569775201 0.9925191549603047\n",
      "6 fold: ls 0.01814806509366967 0.019177309266957144 auc 0.9940930859234468 0.9929494613856703\n",
      "7 fold: ls 0.01777762778318259 0.020336820751843756 auc 0.994376216504202 0.9919983214709585\n",
      "8 fold: ls 0.017805645692933095 0.02068004855294813 auc 0.9943428911779594 0.9915848602732069\n",
      "9 fold: ls 0.01823673250313896 0.01866327495766849 auc 0.994022801027564 0.9935000794275484\n",
      "severe_toxic 0.075 0.56 3\n",
      "this class avg train 0.017880903616791906 avg val 0.020142507501657407\n",
      "this class auc train 0.9942932226739473 auc val 0.9921156210344628\n",
      "========================\n",
      "0 fold: ls 0.03512019861897117 0.036836304250817846 auc 0.9962663497695555 0.9957813661736418\n",
      "1 fold: ls 0.03535812904836048 0.036599228571941526 auc 0.9962221301311058 0.9957256126137732\n",
      "2 fold: ls 0.03433159100073266 0.03888596892205146 auc 0.9964370246586984 0.9953719916927963\n",
      "3 fold: ls 0.034383347855730835 0.033632964322120165 auc 0.9964270103487842 0.9964796188459503\n",
      "4 fold: ls 0.03455927663572802 0.042116266165022516 auc 0.9963954957786353 0.994515742025617\n",
      "5 fold: ls 0.034038070826893876 0.037402344940438446 auc 0.9965035627234606 0.9956307303886406\n",
      "6 fold: ls 0.03586094254370102 0.03751119168887229 auc 0.9961195574009569 0.9954960359101744\n",
      "7 fold: ls 0.03525840964042672 0.03798553726674411 auc 0.9962343994997429 0.9955992103144646\n",
      "8 fold: ls 0.03521643372279692 0.03861882093701766 auc 0.9962557595577051 0.995275277924828\n",
      "9 fold: ls 0.03515705682365998 0.03870211468872423 auc 0.9962623092889533 0.9953364013156738\n",
      "obscene 0.075 0.56 3\n",
      "this class avg train 0.03492834567170017 avg val 0.03782907417537503\n",
      "this class auc train 0.9963123599157597 auc val 0.995521198720556\n",
      "========================\n",
      "0 fold: ls 0.004911674103526912 0.006738305036425995 auc 0.9986076689304255 0.9967499476220407\n",
      "1 fold: ls 0.00442535445092741 0.006283316807031964 auc 0.9990348341015539 0.9967041169076053\n",
      "2 fold: ls 0.004821795561345657 0.00719816448885445 auc 0.9987533761072389 0.9936950031426774\n",
      "3 fold: ls 0.004196191977493129 0.006363478240311931 auc 0.9991679595225659 0.9938975841766715\n",
      "4 fold: ls 0.0038260160848951254 0.006673494875771091 auc 0.9993566417710696 0.9968322438032979\n",
      "5 fold: ls 0.0051719393111713755 0.006891453521112672 auc 0.9982881434719221 0.9963490267563433\n",
      "6 fold: ls 0.004444878638474252 0.006259313806421016 auc 0.998954906290686 0.9955986653257066\n",
      "7 fold: ls 0.005005006792584117 0.0072851952416776574 auc 0.9984940746270209 0.9961185492488529\n",
      "8 fold: ls 0.005156912158861513 0.007200034806934409 auc 0.9983612101081172 0.9941475653417106\n",
      "9 fold: ls 0.004256254707257867 0.007163762505960238 auc 0.9991130698993186 0.9944605154582646\n",
      "threat 0.075 0.56 3\n",
      "this class avg train 0.004621602378653736 avg val 0.006805651933050143\n",
      "this class auc train 0.9988131884829918 auc val 0.995455321778317\n",
      "========================\n",
      "0 fold: ls 0.049156792648059194 0.049547187546405344 auc 0.9920117440268014 0.9916057105762444\n",
      "1 fold: ls 0.051255415977296784 0.053272897404022454 auc 0.9912066634352348 0.9898288935214774\n",
      "2 fold: ls 0.05121114420777992 0.055100496306509164 auc 0.9912639034985475 0.9887535594899095\n",
      "3 fold: ls 0.048947422978618156 0.051317649830815895 auc 0.9920800610621844 0.9907111952859136\n",
      "4 fold: ls 0.05066828858649508 0.055364832742236025 auc 0.9914311081160815 0.9891104219030731\n",
      "5 fold: ls 0.045206912099404105 0.05228216766100923 auc 0.9934451251541181 0.9906322773570062\n",
      "6 fold: ls 0.04791302548047268 0.05570526098657966 auc 0.9924471877413674 0.9894171187363489\n",
      "7 fold: ls 0.04857328637333368 0.05511858211288227 auc 0.9922079280467807 0.9896417348864798\n",
      "8 fold: ls 0.050060659224210005 0.052413960358546105 auc 0.9916713781968254 0.9904319843109438\n",
      "9 fold: ls 0.04946853932772587 0.054330776041244803 auc 0.9918837242878015 0.9895293207750073\n",
      "insult 0.075 0.56 3\n",
      "this class avg train 0.04924614869033954 avg val 0.0534453810990251\n",
      "this class auc train 0.9919648823565742 auc val 0.9899662216842404\n",
      "========================\n",
      "0 fold: ls 0.014134166904697074 0.01693263594039022 auc 0.9960194301160876 0.993086709380382\n",
      "1 fold: ls 0.014255106478021065 0.017312646891697314 auc 0.9959520508118264 0.9922612217665076\n",
      "2 fold: ls 0.013296194890390054 0.01714213577022782 auc 0.9966243154749546 0.9930400767286477\n",
      "3 fold: ls 0.01529803487519017 0.018188950583602276 auc 0.9951362351898361 0.988256194407938\n",
      "4 fold: ls 0.014466385181846184 0.018494796495265062 auc 0.9957745253664754 0.9895749119920796\n",
      "5 fold: ls 0.015110155263460536 0.01631069857138313 auc 0.9952312355284902 0.9937056873707314\n",
      "6 fold: ls 0.014080104889561986 0.016151772289440997 auc 0.9960379402213828 0.9924271984970012\n",
      "7 fold: ls 0.01523267695070668 0.01855682043699465 auc 0.9951550415040589 0.9884188254209119\n",
      "8 fold: ls 0.013449853405638559 0.01673053400345092 auc 0.9965409424801921 0.9927731411229135\n",
      "9 fold: ls 0.014852245007156335 0.016454437045759423 auc 0.9954783275092983 0.9924515860972614\n",
      "identity_hate 0.075 0.56 3\n",
      "this class avg train 0.014417492384666867 avg val 0.01722754280282118\n",
      "this class auc train 0.9957950044202603 auc val 0.9915995552784374\n",
      "========================\n",
      "all loss avg 0.03167908719077014 0.035020288194815515\n",
      "all auc avg 0.9946876385035001 0.992236713304393\n",
      "=======================================================\n",
      "FIND BETTER PARAMS 0.075 3 0.56 identity_hate\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.075 max depth 3 feature fraction 0.62\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: ls 0.06977040812238773 0.07813686865585732 auc 0.9907088565051761 0.9870724770825064\n",
      "1 fold: ls 0.06719504734725641 0.07031744553946755 auc 0.9915465666781276 0.9901108683007442\n",
      "2 fold: ls 0.0714291865971861 0.07520362773084804 auc 0.9901083271876058 0.9886753879076814\n",
      "3 fold: ls 0.06363302765107588 0.0729142339628574 auc 0.9926478094873569 0.9892821420223205\n",
      "4 fold: ls 0.06882686256897019 0.07721411668362818 auc 0.9909576827561396 0.9881589700137967\n",
      "5 fold: ls 0.06519716778950178 0.07483233914177997 auc 0.9921943836985853 0.9881977725529333\n",
      "6 fold: ls 0.06884743802393967 0.0769723584790323 auc 0.9910137768590395 0.9881713904527258\n",
      "7 fold: ls 0.07181699704085817 0.07030531122273023 auc 0.9899668266816009 0.99031410611317\n",
      "8 fold: ls 0.0671012284265956 0.07369377328258528 auc 0.9915394960319831 0.9895506495047822\n",
      "9 fold: ls 0.06869002389920277 0.0763340558031077 auc 0.9910364889158618 0.9882269197402244\n",
      "toxic 0.075 0.62 3\n",
      "this class avg train 0.06825073874669743 avg val 0.0745924130501894\n",
      "this class auc train 0.9911720214801477 auc val 0.9887760683690884\n",
      "========================\n",
      "0 fold: ls 0.01769752688181794 0.020915461472195186 auc 0.9944199235360023 0.9914356247626283\n",
      "1 fold: ls 0.017039052999567046 0.02065035135215348 auc 0.9949976578899993 0.9916852607925054\n",
      "2 fold: ls 0.017943367733111847 0.020825982288057186 auc 0.9942504863241801 0.9912682776300796\n",
      "3 fold: ls 0.01795234206315762 0.019526142145900937 auc 0.9942131184333246 0.9927929959488543\n",
      "4 fold: ls 0.018003669489845095 0.021036171693193318 auc 0.9941963656197171 0.9913782599063172\n",
      "5 fold: ls 0.017156110857169327 0.01965183881823083 auc 0.994881171356036 0.9925454300799162\n",
      "6 fold: ls 0.01780705675019418 0.019069455377800675 auc 0.9943653382635225 0.9929697661724641\n",
      "7 fold: ls 0.01787862580818082 0.020349212875795452 auc 0.9942819686220404 0.9920114598624131\n",
      "8 fold: ls 0.01753255197497127 0.02072373048557913 auc 0.9945714297413275 0.9915313113747017\n",
      "9 fold: ls 0.018035857084828726 0.01871423878643549 auc 0.9941776628978094 0.9934156752157783\n",
      "severe_toxic 0.075 0.62 3\n",
      "this class avg train 0.017704616164284384 avg val 0.020146258529534167\n",
      "this class auc train 0.9944355122683961 auc val 0.9921034061745659\n",
      "========================\n",
      "0 fold: ls 0.03551073105732471 0.03691400748415436 auc 0.9961870449796061 0.9957712647561937\n",
      "1 fold: ls 0.035263031041669425 0.036672289489931696 auc 0.9962435797861553 0.995697266000469\n",
      "2 fold: ls 0.035122222215786175 0.03902171066899426 auc 0.9962625724212422 0.9953416854351415\n",
      "3 fold: ls 0.03576167473175479 0.033613670214590774 auc 0.9961226022075533 0.9964883504938277\n",
      "4 fold: ls 0.03459814783285086 0.042147190889446406 auc 0.9963876275304043 0.9945002364984448\n",
      "5 fold: ls 0.03406836052620391 0.037425109984245086 auc 0.9964913564235325 0.99565876563474\n",
      "6 fold: ls 0.035924758168543146 0.03753200084179352 auc 0.9961075522187716 0.9955047284026801\n",
      "7 fold: ls 0.03496033588144503 0.037911071964926206 auc 0.9962980252248835 0.9956184356019434\n",
      "8 fold: ls 0.03434675097644332 0.03850127092600837 auc 0.9964431234575575 0.9953152947146514\n",
      "9 fold: ls 0.03462881502771717 0.0388401513039361 auc 0.9963785294241766 0.9953121746253566\n",
      "obscene 0.075 0.62 3\n",
      "this class avg train 0.035018482745973856 avg val 0.03785784737680268\n",
      "this class auc train 0.9962922013673884 auc val 0.9955208202163448\n",
      "========================\n",
      "0 fold: ls 0.005143544926671865 0.006809150186197405 auc 0.9984190016061734 0.9967237586423633\n",
      "1 fold: ls 0.004624218444396237 0.006291466369883559 auc 0.9988823052756197 0.9968075633773308\n",
      "2 fold: ls 0.004841120662553969 0.007130061522655927 auc 0.9987400576494319 0.9940433165723863\n",
      "3 fold: ls 0.004197924304801309 0.0063996097291747645 auc 0.9991723285958936 0.9932113897793701\n",
      "4 fold: ls 0.004207426278580202 0.0066845673606603815 auc 0.9991462278492855 0.9967995055209838\n",
      "5 fold: ls 0.004913776683177708 0.006808418801680129 auc 0.998618520128272 0.9963254551930774\n",
      "6 fold: ls 0.0049208068882653265 0.006340058339314099 auc 0.9985885456865389 0.9954153309447482\n",
      "7 fold: ls 0.005032743168182167 0.007371955453648884 auc 0.9984247866462929 0.9958160475202715\n",
      "8 fold: ls 0.004778331832711188 0.007083581386907082 auc 0.9987627021671399 0.9947346811586644\n",
      "9 fold: ls 0.004475487584856116 0.007306364100239969 auc 0.9989571042429399 0.9943508491781047\n",
      "threat 0.075 0.62 3\n",
      "this class avg train 0.004713538077419609 avg val 0.00682252332503622\n",
      "this class auc train 0.9987711579847588 auc val 0.99542278978873\n",
      "========================\n",
      "0 fold: ls 0.050113705582891695 0.049761433692578946 auc 0.9916399501941394 0.9914897657345347\n",
      "1 fold: ls 0.05114695557275142 0.05332316700713937 auc 0.9912333461204399 0.989683460543619\n",
      "2 fold: ls 0.0511389676660549 0.05504750414199245 auc 0.991289606661387 0.9887373723853853\n",
      "3 fold: ls 0.04770596763140062 0.05121423666720006 auc 0.992548704652139 0.9907672436581685\n",
      "4 fold: ls 0.05071350725072441 0.05548953920042936 auc 0.9914170740811834 0.989153088401974\n",
      "5 fold: ls 0.04744344769346994 0.05230432865595969 auc 0.9926464059425574 0.9906374642647158\n",
      "6 fold: ls 0.04946028357942747 0.05580026517107614 auc 0.9918781813740278 0.9893869175479112\n",
      "7 fold: ls 0.04998817153035762 0.05516142309646006 auc 0.9916794497925927 0.989642656313623\n",
      "8 fold: ls 0.0496530721407652 0.05229388956303774 auc 0.9918092575407769 0.9905310796118915\n",
      "9 fold: ls 0.04886271249986139 0.05445274014863209 auc 0.9921153908636416 0.9894083625209342\n",
      "insult 0.075 0.62 3\n",
      "this class avg train 0.04962267911477046 avg val 0.0534848527344506\n",
      "this class auc train 0.9918257367222886 auc val 0.9899437410982757\n",
      "========================\n",
      "0 fold: ls 0.015133814005434813 0.01715671661652796 auc 0.9952280324405047 0.9928096038152684\n",
      "1 fold: ls 0.014733154893428788 0.017334222733093683 auc 0.9955934068399016 0.9924414749010961\n",
      "2 fold: ls 0.015015779520824198 0.017274385074160247 auc 0.9952921187816963 0.9930261766113038\n",
      "3 fold: ls 0.01522629565198611 0.018179513666089747 auc 0.9951692676387888 0.9882257038279579\n",
      "4 fold: ls 0.014997675673644085 0.01864657119238814 auc 0.9953344576722695 0.9906902843112066\n",
      "5 fold: ls 0.015238398695884907 0.016341466987885495 auc 0.9951275655887439 0.9936140138548939\n",
      "6 fold: ls 0.013230237989832297 0.016235597705378295 auc 0.9966805471677419 0.992287647228846\n",
      "7 fold: ls 0.014779398163867195 0.018500927422821976 auc 0.995506632735936 0.987339222487174\n",
      "8 fold: ls 0.013253690445128417 0.01658697846596904 auc 0.9966852230556815 0.9929592094804538\n",
      "9 fold: ls 0.015480370767204757 0.01659378106422569 auc 0.9949419276375798 0.9922858407399378\n",
      "identity_hate 0.075 0.62 3\n",
      "this class avg train 0.014708881580723556 avg val 0.017285016092854027\n",
      "this class auc train 0.9955559179558844 auc val 0.9915679177258138\n",
      "========================\n",
      "all loss avg 0.031669822738311544 0.03503148518481119\n",
      "all auc avg 0.9946754246298107 0.992222457228803\n",
      "=======================================================\n",
      "TEST PARAM DONE ------------------------------------------\n",
      "learning rate 0.05 max depth 3 feature fraction 0.4\n",
      "0 fold: ls 0.07054908247034279 0.07799381380216308 auc 0.9904564854821903 0.9871499408376233\n",
      "1 fold: ls 0.07026490907419657 0.0708493844601407 auc 0.9905569248947377 0.9899895084177281\n",
      "2 fold: ls 0.07010288855777674 0.07511858865400087 auc 0.9905634430396628 0.9887420248572584\n",
      "3 fold: ls 0.06708963346279326 0.07305350836513544 auc 0.9915805911764245 0.9891753688814958\n",
      "4 fold: ls 0.06853288965246013 0.07729230316955127 auc 0.9910896922471606 0.9880766959384076\n"
     ]
    }
   ],
   "source": [
    "# find best params for each column, early stopping = 30\n",
    "\n",
    "best_pred = np.zeros((153164,6))\n",
    "val_auc_res = [0,0,0,0,0,0]\n",
    "for lr in [0.095,0.075,0.05]:\n",
    "    for max_d in [3]:\n",
    "        for s_rate in [0.4,0.48,0.56,0.62]:\n",
    "            print('learning rate',lr,'max depth',max_d,'feature fraction',s_rate)\n",
    "            lgb_res,tmp_auc_res = simple_ens('lgb',k=10,rnd=666,lr=lr,\n",
    "                                 feature_fraction=s_rate,bagging_fraction=0.9,\n",
    "                                 bag_frec=3,met='binary_logloss',max_d=max_d)\n",
    "            # check for each cls\n",
    "            for i in range(6):\n",
    "                # find better params for this class\n",
    "                if tmp_auc_res[i] > val_auc_res[i]:\n",
    "                    val_auc_res[i] = tmp_auc_res[i]\n",
    "                    best_pred[:,i] = lgb_res[:,i]\n",
    "                    print('FIND BETTER PARAMS',lr,max_d,s_rate,list_classes[i])\n",
    "            print('TEST PARAM DONE ------------------------------------------')\n",
    "\n",
    "print(val_auc_res)\n",
    "print(np.mean(val_auc_res))\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = best_pred\n",
    "sample_submission.to_csv(\"../results/lgb_grid_search_fold10_stratified.gz\", index=False, compression='gzip')\n",
    "print(sample_submission.head())\n",
    "print('save done')\n",
    "                \n",
    "            \n",
    "# best auc params\n",
    "# toxic 0.05,4,0.5\n",
    "# severe toxic 0.075 3 0.5\n",
    "# obs 0.075 3 0.6\n",
    "# threat 0.095 3 0.5\n",
    "# insult 0.075 0.5 4\n",
    "# hate 0.05 0.5 3\n",
    "\n",
    "# TEST PARAM DONE ------------------------------------------\n",
    "# [0.9884873039634368, 0.9920148292218363, 0.9954573112511824, \n",
    "#  0.994769628570376, 0.9898761527824232, 0.9912880359235366]\n",
    "# 0.9919822102854652 PUB 9870\n",
    "\n",
    "# updated pool gru v2 10 fold\n",
    "# TEST PARAM DONE ------------------------------------------\n",
    "# [0.988653301673445, 0.9921696955504364, 0.995510773279133, \n",
    "#  0.9948578115535328, 0.9899677533084541, 0.9914911558115035]\n",
    "# 0.9921084151960842"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
