{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keynote 30116\n",
      "catanduanes 108825\n",
      "cccc 26059\n",
      "mathewfenton 88408\n",
      "domed 89978\n",
      "gashapon 113618\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3819\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_27 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, 150, 300)     12000000    input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_65 (Dropout)            (None, 150, 300)     0           embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, 148, 256)     230656      dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, 146, 256)     384256      dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, 144, 256)     537856      dropout_65[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_76 (Global (None, 256)          0           conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_77 (Global (None, 256)          0           conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_78 (Global (None, 256)          0           conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 768)          0           global_max_pooling1d_76[0][0]    \n",
      "                                                                 global_max_pooling1d_77[0][0]    \n",
      "                                                                 global_max_pooling1d_78[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_66 (Dropout)            (None, 768)          0           concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_51 (Dense)                (None, 256)          196864      dropout_66[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_67 (Dropout)            (None, 256)          0           dense_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_52 (Dense)                (None, 6)            1542        dropout_67[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 13,351,174\n",
      "Trainable params: 1,351,174\n",
      "Non-trainable params: 12,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.layers import concatenate\n",
    "from keras import regularizers\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model(comp=True):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x1 = Conv1D(256,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x1 = GlobalMaxPool1D()(x1)\n",
    "    \n",
    "    x2= Conv1D(256,\n",
    "             5,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x2 = GlobalMaxPool1D()(x2)\n",
    "    \n",
    "    x3 = Conv1D(256,\n",
    "             7,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x3 = GlobalMaxPool1D()(x3)\n",
    "    \n",
    "    x = concatenate([x1,x2,x3])\n",
    "    \n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n",
    "\n",
    "\n",
    "tmp_m = get_cnn_model()\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0742 - acc: 0.9751Epoch 00001: val_loss improved from inf to 0.05848, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 79s 1ms/step - loss: 0.0741 - acc: 0.9751 - val_loss: 0.0585 - val_acc: 0.9799\n",
      "Epoch 2/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9798Epoch 00002: val_loss improved from 0.05848 to 0.04735, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 77s 1ms/step - loss: 0.0553 - acc: 0.9798 - val_loss: 0.0474 - val_acc: 0.9820\n",
      "Epoch 3/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9807Epoch 00003: val_loss did not improve\n",
      "63900/63900 [==============================] - 77s 1ms/step - loss: 0.0510 - acc: 0.9807 - val_loss: 0.0476 - val_acc: 0.9820\n",
      "Epoch 4/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9814Epoch 00004: val_loss improved from 0.04735 to 0.04717, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 77s 1ms/step - loss: 0.0486 - acc: 0.9814 - val_loss: 0.0472 - val_acc: 0.9816\n",
      "Epoch 5/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9818Epoch 00005: val_loss improved from 0.04717 to 0.04573, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 78s 1ms/step - loss: 0.0463 - acc: 0.9818 - val_loss: 0.0457 - val_acc: 0.9824\n",
      "Epoch 6/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9826Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 78s 1ms/step - loss: 0.0440 - acc: 0.9826 - val_loss: 0.0474 - val_acc: 0.9823\n",
      "Epoch 7/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9831Epoch 00007: val_loss did not improve\n",
      "63900/63900 [==============================] - 78s 1ms/step - loss: 0.0424 - acc: 0.9831 - val_loss: 0.0462 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9835Epoch 00008: val_loss improved from 0.04573 to 0.04544, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 78s 1ms/step - loss: 0.0408 - acc: 0.9835 - val_loss: 0.0454 - val_acc: 0.9823\n",
      "Epoch 9/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9840Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 78s 1ms/step - loss: 0.0395 - acc: 0.9840 - val_loss: 0.0455 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9845Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 78s 1ms/step - loss: 0.0380 - acc: 0.9845 - val_loss: 0.0465 - val_acc: 0.9824\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0714 - acc: 0.9759Epoch 00001: val_loss improved from inf to 0.05085, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 81s 1ms/step - loss: 0.0714 - acc: 0.9759 - val_loss: 0.0508 - val_acc: 0.9813\n",
      "Epoch 2/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9798Epoch 00002: val_loss improved from 0.05085 to 0.04862, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0544 - acc: 0.9798 - val_loss: 0.0486 - val_acc: 0.9818\n",
      "Epoch 3/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9808Epoch 00003: val_loss improved from 0.04862 to 0.04647, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0508 - acc: 0.9808 - val_loss: 0.0465 - val_acc: 0.9821\n",
      "Epoch 4/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9814Epoch 00004: val_loss improved from 0.04647 to 0.04586, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0480 - acc: 0.9814 - val_loss: 0.0459 - val_acc: 0.9824\n",
      "Epoch 5/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9820Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0456 - acc: 0.9820 - val_loss: 0.0459 - val_acc: 0.9826\n",
      "Epoch 6/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9826Epoch 00006: val_loss improved from 0.04586 to 0.04570, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0437 - acc: 0.9826 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "Epoch 7/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9831Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0418 - acc: 0.9831 - val_loss: 0.0470 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9835Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0406 - acc: 0.9835 - val_loss: 0.0475 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9841Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0390 - acc: 0.9841 - val_loss: 0.0471 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9845Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0380 - acc: 0.9845 - val_loss: 0.0483 - val_acc: 0.9828\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9762Epoch 00001: val_loss improved from inf to 0.05206, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 80s 1ms/step - loss: 0.0709 - acc: 0.9762 - val_loss: 0.0521 - val_acc: 0.9808\n",
      "Epoch 2/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9799Epoch 00002: val_loss improved from 0.05206 to 0.05043, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0549 - acc: 0.9800 - val_loss: 0.0504 - val_acc: 0.9813\n",
      "Epoch 3/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9809Epoch 00003: val_loss improved from 0.05043 to 0.04849, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0507 - acc: 0.9810 - val_loss: 0.0485 - val_acc: 0.9813\n",
      "Epoch 4/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9816Epoch 00004: val_loss improved from 0.04849 to 0.04787, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0478 - acc: 0.9816 - val_loss: 0.0479 - val_acc: 0.9817\n",
      "Epoch 5/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9824Epoch 00005: val_loss improved from 0.04787 to 0.04709, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0458 - acc: 0.9824 - val_loss: 0.0471 - val_acc: 0.9816\n",
      "Epoch 6/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9831Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0433 - acc: 0.9831 - val_loss: 0.0482 - val_acc: 0.9818\n",
      "Epoch 7/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9833Epoch 00007: val_loss improved from 0.04709 to 0.04700, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0417 - acc: 0.9833 - val_loss: 0.0470 - val_acc: 0.9821\n",
      "Epoch 8/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9837Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0400 - acc: 0.9837 - val_loss: 0.0478 - val_acc: 0.9820\n",
      "Epoch 9/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9843Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0389 - acc: 0.9843 - val_loss: 0.0483 - val_acc: 0.9823\n",
      "Epoch 10/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 78s 1ms/step - loss: 0.0381 - acc: 0.9847 - val_loss: 0.0490 - val_acc: 0.9821\n",
      "-------------------------------\n",
      "0 0.100425712722 0.962066123462\n",
      "1 0.0236671387946 0.990005320758\n",
      "2 0.0522276963704 0.980469687327\n",
      "3 0.0105957934793 0.997172695121\n",
      "4 0.0667169575309 0.971591324034\n",
      "5 0.0231201230255 0.992738729904\n",
      "final 0.0461255703204 0.982340646768\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_glove_2_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_glove_2_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.536,  0.   ,  0.046,  0.   ,  0.165,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.018,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.043,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.018,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
