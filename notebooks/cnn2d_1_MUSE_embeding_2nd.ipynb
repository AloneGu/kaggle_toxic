{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D,Conv2D,Reshape\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaszag 135029\n",
      "ocbvious 142057\n",
      "slabs 44626\n",
      "205m 142987\n",
      "981e 88058\n",
      "diaoyutai 117101\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4273\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     12000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, 150, 300, 1)  0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 150, 300, 1)  0           reshape_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 148, 1, 128)  115328      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 146, 1, 128)  192128      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 144, 1, 128)  268928      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 128)    0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 1, 384)    0           max_pooling2d_1[0][0]            \n",
      "                                                                 max_pooling2d_2[0][0]            \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          98560       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 256)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1542        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 12,676,486\n",
      "Trainable params: 676,486\n",
      "Non-trainable params: 12,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.layers import MaxPool2D,concatenate,Flatten\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model(comp=True):\n",
    "    # https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Reshape((maxlen,EMBEDDING_DIM,1))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "   \n",
    "    x1 = Conv2D(128,kernel_size=(3,EMBEDDING_DIM),activation='relu')(x)\n",
    "    x1 = MaxPool2D(pool_size=(maxlen - 3 + 1, 1), strides=(1,1), padding='valid')(x1)\n",
    "    \n",
    "    x2 = Conv2D(128,kernel_size=(5,EMBEDDING_DIM),activation='relu')(x)\n",
    "    x2 = MaxPool2D(pool_size=(maxlen - 5 + 1, 1), strides=(1,1), padding='valid')(x2)\n",
    "    \n",
    "    x3 = Conv2D(128,kernel_size=(7,EMBEDDING_DIM),activation='relu')(x)\n",
    "    x3 = MaxPool2D(pool_size=(maxlen - 7 + 1, 1), strides=(1,1), padding='valid')(x3)\n",
    "    \n",
    "    x = concatenate([x1,x2,x3])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n",
    "\n",
    "tmp_m = get_cnn_model()\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(x_data,y_data,batch_size=64):\n",
    "    data_cnt = len(y_data)\n",
    "    curr_idx = 0\n",
    "    while True:\n",
    "        if curr_idx+batch_size>=data_cnt:\n",
    "            start_idx,end_idx = data_cnt-batch_size,data_cnt\n",
    "            curr_idx = 0\n",
    "        else:\n",
    "            start_idx,end_idx = curr_idx,curr_idx+batch_size\n",
    "            curr_idx += batch_size\n",
    "            \n",
    "        curr_x = x_data[start_idx:end_idx]\n",
    "        curr_y = y_data[start_idx:end_idx]\n",
    "        yield curr_x,curr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9710Epoch 00001: val_loss improved from inf to 0.05712, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 27s 67ms/step - loss: 0.0999 - acc: 0.9710 - val_loss: 0.0571 - val_acc: 0.9792\n",
      "Epoch 2/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9792Epoch 00002: val_loss improved from 0.05712 to 0.05365, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.0589 - acc: 0.9792 - val_loss: 0.0536 - val_acc: 0.9803\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0575 - acc: 0.9793Epoch 00003: val_loss improved from 0.05365 to 0.05285, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0575 - acc: 0.9793 - val_loss: 0.0528 - val_acc: 0.9807\n",
      "Epoch 4/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0570 - acc: 0.9797Epoch 00004: val_loss improved from 0.05285 to 0.05247, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0570 - acc: 0.9797 - val_loss: 0.0525 - val_acc: 0.9809\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0564 - acc: 0.9800Epoch 00005: val_loss improved from 0.05247 to 0.05148, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 66ms/step - loss: 0.0564 - acc: 0.9801 - val_loss: 0.0515 - val_acc: 0.9810\n",
      "Epoch 6/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9809Epoch 00006: val_loss improved from 0.05148 to 0.05105, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0522 - acc: 0.9809 - val_loss: 0.0510 - val_acc: 0.9811\n",
      "Epoch 7/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9808Epoch 00007: val_loss improved from 0.05105 to 0.04975, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0525 - acc: 0.9808 - val_loss: 0.0498 - val_acc: 0.9814\n",
      "Epoch 8/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9811Epoch 00008: val_loss improved from 0.04975 to 0.04901, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 66ms/step - loss: 0.0515 - acc: 0.9811 - val_loss: 0.0490 - val_acc: 0.9817\n",
      "Epoch 9/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9814Epoch 00009: val_loss improved from 0.04901 to 0.04877, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0506 - acc: 0.9814 - val_loss: 0.0488 - val_acc: 0.9817\n",
      "Epoch 10/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9821Epoch 00010: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0490 - acc: 0.9820 - val_loss: 0.0494 - val_acc: 0.9813\n",
      "Epoch 11/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9819Epoch 00011: val_loss improved from 0.04877 to 0.04815, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 66ms/step - loss: 0.0476 - acc: 0.9819 - val_loss: 0.0481 - val_acc: 0.9822\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9819Epoch 00012: val_loss improved from 0.04815 to 0.04762, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0478 - acc: 0.9820 - val_loss: 0.0476 - val_acc: 0.9818\n",
      "Epoch 13/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9815Epoch 00013: val_loss improved from 0.04762 to 0.04683, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0491 - acc: 0.9815 - val_loss: 0.0468 - val_acc: 0.9824\n",
      "Epoch 14/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9823Epoch 00014: val_loss did not improve\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0470 - acc: 0.9823 - val_loss: 0.0473 - val_acc: 0.9820\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9828Epoch 00015: val_loss did not improve\n",
      "400/400 [==============================] - 25s 64ms/step - loss: 0.0450 - acc: 0.9828 - val_loss: 0.0479 - val_acc: 0.9818\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9826Epoch 00016: val_loss improved from 0.04683 to 0.04588, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0459 - acc: 0.9826 - val_loss: 0.0459 - val_acc: 0.9827\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9825Epoch 00017: val_loss improved from 0.04588 to 0.04583, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0460 - acc: 0.9825 - val_loss: 0.0458 - val_acc: 0.9827\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9829Epoch 00018: val_loss improved from 0.04583 to 0.04533, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 66ms/step - loss: 0.0454 - acc: 0.9829 - val_loss: 0.0453 - val_acc: 0.9829\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9836Epoch 00019: val_loss did not improve\n",
      "400/400 [==============================] - 25s 61ms/step - loss: 0.0427 - acc: 0.9836 - val_loss: 0.0466 - val_acc: 0.9824\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9832Epoch 00020: val_loss did not improve\n",
      "400/400 [==============================] - 25s 62ms/step - loss: 0.0436 - acc: 0.9832 - val_loss: 0.0486 - val_acc: 0.9822\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9691Epoch 00001: val_loss improved from inf to 0.06045, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.1048 - acc: 0.9691 - val_loss: 0.0605 - val_acc: 0.9791\n",
      "Epoch 2/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9775Epoch 00002: val_loss improved from 0.06045 to 0.05454, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0641 - acc: 0.9775 - val_loss: 0.0545 - val_acc: 0.9807\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9789Epoch 00003: val_loss improved from 0.05454 to 0.05153, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0599 - acc: 0.9789 - val_loss: 0.0515 - val_acc: 0.9814\n",
      "Epoch 4/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9790Epoch 00004: val_loss improved from 0.05153 to 0.05110, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0577 - acc: 0.9790 - val_loss: 0.0511 - val_acc: 0.9817\n",
      "Epoch 5/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0563 - acc: 0.9795Epoch 00005: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0561 - acc: 0.9796 - val_loss: 0.0533 - val_acc: 0.9808\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9804Epoch 00006: val_loss improved from 0.05110 to 0.05044, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0536 - acc: 0.9805 - val_loss: 0.0504 - val_acc: 0.9814\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9808Epoch 00007: val_loss improved from 0.05044 to 0.04768, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0518 - acc: 0.9808 - val_loss: 0.0477 - val_acc: 0.9823\n",
      "Epoch 8/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9813Epoch 00008: val_loss improved from 0.04768 to 0.04732, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0513 - acc: 0.9813 - val_loss: 0.0473 - val_acc: 0.9824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9811Epoch 00009: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0507 - acc: 0.9811 - val_loss: 0.0484 - val_acc: 0.9822\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9814Epoch 00010: val_loss did not improve\n",
      "400/400 [==============================] - 25s 64ms/step - loss: 0.0501 - acc: 0.9814 - val_loss: 0.0487 - val_acc: 0.9821\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9821Epoch 00011: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0478 - acc: 0.9821 - val_loss: 0.0477 - val_acc: 0.9825\n",
      "Epoch 12/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9822Epoch 00012: val_loss improved from 0.04732 to 0.04625, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0479 - acc: 0.9822 - val_loss: 0.0462 - val_acc: 0.9826\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9817Epoch 00013: val_loss did not improve\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0483 - acc: 0.9817 - val_loss: 0.0470 - val_acc: 0.9824\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9818Epoch 00014: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0474 - acc: 0.9818 - val_loss: 0.0464 - val_acc: 0.9828\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9826Epoch 00015: val_loss did not improve\n",
      "400/400 [==============================] - 25s 64ms/step - loss: 0.0467 - acc: 0.9826 - val_loss: 0.0466 - val_acc: 0.9827\n",
      "Epoch 16/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9828Epoch 00016: val_loss improved from 0.04625 to 0.04568, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0454 - acc: 0.9828 - val_loss: 0.0457 - val_acc: 0.9828\n",
      "Epoch 17/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9823Epoch 00017: val_loss improved from 0.04568 to 0.04528, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0453 - val_acc: 0.9830\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9825Epoch 00018: val_loss did not improve\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.0454 - acc: 0.9824 - val_loss: 0.0465 - val_acc: 0.9831\n",
      "Epoch 19/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9832Epoch 00019: val_loss improved from 0.04528 to 0.04518, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0432 - acc: 0.9832 - val_loss: 0.0452 - val_acc: 0.9831\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9832Epoch 00020: val_loss did not improve\n",
      "400/400 [==============================] - 27s 66ms/step - loss: 0.0442 - acc: 0.9832 - val_loss: 0.0475 - val_acc: 0.9829\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.1064 - acc: 0.9687Epoch 00001: val_loss improved from inf to 0.06086, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 65ms/step - loss: 0.1064 - acc: 0.9686 - val_loss: 0.0609 - val_acc: 0.9788\n",
      "Epoch 2/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9780Epoch 00002: val_loss improved from 0.06086 to 0.05409, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0620 - acc: 0.9779 - val_loss: 0.0541 - val_acc: 0.9805\n",
      "Epoch 3/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9795Epoch 00003: val_loss improved from 0.05409 to 0.05226, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0589 - acc: 0.9795 - val_loss: 0.0523 - val_acc: 0.9810\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9799Epoch 00004: val_loss improved from 0.05226 to 0.05164, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0554 - acc: 0.9799 - val_loss: 0.0516 - val_acc: 0.9811\n",
      "Epoch 5/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9799Epoch 00005: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0553 - acc: 0.9800 - val_loss: 0.0527 - val_acc: 0.9811\n",
      "Epoch 6/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9800Epoch 00006: val_loss did not improve\n",
      "400/400 [==============================] - 25s 64ms/step - loss: 0.0535 - acc: 0.9800 - val_loss: 0.0520 - val_acc: 0.9811\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9810Epoch 00007: val_loss improved from 0.05164 to 0.04996, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0532 - acc: 0.9810 - val_loss: 0.0500 - val_acc: 0.9816\n",
      "Epoch 8/20\n",
      "398/400 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9818Epoch 00008: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0510 - acc: 0.9818 - val_loss: 0.0503 - val_acc: 0.9816\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9812Epoch 00009: val_loss improved from 0.04996 to 0.04973, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0513 - acc: 0.9812 - val_loss: 0.0497 - val_acc: 0.9817\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9811Epoch 00010: val_loss did not improve\n",
      "400/400 [==============================] - 26s 64ms/step - loss: 0.0507 - acc: 0.9811 - val_loss: 0.0505 - val_acc: 0.9820\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9819Epoch 00011: val_loss did not improve\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0489 - acc: 0.9819 - val_loss: 0.0522 - val_acc: 0.9799\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9824Epoch 00012: val_loss improved from 0.04973 to 0.04827, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.0484 - acc: 0.9824 - val_loss: 0.0483 - val_acc: 0.9820\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9823Epoch 00013: val_loss did not improve\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.0472 - acc: 0.9823 - val_loss: 0.0487 - val_acc: 0.9820\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9819Epoch 00014: val_loss did not improve\n",
      "400/400 [==============================] - 25s 62ms/step - loss: 0.0485 - acc: 0.9819 - val_loss: 0.0494 - val_acc: 0.9822\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9826Epoch 00015: val_loss improved from 0.04827 to 0.04728, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 63ms/step - loss: 0.0468 - acc: 0.9826 - val_loss: 0.0473 - val_acc: 0.9824\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9827Epoch 00016: val_loss did not improve\n",
      "400/400 [==============================] - 25s 62ms/step - loss: 0.0456 - acc: 0.9828 - val_loss: 0.0476 - val_acc: 0.9825\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9828Epoch 00017: val_loss improved from 0.04728 to 0.04609, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 25s 62ms/step - loss: 0.0450 - acc: 0.9828 - val_loss: 0.0461 - val_acc: 0.9826\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9827Epoch 00018: val_loss did not improve\n",
      "400/400 [==============================] - 24s 61ms/step - loss: 0.0459 - acc: 0.9826 - val_loss: 0.0474 - val_acc: 0.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9838Epoch 00019: val_loss did not improve\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.0434 - acc: 0.9838 - val_loss: 0.0465 - val_acc: 0.9827\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9829Epoch 00020: val_loss did not improve\n",
      "400/400 [==============================] - 24s 60ms/step - loss: 0.0441 - acc: 0.9829 - val_loss: 0.0468 - val_acc: 0.9828\n",
      "-------------------------------\n",
      "0 0.100683158889 0.962493184852\n",
      "1 0.0229545795711 0.99072513176\n",
      "2 0.0515338859212 0.980554110709\n",
      "3 0.010184966569 0.997242606739\n",
      "4 0.064816459702 0.973973967701\n",
      "5 0.0230279028474 0.992204097236\n",
      "final 0.04553349225 0.982865516499\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        train_gen = data_gen(curr_x,curr_y)\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit_generator(train_gen, \n",
    "                            steps_per_epoch=400, \n",
    "                            epochs=epochs, \n",
    "                            validation_data=(hold_out_x,hold_out_y), \n",
    "                            callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn2d_muse_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn2d_muse_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "\n",
    "# pre cnn 4551, 4565, 4717\n",
    "# cnn2d, with data gen\n",
    "# 4685, 4647,4865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.007,  0.   ,  0.001,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.044,  0.   ,  0.005,  0.   ,  0.002,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.131,  0.001,  0.019,  0.002,  0.028,  0.002],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.873,  0.017,  0.608,  0.008,  0.345,  0.012],\n",
       "       [ 0.018,  0.   ,  0.003,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.007,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.007,  0.   ,  0.001,  0.   ,  0.001,  0.   ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
