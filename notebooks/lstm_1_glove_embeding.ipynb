{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 23861\n",
      "suheadings 103512\n",
      "hrw 13049\n",
      "funerals 21181\n",
      "youthe 126330\n",
      "fctm 104534\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3819\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "# print(word_vec_dict['is'])\n",
    "# print(word_vec_dict['are'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9762Epoch 00001: val_loss improved from inf to 0.04953, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 786s 12ms/step - loss: 0.0689 - acc: 0.9762 - val_loss: 0.0495 - val_acc: 0.9814\n",
      "Epoch 2/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9821Epoch 00002: val_loss improved from 0.04953 to 0.04468, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 767s 12ms/step - loss: 0.0483 - acc: 0.9821 - val_loss: 0.0447 - val_acc: 0.9830\n",
      "Epoch 3/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9832Epoch 00003: val_loss improved from 0.04468 to 0.04311, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 822s 13ms/step - loss: 0.0441 - acc: 0.9832 - val_loss: 0.0431 - val_acc: 0.9837\n",
      "Epoch 4/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9839Epoch 00004: val_loss improved from 0.04311 to 0.04264, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 775s 12ms/step - loss: 0.0416 - acc: 0.9839 - val_loss: 0.0426 - val_acc: 0.9839\n",
      "Epoch 5/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9844Epoch 00005: val_loss improved from 0.04264 to 0.04261, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 777s 12ms/step - loss: 0.0399 - acc: 0.9844 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "Epoch 6/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9850Epoch 00006: val_loss improved from 0.04261 to 0.04253, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 727s 11ms/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 7/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9856Epoch 00007: val_loss improved from 0.04253 to 0.04166, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 786s 12ms/step - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0417 - val_acc: 0.9840\n",
      "Epoch 8/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9861Epoch 00008: val_loss did not improve\n",
      "63900/63900 [==============================] - 781s 12ms/step - loss: 0.0341 - acc: 0.9861 - val_loss: 0.0438 - val_acc: 0.9840\n",
      "Epoch 9/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0326 - acc: 0.9867Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 770s 12ms/step - loss: 0.0326 - acc: 0.9867 - val_loss: 0.0437 - val_acc: 0.9837\n",
      "Epoch 10/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9873Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 783s 12ms/step - loss: 0.0312 - acc: 0.9873 - val_loss: 0.0441 - val_acc: 0.9834\n",
      "Epoch 11/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9879Epoch 00011: val_loss did not improve\n",
      "63900/63900 [==============================] - 783s 12ms/step - loss: 0.0299 - acc: 0.9879 - val_loss: 0.0446 - val_acc: 0.9833\n",
      "Epoch 12/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9883Epoch 00012: val_loss did not improve\n",
      "63900/63900 [==============================] - 766s 12ms/step - loss: 0.0286 - acc: 0.9883 - val_loss: 0.0463 - val_acc: 0.9826\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0682 - acc: 0.9770Epoch 00001: val_loss improved from inf to 0.04739, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 447s 7ms/step - loss: 0.0682 - acc: 0.9770 - val_loss: 0.0474 - val_acc: 0.9824\n",
      "Epoch 2/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9819Epoch 00002: val_loss did not improve\n",
      "63901/63901 [==============================] - 452s 7ms/step - loss: 0.0482 - acc: 0.9819 - val_loss: 0.0485 - val_acc: 0.9817\n",
      "Epoch 3/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9829Epoch 00003: val_loss improved from 0.04739 to 0.04455, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 455s 7ms/step - loss: 0.0447 - acc: 0.9829 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Epoch 4/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9836Epoch 00004: val_loss improved from 0.04455 to 0.04316, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0421 - acc: 0.9836 - val_loss: 0.0432 - val_acc: 0.9837\n",
      "Epoch 5/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9840Epoch 00005: val_loss improved from 0.04316 to 0.04311, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0401 - acc: 0.9840 - val_loss: 0.0431 - val_acc: 0.9831\n",
      "Epoch 6/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9847Epoch 00006: val_loss improved from 0.04311 to 0.04161, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0384 - acc: 0.9847 - val_loss: 0.0416 - val_acc: 0.9839\n",
      "Epoch 7/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 8/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9858Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 448s 7ms/step - loss: 0.0349 - acc: 0.9858 - val_loss: 0.0426 - val_acc: 0.9838\n",
      "Epoch 9/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9865Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0336 - acc: 0.9865 - val_loss: 0.0436 - val_acc: 0.9832\n",
      "Epoch 10/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9870Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0323 - acc: 0.9869 - val_loss: 0.0448 - val_acc: 0.9832\n",
      "Epoch 11/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9875Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0309 - acc: 0.9875 - val_loss: 0.0465 - val_acc: 0.9833\n",
      "Epoch 12/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9880Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0296 - acc: 0.9880 - val_loss: 0.0472 - val_acc: 0.9833\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0661 - acc: 0.9779Epoch 00001: val_loss improved from inf to 0.04799, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0661 - acc: 0.9779 - val_loss: 0.0480 - val_acc: 0.9815\n",
      "Epoch 2/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9823Epoch 00002: val_loss improved from 0.04799 to 0.04693, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 448s 7ms/step - loss: 0.0466 - acc: 0.9823 - val_loss: 0.0469 - val_acc: 0.9821\n",
      "Epoch 3/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9835Epoch 00003: val_loss improved from 0.04693 to 0.04449, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0433 - acc: 0.9835 - val_loss: 0.0445 - val_acc: 0.9825\n",
      "Epoch 4/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9841Epoch 00004: val_loss improved from 0.04449 to 0.04324, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 460s 7ms/step - loss: 0.0406 - acc: 0.9841 - val_loss: 0.0432 - val_acc: 0.9826\n",
      "Epoch 5/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9848Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 449s 7ms/step - loss: 0.0384 - acc: 0.9848 - val_loss: 0.0436 - val_acc: 0.9826\n",
      "Epoch 6/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00006: val_loss improved from 0.04324 to 0.04300, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 443s 7ms/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0430 - val_acc: 0.9833\n",
      "Epoch 7/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9859Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 436s 7ms/step - loss: 0.0350 - acc: 0.9859 - val_loss: 0.0434 - val_acc: 0.9835\n",
      "Epoch 8/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 436s 7ms/step - loss: 0.0334 - acc: 0.9866 - val_loss: 0.0437 - val_acc: 0.9830\n",
      "Epoch 9/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9871Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 435s 7ms/step - loss: 0.0317 - acc: 0.9871 - val_loss: 0.0440 - val_acc: 0.9830\n",
      "Epoch 10/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9876Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 436s 7ms/step - loss: 0.0305 - acc: 0.9876 - val_loss: 0.0443 - val_acc: 0.9834\n",
      "Epoch 11/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9882Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 436s 7ms/step - loss: 0.0291 - acc: 0.9882 - val_loss: 0.0469 - val_acc: 0.9832\n",
      "Epoch 12/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0278 - acc: 0.9887Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 436s 7ms/step - loss: 0.0278 - acc: 0.9887 - val_loss: 0.0474 - val_acc: 0.9824\n",
      "-------------------------------\n",
      "0 0.0927296449642 0.965248145559\n",
      "1 0.022551893566 0.99059999374\n",
      "2 0.0467388975243 0.982034616227\n",
      "3 0.00947151994752 0.996964037934\n",
      "4 0.0608183241389 0.97432473318\n",
      "5 0.0202304487401 0.993354268604\n",
      "final 0.0420901214802 0.983754299207\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 12\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_glove_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_glove_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "# pre: 0.04278 0.04386 0.04434 muse 0.0436"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.134,  0.   ,  0.005,  0.001,  0.02 ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.006,  0.   ,  0.   ,  0.   ,  0.002,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.459,  0.001,  0.078,  0.001,  0.198,  0.006],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.119,  0.   ,  0.002,  0.01 ,  0.007,  0.001],\n",
       "       [ 0.004,  0.   ,  0.   ,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
