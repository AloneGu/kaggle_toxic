{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mushu 53488\n",
      "rockport 117092\n",
      "c112 79946\n",
      "moslims 39522\n",
      "silicate 37202\n",
      "inflate 23207\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 1082\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9722Epoch 00001: val_loss improved from inf to 0.05261, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 294s 5ms/step - loss: 0.0907 - acc: 0.9722 - val_loss: 0.0526 - val_acc: 0.9810\n",
      "Epoch 2/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9805Epoch 00002: val_loss improved from 0.05261 to 0.05043, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 293s 5ms/step - loss: 0.0544 - acc: 0.9805 - val_loss: 0.0504 - val_acc: 0.9817\n",
      "Epoch 3/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9813Epoch 00003: val_loss improved from 0.05043 to 0.04896, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 293s 5ms/step - loss: 0.0512 - acc: 0.9813 - val_loss: 0.0490 - val_acc: 0.9822\n",
      "Epoch 4/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9820Epoch 00004: val_loss improved from 0.04896 to 0.04865, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 292s 5ms/step - loss: 0.0492 - acc: 0.9820 - val_loss: 0.0486 - val_acc: 0.9818\n",
      "Epoch 5/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9823Epoch 00005: val_loss improved from 0.04865 to 0.04717, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 291s 5ms/step - loss: 0.0478 - acc: 0.9823 - val_loss: 0.0472 - val_acc: 0.9824\n",
      "Epoch 6/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9827Epoch 00006: val_loss improved from 0.04717 to 0.04547, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 288s 5ms/step - loss: 0.0460 - acc: 0.9827 - val_loss: 0.0455 - val_acc: 0.9830\n",
      "Epoch 7/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9833Epoch 00007: val_loss improved from 0.04547 to 0.04531, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 284s 4ms/step - loss: 0.0443 - acc: 0.9833 - val_loss: 0.0453 - val_acc: 0.9830\n",
      "Epoch 8/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9838Epoch 00008: val_loss improved from 0.04531 to 0.04524, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 284s 4ms/step - loss: 0.0427 - acc: 0.9838 - val_loss: 0.0452 - val_acc: 0.9833\n",
      "Epoch 9/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9841Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 284s 4ms/step - loss: 0.0413 - acc: 0.9841 - val_loss: 0.0458 - val_acc: 0.9833\n",
      "Epoch 10/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9844Epoch 00010: val_loss improved from 0.04524 to 0.04474, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 284s 4ms/step - loss: 0.0402 - acc: 0.9844 - val_loss: 0.0447 - val_acc: 0.9834\n",
      "Epoch 11/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9850Epoch 00011: val_loss did not improve\n",
      "63900/63900 [==============================] - 284s 4ms/step - loss: 0.0388 - acc: 0.9850 - val_loss: 0.0458 - val_acc: 0.9827\n",
      "Epoch 12/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9854Epoch 00012: val_loss did not improve\n",
      "63900/63900 [==============================] - 284s 4ms/step - loss: 0.0376 - acc: 0.9854 - val_loss: 0.0458 - val_acc: 0.9835\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0873 - acc: 0.9737Epoch 00001: val_loss improved from inf to 0.05723, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 290s 5ms/step - loss: 0.0873 - acc: 0.9737 - val_loss: 0.0572 - val_acc: 0.9793\n",
      "Epoch 2/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9802Epoch 00002: val_loss improved from 0.05723 to 0.05004, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 289s 5ms/step - loss: 0.0547 - acc: 0.9802 - val_loss: 0.0500 - val_acc: 0.9814\n",
      "Epoch 3/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9813Epoch 00003: val_loss improved from 0.05004 to 0.04848, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 289s 5ms/step - loss: 0.0509 - acc: 0.9813 - val_loss: 0.0485 - val_acc: 0.9820\n",
      "Epoch 4/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0489 - acc: 0.9818Epoch 00004: val_loss improved from 0.04848 to 0.04683, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 288s 5ms/step - loss: 0.0489 - acc: 0.9818 - val_loss: 0.0468 - val_acc: 0.9825\n",
      "Epoch 5/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9826Epoch 00005: val_loss improved from 0.04683 to 0.04567, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 289s 5ms/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "Epoch 6/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9829Epoch 00006: val_loss improved from 0.04567 to 0.04556, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 289s 5ms/step - loss: 0.0443 - acc: 0.9829 - val_loss: 0.0456 - val_acc: 0.9830\n",
      "Epoch 7/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9836Epoch 00007: val_loss improved from 0.04556 to 0.04547, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 289s 5ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0455 - val_acc: 0.9830\n",
      "Epoch 8/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9840Epoch 00008: val_loss improved from 0.04547 to 0.04451, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 289s 5ms/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "Epoch 9/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9842Epoch 00009: val_loss improved from 0.04451 to 0.04427, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 288s 5ms/step - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 10/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 288s 5ms/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0449 - val_acc: 0.9833\n",
      "Epoch 11/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 288s 5ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0457 - val_acc: 0.9833\n",
      "Epoch 12/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9855Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 288s 5ms/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0459 - val_acc: 0.9830\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0865 - acc: 0.9744Epoch 00001: val_loss improved from inf to 0.05625, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 287s 4ms/step - loss: 0.0865 - acc: 0.9744 - val_loss: 0.0562 - val_acc: 0.9800\n",
      "Epoch 2/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9812Epoch 00002: val_loss improved from 0.05625 to 0.05075, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 286s 4ms/step - loss: 0.0527 - acc: 0.9812 - val_loss: 0.0507 - val_acc: 0.9816\n",
      "Epoch 3/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9822Epoch 00003: val_loss improved from 0.05075 to 0.04964, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 286s 4ms/step - loss: 0.0493 - acc: 0.9822 - val_loss: 0.0496 - val_acc: 0.9818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9826Epoch 00004: val_loss did not improve\n",
      "63901/63901 [==============================] - 286s 4ms/step - loss: 0.0474 - acc: 0.9826 - val_loss: 0.0507 - val_acc: 0.9815\n",
      "Epoch 5/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9831Epoch 00005: val_loss improved from 0.04964 to 0.04870, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 286s 4ms/step - loss: 0.0455 - acc: 0.9831 - val_loss: 0.0487 - val_acc: 0.9816\n",
      "Epoch 6/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9835Epoch 00006: val_loss improved from 0.04870 to 0.04615, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 286s 4ms/step - loss: 0.0438 - acc: 0.9835 - val_loss: 0.0462 - val_acc: 0.9823\n",
      "Epoch 7/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9842Epoch 00007: val_loss improved from 0.04615 to 0.04593, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 286s 4ms/step - loss: 0.0420 - acc: 0.9842 - val_loss: 0.0459 - val_acc: 0.9824\n",
      "Epoch 8/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9845Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 285s 4ms/step - loss: 0.0405 - acc: 0.9845 - val_loss: 0.0460 - val_acc: 0.9829\n",
      "Epoch 9/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9846Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 285s 4ms/step - loss: 0.0396 - acc: 0.9846 - val_loss: 0.0526 - val_acc: 0.9823\n",
      "Epoch 10/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9852Epoch 00010: val_loss improved from 0.04593 to 0.04590, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 285s 4ms/step - loss: 0.0380 - acc: 0.9852 - val_loss: 0.0459 - val_acc: 0.9831\n",
      "Epoch 11/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9856Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 285s 4ms/step - loss: 0.0367 - acc: 0.9856 - val_loss: 0.0466 - val_acc: 0.9825\n",
      "Epoch 12/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9861Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 285s 4ms/step - loss: 0.0353 - acc: 0.9861 - val_loss: 0.0477 - val_acc: 0.9828\n",
      "-------------------------------\n",
      "0 0.0979630918612 0.964058799595\n",
      "1 0.0228574586634 0.990537396584\n",
      "2 0.0521766125162 0.980980897435\n",
      "3 0.00946064555433 0.997068366527\n",
      "4 0.0655468108917 0.973865687369\n",
      "5 0.0218148635927 0.993072581402\n",
      "final 0.0449699138466 0.983263954819\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 12\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_muse_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_muse_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "\n",
    "# 4468, 4379, 4639\n",
    "# new 4458, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.048,  0.   ,  0.005,  0.001,  0.003,  0.001],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.006,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.06 ,  0.001,  0.012,  0.002,  0.008,  0.003],\n",
       "       [ 0.007,  0.   ,  0.001,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.026,  0.   ,  0.002,  0.   ,  0.001,  0.001],\n",
       "       [ 0.009,  0.   ,  0.001,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.011,  0.   ,  0.001,  0.   ,  0.001,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
