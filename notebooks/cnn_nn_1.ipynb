{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 100) (226998, 100)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "def get_cnn_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Conv1D(64,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/5\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9718Epoch 00001: val_loss improved from inf to 0.05359, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 11s 165us/step - loss: 0.0914 - acc: 0.9718 - val_loss: 0.0536 - val_acc: 0.9809\n",
      "Epoch 2/5\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9817Epoch 00002: val_loss improved from 0.05359 to 0.05216, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 149us/step - loss: 0.0498 - acc: 0.9817 - val_loss: 0.0522 - val_acc: 0.9813\n",
      "Epoch 3/5\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9841Epoch 00003: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 149us/step - loss: 0.0415 - acc: 0.9841 - val_loss: 0.0541 - val_acc: 0.9806\n",
      "Epoch 4/5\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9864Epoch 00004: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 149us/step - loss: 0.0353 - acc: 0.9863 - val_loss: 0.0563 - val_acc: 0.9807\n",
      "Epoch 5/5\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9884Epoch 00005: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 149us/step - loss: 0.0298 - acc: 0.9884 - val_loss: 0.0626 - val_acc: 0.9803\n",
      "temp eval\n",
      "0 0.11644498065 0.957247034522\n",
      "1 0.0227608195935 0.990548026666\n",
      "2 0.0554785950244 0.980063221808\n",
      "3 0.016691769332 0.996432036556\n",
      "4 0.0700030086583 0.971894463397\n",
      "5 0.0315602353003 0.991612156114\n",
      "final 0.0521565680931 0.981299489844\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/5\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0933 - acc: 0.9710Epoch 00001: val_loss improved from inf to 0.05342, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 160us/step - loss: 0.0932 - acc: 0.9710 - val_loss: 0.0534 - val_acc: 0.9809\n",
      "Epoch 2/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9819Epoch 00002: val_loss improved from 0.05342 to 0.05163, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 149us/step - loss: 0.0497 - acc: 0.9820 - val_loss: 0.0516 - val_acc: 0.9811\n",
      "Epoch 3/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9845Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 9s 148us/step - loss: 0.0411 - acc: 0.9845 - val_loss: 0.0532 - val_acc: 0.9815\n",
      "Epoch 4/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9866Epoch 00004: val_loss did not improve\n",
      "63901/63901 [==============================] - 9s 148us/step - loss: 0.0345 - acc: 0.9866 - val_loss: 0.0550 - val_acc: 0.9812\n",
      "Epoch 5/5\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9886Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 9s 148us/step - loss: 0.0290 - acc: 0.9886 - val_loss: 0.0598 - val_acc: 0.9811\n",
      "temp eval\n",
      "0 0.114149555153 0.959092331768\n",
      "1 0.024821540177 0.990234741784\n",
      "2 0.0579094731294 0.977715179969\n",
      "3 0.0131937058295 0.997276995305\n",
      "4 0.0697950912904 0.970672926448\n",
      "5 0.0298985993205 0.991830985915\n",
      "final 0.0516279941499 0.981137193532\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/5\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9634Epoch 00001: val_loss improved from inf to 0.05573, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 11s 169us/step - loss: 0.0983 - acc: 0.9635 - val_loss: 0.0557 - val_acc: 0.9803\n",
      "Epoch 2/5\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9821Epoch 00002: val_loss improved from 0.05573 to 0.05367, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 163us/step - loss: 0.0496 - acc: 0.9821 - val_loss: 0.0537 - val_acc: 0.9804\n",
      "Epoch 3/5\n",
      "63552/63901 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9842Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 156us/step - loss: 0.0415 - acc: 0.9842 - val_loss: 0.0561 - val_acc: 0.9799\n",
      "Epoch 4/5\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9863Epoch 00004: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0352 - acc: 0.9862 - val_loss: 0.0594 - val_acc: 0.9798\n",
      "Epoch 5/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9881Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 159us/step - loss: 0.0301 - acc: 0.9881 - val_loss: 0.0638 - val_acc: 0.9798\n",
      "temp eval\n",
      "0 0.112474777024 0.958372456964\n",
      "1 0.024617461576 0.989608763693\n",
      "2 0.0606141010598 0.977370892019\n",
      "3 0.0159338997934 0.996744913928\n",
      "4 0.0763932749633 0.969201877934\n",
      "5 0.0319860242058 0.991079812207\n",
      "final 0.0536699231037 0.980396452791\n",
      "-------------------------------\n",
      "0 0.114356459409 0.958237264087\n",
      "1 0.0240665935598 0.99013051507\n",
      "2 0.0580006967855 0.97838311546\n",
      "3 0.0152731398063 0.996817977903\n",
      "4 0.0720637702078 0.970589769538\n",
      "5 0.0311482905998 0.991507652502\n",
      "final 0.0524848250613 0.980944382427\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 5\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        print('temp eval')\n",
    "        eval_val(hold_out_y,hold_out_pred)\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22 ,  0.001,  0.017,  0.002,  0.038,  0.006],\n",
       "       [ 0.006,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.004,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.041,  0.   ,  0.006,  0.001,  0.005,  0.003],\n",
       "       [ 0.026,  0.   ,  0.002,  0.   ,  0.003,  0.001],\n",
       "       [ 0.052,  0.   ,  0.002,  0.   ,  0.007,  0.002],\n",
       "       [ 0.018,  0.   ,  0.002,  0.   ,  0.002,  0.001],\n",
       "       [ 0.03 ,  0.   ,  0.002,  0.   ,  0.003,  0.002]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "def get_nn_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/6\n",
      "63488/63900 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.9582Epoch 00001: val_loss improved from inf to 0.10097, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 149us/step - loss: 0.1425 - acc: 0.9582 - val_loss: 0.1010 - val_acc: 0.9670\n",
      "Epoch 2/6\n",
      "63488/63900 [============================>.] - ETA: 0s - loss: 0.0708 - acc: 0.9765Epoch 00002: val_loss improved from 0.10097 to 0.05698, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 9s 140us/step - loss: 0.0707 - acc: 0.9765 - val_loss: 0.0570 - val_acc: 0.9802\n",
      "Epoch 3/6\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9812Epoch 00003: val_loss improved from 0.05698 to 0.05352, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 9s 138us/step - loss: 0.0527 - acc: 0.9811 - val_loss: 0.0535 - val_acc: 0.9811\n",
      "Epoch 4/6\n",
      "63680/63900 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9828Epoch 00004: val_loss improved from 0.05352 to 0.05214, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 9s 137us/step - loss: 0.0459 - acc: 0.9828 - val_loss: 0.0521 - val_acc: 0.9813\n",
      "Epoch 5/6\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9841Epoch 00005: val_loss did not improve\n",
      "63900/63900 [==============================] - 8s 132us/step - loss: 0.0417 - acc: 0.9841 - val_loss: 0.0524 - val_acc: 0.9815\n",
      "Epoch 6/6\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9852Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 8s 132us/step - loss: 0.0380 - acc: 0.9852 - val_loss: 0.0531 - val_acc: 0.9816\n",
      "temp eval\n",
      "0 0.114027832404 0.95849895152\n",
      "1 0.0236239090643 0.990861005915\n",
      "2 0.0557343679851 0.978842602735\n",
      "3 0.0174336698526 0.996432036556\n",
      "4 0.0706199096493 0.971550186223\n",
      "5 0.0314140446826 0.991612156114\n",
      "final 0.0521422889396 0.981299489844\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/6\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.1297 - acc: 0.9655Epoch 00001: val_loss improved from inf to 0.06528, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 148us/step - loss: 0.1294 - acc: 0.9655 - val_loss: 0.0653 - val_acc: 0.9780\n",
      "Epoch 2/6\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9791Epoch 00002: val_loss improved from 0.06528 to 0.05479, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 8s 133us/step - loss: 0.0609 - acc: 0.9791 - val_loss: 0.0548 - val_acc: 0.9807\n",
      "Epoch 3/6\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9815Epoch 00003: val_loss improved from 0.05479 to 0.05119, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 133us/step - loss: 0.0509 - acc: 0.9815 - val_loss: 0.0512 - val_acc: 0.9815\n",
      "Epoch 4/6\n",
      "63488/63901 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9831Epoch 00004: val_loss improved from 0.05119 to 0.05119, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 133us/step - loss: 0.0451 - acc: 0.9831 - val_loss: 0.0512 - val_acc: 0.9815\n",
      "Epoch 5/6\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9843Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 8s 132us/step - loss: 0.0410 - acc: 0.9843 - val_loss: 0.0516 - val_acc: 0.9813\n",
      "Epoch 6/6\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9855Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 9s 134us/step - loss: 0.0374 - acc: 0.9855 - val_loss: 0.0532 - val_acc: 0.9817\n",
      "temp eval\n",
      "0 0.113554070804 0.960312989045\n",
      "1 0.0246087827503 0.990297339593\n",
      "2 0.0575341494183 0.978560250391\n",
      "3 0.0127040567617 0.997276995305\n",
      "4 0.0706777608747 0.970985915493\n",
      "5 0.0280508074953 0.991830985915\n",
      "final 0.0511882713508 0.981544079291\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/6\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.1277 - acc: 0.9653Epoch 00001: val_loss improved from inf to 0.07412, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 152us/step - loss: 0.1275 - acc: 0.9653 - val_loss: 0.0741 - val_acc: 0.9767\n",
      "Epoch 2/6\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9785Epoch 00002: val_loss improved from 0.07412 to 0.05840, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 142us/step - loss: 0.0636 - acc: 0.9785 - val_loss: 0.0584 - val_acc: 0.9799\n",
      "Epoch 3/6\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9810Epoch 00003: val_loss improved from 0.05840 to 0.05518, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 137us/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0552 - val_acc: 0.9803\n",
      "Epoch 4/6\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9831Epoch 00004: val_loss improved from 0.05518 to 0.05424, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 136us/step - loss: 0.0457 - acc: 0.9831 - val_loss: 0.0542 - val_acc: 0.9808\n",
      "Epoch 5/6\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9844Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 9s 136us/step - loss: 0.0409 - acc: 0.9844 - val_loss: 0.0548 - val_acc: 0.9808\n",
      "Epoch 6/6\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9859Epoch 00006: val_loss improved from 0.05424 to 0.05416, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 9s 137us/step - loss: 0.0366 - acc: 0.9859 - val_loss: 0.0542 - val_acc: 0.9811\n",
      "temp eval\n",
      "0 0.122074614524 0.958341158059\n",
      "1 0.0249889124779 0.990203442879\n",
      "2 0.059565386678 0.979061032864\n",
      "3 0.0139239407141 0.996744913928\n",
      "4 0.075920891853 0.970359937402\n",
      "5 0.0284836707606 0.992018779343\n",
      "final 0.0541595695013 0.981121544079\n",
      "-------------------------------\n",
      "0 0.116552146252 0.959051027115\n",
      "1 0.0244071933438 0.99045393371\n",
      "2 0.0576112818261 0.978821295552\n",
      "3 0.014687251076 0.996817977903\n",
      "4 0.0724061688182 0.970965352474\n",
      "5 0.0293161961507 0.991820638282\n",
      "final 0.0524967062444 0.981321704173\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n"
     ]
    }
   ],
   "source": [
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_nn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 6\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, \n",
    "                                     save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        print('temp eval')\n",
    "        eval_val(hold_out_y,hold_out_pred)\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/nn_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/nn_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
