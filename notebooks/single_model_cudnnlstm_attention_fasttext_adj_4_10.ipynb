{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "hatre 88860\n",
      "schwartzchild 85345\n",
      "shoulod 94919\n",
      "repulsive 18402\n",
      "niggling 44622\n",
      "a1dcrwtutq 58358\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 29069\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool])\n",
    "    x = Dense(256, activation=\"relu\")(conc)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval')\n",
    "    eval_val(y,train_pred)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9810Epoch 00001: val_loss improved from inf to 0.04217, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 77s 646us/step - loss: 0.0529 - acc: 0.9810 - val_loss: 0.0422 - val_acc: 0.9833\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9840Epoch 00002: val_loss improved from 0.04217 to 0.04032, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 76s 636us/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9846Epoch 00003: val_loss improved from 0.04032 to 0.03952, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 76s 638us/step - loss: 0.0389 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9840\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00004: val_loss improved from 0.03952 to 0.03928, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 77s 639us/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0393 - val_acc: 0.9841\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9861Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 76s 637us/step - loss: 0.0349 - acc: 0.9861 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9867Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 77s 640us/step - loss: 0.0330 - acc: 0.9867 - val_loss: 0.0400 - val_acc: 0.9840\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9873Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 77s 640us/step - loss: 0.0314 - acc: 0.9873 - val_loss: 0.0406 - val_acc: 0.9836\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9880Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 77s 642us/step - loss: 0.0295 - acc: 0.9880 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9885Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 79s 662us/step - loss: 0.0282 - acc: 0.9884 - val_loss: 0.0424 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9889Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 78s 653us/step - loss: 0.0271 - acc: 0.9889 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9805Epoch 00001: val_loss improved from inf to 0.04719, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 80s 672us/step - loss: 0.0541 - acc: 0.9805 - val_loss: 0.0472 - val_acc: 0.9829\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9837Epoch 00002: val_loss improved from 0.04719 to 0.04066, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 80s 672us/step - loss: 0.0414 - acc: 0.9837 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9846Epoch 00003: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 670us/step - loss: 0.0386 - acc: 0.9846 - val_loss: 0.0415 - val_acc: 0.9834\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9851Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 668us/step - loss: 0.0367 - acc: 0.9851 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9858Epoch 00005: val_loss improved from 0.04066 to 0.03935, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 80s 672us/step - loss: 0.0348 - acc: 0.9858 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 669us/step - loss: 0.0331 - acc: 0.9864 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 665us/step - loss: 0.0313 - acc: 0.9871 - val_loss: 0.0429 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 81s 680us/step - loss: 0.0296 - acc: 0.9878 - val_loss: 0.0442 - val_acc: 0.9831\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9883Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 666us/step - loss: 0.0284 - acc: 0.9882 - val_loss: 0.0454 - val_acc: 0.9820\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9888Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 79s 656us/step - loss: 0.0267 - acc: 0.9888 - val_loss: 0.0465 - val_acc: 0.9826\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9798Epoch 00001: val_loss improved from inf to 0.04334, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 80s 671us/step - loss: 0.0555 - acc: 0.9798 - val_loss: 0.0433 - val_acc: 0.9834\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9836Epoch 00002: val_loss improved from 0.04334 to 0.03935, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 80s 669us/step - loss: 0.0418 - acc: 0.9836 - val_loss: 0.0394 - val_acc: 0.9847\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9846Epoch 00003: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 665us/step - loss: 0.0392 - acc: 0.9846 - val_loss: 0.0402 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9854Epoch 00004: val_loss improved from 0.03935 to 0.03826, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 80s 670us/step - loss: 0.0370 - acc: 0.9854 - val_loss: 0.0383 - val_acc: 0.9848\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 669us/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0385 - val_acc: 0.9845\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 668us/step - loss: 0.0333 - acc: 0.9866 - val_loss: 0.0394 - val_acc: 0.9843\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9872Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 79s 664us/step - loss: 0.0315 - acc: 0.9872 - val_loss: 0.0407 - val_acc: 0.9836\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9880Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 671us/step - loss: 0.0297 - acc: 0.9880 - val_loss: 0.0412 - val_acc: 0.9843\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9885Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 80s 667us/step - loss: 0.0282 - acc: 0.9885 - val_loss: 0.0417 - val_acc: 0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0267 - acc: 0.9892Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 76s 635us/step - loss: 0.0267 - acc: 0.9892 - val_loss: 0.0429 - val_acc: 0.9838\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9802Epoch 00001: val_loss improved from inf to 0.04411, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 77s 641us/step - loss: 0.0547 - acc: 0.9802 - val_loss: 0.0441 - val_acc: 0.9831\n",
      "Epoch 2/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9837Epoch 00002: val_loss improved from 0.04411 to 0.04215, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 76s 638us/step - loss: 0.0417 - acc: 0.9837 - val_loss: 0.0421 - val_acc: 0.9837\n",
      "Epoch 3/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9846Epoch 00003: val_loss improved from 0.04215 to 0.04049, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 76s 639us/step - loss: 0.0387 - acc: 0.9847 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 4/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "119679/119679 [==============================] - 76s 637us/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 5/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "119679/119679 [==============================] - 77s 640us/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0430 - val_acc: 0.9833\n",
      "Epoch 6/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9867Epoch 00006: val_loss did not improve\n",
      "119679/119679 [==============================] - 77s 641us/step - loss: 0.0330 - acc: 0.9867 - val_loss: 0.0420 - val_acc: 0.9832\n",
      "Epoch 7/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9873Epoch 00007: val_loss did not improve\n",
      "119679/119679 [==============================] - 78s 648us/step - loss: 0.0312 - acc: 0.9873 - val_loss: 0.0429 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9879Epoch 00008: val_loss did not improve\n",
      "119679/119679 [==============================] - 78s 649us/step - loss: 0.0296 - acc: 0.9879 - val_loss: 0.0455 - val_acc: 0.9822\n",
      "Epoch 9/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9883Epoch 00009: val_loss did not improve\n",
      "119679/119679 [==============================] - 78s 649us/step - loss: 0.0282 - acc: 0.9883 - val_loss: 0.0452 - val_acc: 0.9822\n",
      "Epoch 10/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9892Epoch 00010: val_loss did not improve\n",
      "119679/119679 [==============================] - 78s 649us/step - loss: 0.0265 - acc: 0.9892 - val_loss: 0.0461 - val_acc: 0.9823\n",
      "-------------------------------\n",
      "all eval\n",
      "0 0.0860980906211 0.966779678012\n",
      "1 0.021548503393 0.99085673462\n",
      "2 0.0433285088743 0.982948029404\n",
      "3 0.00767947049149 0.997386743205\n",
      "4 0.0580409550316 0.975709872095\n",
      "5 0.0193780344201 0.992993714397\n",
      "final 0.0393455938053 0.984445795289\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "# 40000,150,lstm + global max_pool\n",
    "# final 0.0407274256871 0.984048897774\n",
    "\n",
    "# 100000,150 lstm + attention, glove embedding\n",
    "# final 0.0404159162853 0.984188856371, pub 9849\n",
    "# 3996, 4093\n",
    "\n",
    "# 100000,150 lstm + attention, use spacial dropout,spacial 0.2, last dropout 0.5, fasttext embedding\n",
    "# 1st epo 4016, 2nd epo 4117, not better compare to glove res\n",
    "\n",
    "# 100000,150,test arch\n",
    "#     x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "#     x = Attention(maxlen)(x)\n",
    "#     x = Dense(6, activation=\"sigmoid\")(x)\n",
    "# 1st epo 4116, not good\n",
    "\n",
    "# 100000,150,test arch\n",
    "#     x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "#     att = Attention(maxlen)(x)\n",
    "#     avg_pool = GlobalAveragePooling1D()(x)\n",
    "#     max_pool = GlobalMaxPooling1D()(x)\n",
    "#     conc = concatenate([att,avg_pool, max_pool])\n",
    "#     x = Dense(256, activation=\"relu\")(conc)\n",
    "#     x = Dense(6, activation=\"sigmoid\")(x)\n",
    "# 1st epo , old LSTM 3945\n",
    "# to save time ,change to CuDNNLSTM\n",
    "# 1st epo , 3928, 4 fold: final 0.0393455938053 0.984445795289\n",
    "# 10 fold: final 0.0391567844913 0.984588887287 PUB 9857\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.991859  3.417225e-01  0.934774  0.123595  0.883666   \n",
      "1  0000247867823ef7  0.000491  3.515752e-06  0.000112  0.000004  0.000050   \n",
      "2  00013b17ad220c46  0.000944  9.291653e-06  0.000206  0.000021  0.000140   \n",
      "3  00017563c3f7919a  0.000116  8.790947e-07  0.000033  0.000004  0.000021   \n",
      "4  00017695ad8997eb  0.009057  1.704790e-05  0.000801  0.000089  0.000331   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.383749  \n",
      "1       0.000008  \n",
      "2       0.000072  \n",
      "3       0.000002  \n",
      "4       0.000049  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_4.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_4_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9809Epoch 00001: val_loss improved from inf to 0.04134, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 86s 598us/step - loss: 0.0528 - acc: 0.9809 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 2/10\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9839Epoch 00002: val_loss improved from 0.04134 to 0.04064, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 88s 613us/step - loss: 0.0411 - acc: 0.9839 - val_loss: 0.0406 - val_acc: 0.9838\n",
      "Epoch 3/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9848Epoch 00003: val_loss improved from 0.04064 to 0.03881, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 85s 589us/step - loss: 0.0383 - acc: 0.9848 - val_loss: 0.0388 - val_acc: 0.9847\n",
      "Epoch 4/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9853Epoch 00004: val_loss improved from 0.03881 to 0.03828, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 86s 597us/step - loss: 0.0365 - acc: 0.9853 - val_loss: 0.0383 - val_acc: 0.9848\n",
      "Epoch 5/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 85s 590us/step - loss: 0.0348 - acc: 0.9860 - val_loss: 0.0386 - val_acc: 0.9846\n",
      "Epoch 6/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 85s 590us/step - loss: 0.0331 - acc: 0.9866 - val_loss: 0.0394 - val_acc: 0.9842\n",
      "Epoch 7/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9872Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 85s 592us/step - loss: 0.0314 - acc: 0.9872 - val_loss: 0.0418 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 87s 605us/step - loss: 0.0300 - acc: 0.9877 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 9/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0285 - acc: 0.9883Epoch 00009: val_loss did not improve\n",
      "143613/143613 [==============================] - 94s 657us/step - loss: 0.0285 - acc: 0.9883 - val_loss: 0.0412 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9889Epoch 00010: val_loss did not improve\n",
      "143613/143613 [==============================] - 103s 715us/step - loss: 0.0272 - acc: 0.9888 - val_loss: 0.0438 - val_acc: 0.9840\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9809Epoch 00001: val_loss improved from inf to 0.04485, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 110s 768us/step - loss: 0.0530 - acc: 0.9809 - val_loss: 0.0448 - val_acc: 0.9823\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9840Epoch 00002: val_loss improved from 0.04485 to 0.04321, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 98s 685us/step - loss: 0.0411 - acc: 0.9839 - val_loss: 0.0432 - val_acc: 0.9827\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9848Epoch 00003: val_loss improved from 0.04321 to 0.04029, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 83s 579us/step - loss: 0.0386 - acc: 0.9848 - val_loss: 0.0403 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 88s 613us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0423 - val_acc: 0.9832\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 621us/step - loss: 0.0349 - acc: 0.9859 - val_loss: 0.0405 - val_acc: 0.9835\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 619us/step - loss: 0.0333 - acc: 0.9865 - val_loss: 0.0418 - val_acc: 0.9826\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 622us/step - loss: 0.0316 - acc: 0.9871 - val_loss: 0.0429 - val_acc: 0.9830\n",
      "Epoch 8/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 619us/step - loss: 0.0301 - acc: 0.9877 - val_loss: 0.0428 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 619us/step - loss: 0.0287 - acc: 0.9882 - val_loss: 0.0435 - val_acc: 0.9827\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9887Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 617us/step - loss: 0.0275 - acc: 0.9887 - val_loss: 0.0441 - val_acc: 0.9827\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9810Epoch 00001: val_loss improved from inf to 0.04173, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 90s 630us/step - loss: 0.0526 - acc: 0.9810 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9839Epoch 00002: val_loss improved from 0.04173 to 0.03887, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 86s 597us/step - loss: 0.0410 - acc: 0.9839 - val_loss: 0.0389 - val_acc: 0.9846\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9847Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 622us/step - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0412 - val_acc: 0.9838\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9852Epoch 00004: val_loss improved from 0.03887 to 0.03786, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 89s 623us/step - loss: 0.0368 - acc: 0.9852 - val_loss: 0.0379 - val_acc: 0.9851\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 617us/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0389 - val_acc: 0.9846\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 618us/step - loss: 0.0330 - acc: 0.9866 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 617us/step - loss: 0.0316 - acc: 0.9871 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 620us/step - loss: 0.0302 - acc: 0.9877 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 616us/step - loss: 0.0286 - acc: 0.9882 - val_loss: 0.0432 - val_acc: 0.9832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9886Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 583us/step - loss: 0.0275 - acc: 0.9886 - val_loss: 0.0449 - val_acc: 0.9819\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9811Epoch 00001: val_loss improved from inf to 0.04449, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0521 - acc: 0.9811 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9838Epoch 00002: val_loss improved from 0.04449 to 0.04138, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9847Epoch 00003: val_loss improved from 0.04138 to 0.04075, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0408 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00004: val_loss improved from 0.04075 to 0.04070, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9861Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 86s 597us/step - loss: 0.0331 - acc: 0.9866 - val_loss: 0.0414 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 87s 607us/step - loss: 0.0318 - acc: 0.9871 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 88s 610us/step - loss: 0.0303 - acc: 0.9877 - val_loss: 0.0418 - val_acc: 0.9843\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 88s 611us/step - loss: 0.0288 - acc: 0.9882 - val_loss: 0.0453 - val_acc: 0.9826\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9887Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 620us/step - loss: 0.0275 - acc: 0.9887 - val_loss: 0.0447 - val_acc: 0.9833\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9806Epoch 00001: val_loss improved from inf to 0.04199, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 90s 624us/step - loss: 0.0534 - acc: 0.9806 - val_loss: 0.0420 - val_acc: 0.9837\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9837Epoch 00002: val_loss improved from 0.04199 to 0.04001, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 90s 624us/step - loss: 0.0417 - acc: 0.9837 - val_loss: 0.0400 - val_acc: 0.9842\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9846Epoch 00003: val_loss improved from 0.04001 to 0.03899, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 89s 623us/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.0390 - val_acc: 0.9847\n",
      "Epoch 4/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00004: val_loss improved from 0.03899 to 0.03885, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 95s 661us/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0389 - val_acc: 0.9851\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 99s 686us/step - loss: 0.0354 - acc: 0.9857 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 91s 635us/step - loss: 0.0336 - acc: 0.9864 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 7/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9870Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 91s 634us/step - loss: 0.0320 - acc: 0.9870 - val_loss: 0.0410 - val_acc: 0.9845\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 94s 655us/step - loss: 0.0305 - acc: 0.9875 - val_loss: 0.0404 - val_acc: 0.9846\n",
      "Epoch 9/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9880Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 97s 678us/step - loss: 0.0290 - acc: 0.9880 - val_loss: 0.0415 - val_acc: 0.9840\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9887Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 100s 697us/step - loss: 0.0277 - acc: 0.9887 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0527 - acc: 0.9807Epoch 00001: val_loss improved from inf to 0.04056, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 90s 629us/step - loss: 0.0527 - acc: 0.9806 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 2/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9838Epoch 00002: val_loss improved from 0.04056 to 0.03853, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 93s 650us/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.0385 - val_acc: 0.9848\n",
      "Epoch 3/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9846Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 99s 690us/step - loss: 0.0387 - acc: 0.9846 - val_loss: 0.0390 - val_acc: 0.9846\n",
      "Epoch 4/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9852Epoch 00004: val_loss improved from 0.03853 to 0.03808, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 95s 664us/step - loss: 0.0368 - acc: 0.9852 - val_loss: 0.0381 - val_acc: 0.9850\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 88s 615us/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 620us/step - loss: 0.0335 - acc: 0.9864 - val_loss: 0.0390 - val_acc: 0.9844\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9870Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 89s 617us/step - loss: 0.0318 - acc: 0.9870 - val_loss: 0.0390 - val_acc: 0.9848\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0304 - acc: 0.9875 - val_loss: 0.0402 - val_acc: 0.9847\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0289 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 589us/step - loss: 0.0289 - acc: 0.9882 - val_loss: 0.0425 - val_acc: 0.9831\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9887Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 595us/step - loss: 0.0275 - acc: 0.9887 - val_loss: 0.0418 - val_acc: 0.9844\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9809Epoch 00001: val_loss improved from inf to 0.04066, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 91s 631us/step - loss: 0.0525 - acc: 0.9809 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 2/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9840Epoch 00002: val_loss improved from 0.04066 to 0.03933, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 93s 647us/step - loss: 0.0410 - acc: 0.9840 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 3/10\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9847Epoch 00003: val_loss improved from 0.03933 to 0.03850, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 98s 680us/step - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0385 - val_acc: 0.9847\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0412 - val_acc: 0.9836\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 86s 596us/step - loss: 0.0350 - acc: 0.9859 - val_loss: 0.0403 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 86s 601us/step - loss: 0.0332 - acc: 0.9865 - val_loss: 0.0403 - val_acc: 0.9837\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 91s 635us/step - loss: 0.0317 - acc: 0.9871 - val_loss: 0.0398 - val_acc: 0.9847\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9878Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 107s 747us/step - loss: 0.0300 - acc: 0.9878 - val_loss: 0.0418 - val_acc: 0.9836\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 105s 728us/step - loss: 0.0287 - acc: 0.9882 - val_loss: 0.0428 - val_acc: 0.9832\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9888Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 104s 722us/step - loss: 0.0276 - acc: 0.9888 - val_loss: 0.0418 - val_acc: 0.9841\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9809Epoch 00001: val_loss improved from inf to 0.04318, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 87s 608us/step - loss: 0.0526 - acc: 0.9809 - val_loss: 0.0432 - val_acc: 0.9837\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9840Epoch 00002: val_loss improved from 0.04318 to 0.04039, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 86s 599us/step - loss: 0.0410 - acc: 0.9840 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9847Epoch 00003: val_loss improved from 0.04039 to 0.03990, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 593us/step - loss: 0.0384 - acc: 0.9847 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0403 - val_acc: 0.9837\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0411 - val_acc: 0.9836\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0332 - acc: 0.9866 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0315 - acc: 0.9871 - val_loss: 0.0418 - val_acc: 0.9840\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0300 - acc: 0.9877 - val_loss: 0.0429 - val_acc: 0.9830\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0286 - acc: 0.9882 - val_loss: 0.0436 - val_acc: 0.9825\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9888Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 592us/step - loss: 0.0273 - acc: 0.9888 - val_loss: 0.0435 - val_acc: 0.9833\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9808Epoch 00001: val_loss improved from inf to 0.04058, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 86s 601us/step - loss: 0.0529 - acc: 0.9808 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9838Epoch 00002: val_loss improved from 0.04058 to 0.03882, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9846Epoch 00003: val_loss improved from 0.03882 to 0.03836, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 85s 591us/step - loss: 0.0385 - acc: 0.9846 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9852Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 589us/step - loss: 0.0366 - acc: 0.9852 - val_loss: 0.0392 - val_acc: 0.9849\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 589us/step - loss: 0.0348 - acc: 0.9858 - val_loss: 0.0387 - val_acc: 0.9853\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 85s 589us/step - loss: 0.0332 - acc: 0.9865 - val_loss: 0.0397 - val_acc: 0.9849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 583us/step - loss: 0.0315 - acc: 0.9871 - val_loss: 0.0410 - val_acc: 0.9838\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9876Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 582us/step - loss: 0.0300 - acc: 0.9876 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9882Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 583us/step - loss: 0.0286 - acc: 0.9882 - val_loss: 0.0441 - val_acc: 0.9825\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9887Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 583us/step - loss: 0.0274 - acc: 0.9887 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9810Epoch 00001: val_loss improved from inf to 0.04356, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 86s 598us/step - loss: 0.0523 - acc: 0.9810 - val_loss: 0.0436 - val_acc: 0.9831\n",
      "Epoch 2/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9839Epoch 00002: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0409 - acc: 0.9839 - val_loss: 0.0442 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9847Epoch 00003: val_loss improved from 0.04356 to 0.04074, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 84s 587us/step - loss: 0.0385 - acc: 0.9847 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 4/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0365 - acc: 0.9853 - val_loss: 0.0436 - val_acc: 0.9828\n",
      "Epoch 5/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0347 - acc: 0.9860 - val_loss: 0.0417 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0329 - acc: 0.9866 - val_loss: 0.0433 - val_acc: 0.9829\n",
      "Epoch 7/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0315 - acc: 0.9871 - val_loss: 0.0421 - val_acc: 0.9838\n",
      "Epoch 8/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9878Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0299 - acc: 0.9878 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 9/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0284 - acc: 0.9884Epoch 00009: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0284 - acc: 0.9884 - val_loss: 0.0453 - val_acc: 0.9830\n",
      "Epoch 10/10\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9889Epoch 00010: val_loss did not improve\n",
      "143614/143614 [==============================] - 84s 585us/step - loss: 0.0272 - acc: 0.9889 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "-------------------------------\n",
      "all eval\n",
      "0 0.0864134873758 0.966804745223\n",
      "1 0.0214379711889 0.990537127674\n",
      "2 0.043058395495 0.983204968321\n",
      "3 0.00752564715107 0.997468211642\n",
      "4 0.0575143864776 0.976298951564\n",
      "5 0.0189908192596 0.9932193193\n",
      "final 0.0391567844913 0.984588887287\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.992837  4.137306e-01  0.936927  0.164999  0.895366   \n",
      "1  0000247867823ef7  0.000249  1.016843e-06  0.000046  0.000001  0.000027   \n",
      "2  00013b17ad220c46  0.000740  1.048361e-05  0.000199  0.000013  0.000102   \n",
      "3  00017563c3f7919a  0.000172  7.982660e-07  0.000044  0.000009  0.000040   \n",
      "4  00017695ad8997eb  0.004180  1.450363e-05  0.000314  0.000043  0.000222   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.557324  \n",
      "1       0.000011  \n",
      "2       0.000073  \n",
      "3       0.000008  \n",
      "4       0.000050  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape) \n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
