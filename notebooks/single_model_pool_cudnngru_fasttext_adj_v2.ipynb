{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 73)\n",
      "[-8.71819733e-03 -1.09741811e-02 -1.36728622e-02 -4.26841221e-03\n",
      " -2.65620055e-02 -1.09669200e-02  5.00000000e+01  4.60000000e+01\n",
      "  2.64000000e+02  0.00000000e+00  2.00000000e+01  0.00000000e+00\n",
      "  3.00000000e+00  1.20000000e+01  4.24000000e+00  0.00000000e+00\n",
      "  6.43939394e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  1.51515152e-02  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  4.60000000e+01\n",
      "  3.40000000e+02  2.50000000e+01  2.50000000e-01  9.20000000e-01\n",
      "  5.00000000e+01  1.00000000e+00  4.00000000e-01  3.00000000e+01\n",
      "  6.00000000e-01  6.00000000e-02  2.40000000e-01  4.97183589e-03\n",
      "  2.19470500e-04  1.91532348e-03  1.12080601e-04  6.59428638e-04\n",
      "  2.94189463e-04  7.35962419e-03  6.12739154e-04  2.56212582e-03\n",
      "  3.21850807e-04  2.84586342e-03  5.57503823e-04  3.02297447e-04\n",
      "  5.99867599e-09  2.28718188e-05  1.14941809e-08  1.50047469e-05\n",
      "  1.12412318e-07  4.78512980e-01  4.94473060e-01  4.93796720e-01\n",
      "  4.86044940e-01  4.89667930e-01  4.97940090e-01  4.09644022e-03\n",
      "  6.09518792e-03  1.30472148e-02  2.28091293e-03  1.25153108e-02\n",
      "  6.42610836e-03]\n",
      "[6.39271377e-02 4.78549963e-02 2.95811533e-02 4.17371945e-02\n",
      " 3.53564038e-02 3.71946016e-02 3.49500713e-02 9.33609959e-02\n",
      " 5.16619944e-02 0.00000000e+00 2.09863589e-02 0.00000000e+00\n",
      " 2.21893491e-03 1.20000000e-02 2.61501211e-03 0.00000000e+00\n",
      " 6.45107832e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.19159566e-02 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 3.20970043e-02\n",
      " 4.39760604e-02 8.88888889e-01 2.64705882e-01 9.19935949e-01\n",
      " 3.49500713e-02 0.00000000e+00 4.00000000e-01 2.40000000e-02\n",
      " 6.00000000e-01 6.00000000e-02 2.40000000e-01 4.83638733e-03\n",
      " 2.17394968e-04 1.79814374e-03 1.11942435e-04 5.97145528e-04\n",
      " 2.77291187e-04 7.31546044e-03 5.68872108e-04 2.40054267e-03\n",
      " 3.05619484e-04 2.77494446e-03 5.34113253e-04 3.02297447e-04\n",
      " 5.99867599e-09 2.28718188e-05 1.14941809e-08 1.50047469e-05\n",
      " 1.12412318e-07 6.12307004e-02 9.00218375e-02 5.42136083e-02\n",
      " 5.61924912e-02 4.89451054e-02 8.05734814e-02 4.06211702e-03\n",
      " 5.70668569e-03 1.26388567e-02 2.22039166e-03 1.22442901e-02\n",
      " 6.32540379e-03]\n",
      "(159571, 6)\n",
      "(159571, 250) (153164, 250)\n",
      "rollbackers 93878\n",
      "alauk 142782\n",
      "britton 33170\n",
      "eventuallynot 93037\n",
      "bonnes 80240\n",
      "rathwell 133003\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 66521\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import string\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "# prepare other feat, change to 9865 version and add ridge2\n",
    "fl = [\n",
    "    '../features/ridge_feat2.pkl',\n",
    "    '../features/other_feat.pkl',\n",
    "    '../features/lgb1_feat.pkl',\n",
    "    '../features/lr_feat2.pkl',\n",
    "    '../features/mnb_feat2.pkl',\n",
    "    '../features/wordbatch_feat.pkl',\n",
    "    '../features/tilli_lr_feat.pkl',\n",
    "\n",
    "]\n",
    "def get_feat(f):\n",
    "    with open(f,'rb') as fin:\n",
    "        a,b = pickle.load(fin)\n",
    "        return a,b\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in fl:\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "print(train_x[0])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(train_x[0])\n",
    "\n",
    "max_features = 160000\n",
    "maxlen = 250\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "    (b'&lt;3', b' heart '),\n",
    "    (b':d', b' smile '),\n",
    "    (b':dd', b' smile '),\n",
    "    (b':p', b' smile '),\n",
    "    (b'8\\)', b' smile '),\n",
    "    (b':-\\)', b' smile '),\n",
    "    (b':\\)', b' smile '),\n",
    "    (b';\\)', b' smile '),\n",
    "    (b'\\(-:', b' smile '),\n",
    "    (b'\\(:', b' smile '),\n",
    "    (b'yay!', b' good '),\n",
    "    (b'yay', b' good '),\n",
    "    (b'yaay', b' good '),\n",
    "    (b':/', b' worry '),\n",
    "    (b':&gt;', b' angry '),\n",
    "    (b\":'\\)\", b' sad '),\n",
    "    (b':-\\(', b' sad '),\n",
    "    (b':\\(', b' sad '),\n",
    "    (b':s', b' sad '),\n",
    "    (b':-s', b' sad '),\n",
    "    (b'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', b' '),\n",
    "    (b'(\\[[\\s\\S]*\\])', b' '),\n",
    "    (b'[\\s]*?(www.[\\S]*)', b' ')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def new_clean(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    \n",
    "    # replace words like hhhhhhhhhhhhhhi with hi\n",
    "    for ch in string.ascii_lowercase:\n",
    "        pattern = bytes(ch+'{3,}', encoding=\"utf-8\")\n",
    "        clean = re.sub(pattern, bytes(ch, encoding=\"utf-8\"), clean)\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return new_clean(text)\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model(comp):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    inp_2 = Input(shape=[train_x.shape[1]], name=\"other\")\n",
    "    emb = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    emb = SpatialDropout1D(0.4)(emb)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(emb)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    \n",
    "    conc = concatenate([att,avg_pool, max_pool, inp_2])\n",
    "    # conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp,inp_2], outputs=outp)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     48000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 256)     330240      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          506         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "other (InputLayer)              (None, 73)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 841)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 other[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            5052        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 48,335,798\n",
      "Trainable params: 335,798\n",
      "Non-trainable params: 48,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = get_model(False)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        curr_other_x = train_x[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        hold_out_other_x = train_x[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model(True)\n",
    "        batch_size = 64\n",
    "        epochs = 7\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit([curr_x,curr_other_x], curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=([hold_out_x,hold_out_other_x],hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict([X_test,test_x])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x,hold_out_other_x])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.04089, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0473 - acc: 0.9827 - val_loss: 0.0409 - val_acc: 0.9836\n",
      "Epoch 2/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9847Epoch 00002: val_loss did not improve\n",
      "127656/127656 [==============================] - 159s 1ms/step - loss: 0.0396 - acc: 0.9847 - val_loss: 0.0410 - val_acc: 0.9838\n",
      "Epoch 3/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852Epoch 00003: val_loss improved from 0.04089 to 0.03882, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 159s 1ms/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0388 - val_acc: 0.9842\n",
      "Epoch 4/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9856Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0402 - val_acc: 0.9839\n",
      "Epoch 5/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9863Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0350 - acc: 0.9863 - val_loss: 0.0413 - val_acc: 0.9838\n",
      "Epoch 6/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 159s 1ms/step - loss: 0.0340 - acc: 0.9865 - val_loss: 0.0391 - val_acc: 0.9842\n",
      "Epoch 7/7\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9869Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0328 - acc: 0.9869 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9825Epoch 00001: val_loss improved from inf to 0.03932, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 160s 1ms/step - loss: 0.0478 - acc: 0.9825 - val_loss: 0.0393 - val_acc: 0.9847\n",
      "Epoch 2/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9847Epoch 00002: val_loss improved from 0.03932 to 0.03835, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 160s 1ms/step - loss: 0.0396 - acc: 0.9847 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 3/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 159s 1ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0393 - val_acc: 0.9849\n",
      "Epoch 4/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 155s 1ms/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0387 - val_acc: 0.9844\n",
      "Epoch 5/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0353 - acc: 0.9858 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 6/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0341 - acc: 0.9863 - val_loss: 0.0398 - val_acc: 0.9848\n",
      "Epoch 7/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9869Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0331 - acc: 0.9869 - val_loss: 0.0402 - val_acc: 0.9846\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03901, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 153s 1ms/step - loss: 0.0476 - acc: 0.9826 - val_loss: 0.0390 - val_acc: 0.9849\n",
      "Epoch 2/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9845Epoch 00002: val_loss improved from 0.03901 to 0.03841, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 153s 1ms/step - loss: 0.0398 - acc: 0.9845 - val_loss: 0.0384 - val_acc: 0.9850\n",
      "Epoch 3/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9849Epoch 00003: val_loss improved from 0.03841 to 0.03680, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0368 - val_acc: 0.9855\n",
      "Epoch 4/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0373 - val_acc: 0.9855\n",
      "Epoch 5/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0356 - acc: 0.9857 - val_loss: 0.0381 - val_acc: 0.9849\n",
      "Epoch 6/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0345 - acc: 0.9861 - val_loss: 0.0397 - val_acc: 0.9845\n",
      "Epoch 7/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0336 - acc: 0.9865 - val_loss: 0.0383 - val_acc: 0.9850\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9824Epoch 00001: val_loss improved from inf to 0.03867, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0479 - acc: 0.9824 - val_loss: 0.0387 - val_acc: 0.9844\n",
      "Epoch 2/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9845Epoch 00002: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0399 - acc: 0.9845 - val_loss: 0.0403 - val_acc: 0.9846\n",
      "Epoch 3/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0383 - acc: 0.9850 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Epoch 4/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9854Epoch 00004: val_loss improved from 0.03867 to 0.03824, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0382 - val_acc: 0.9847\n",
      "Epoch 5/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9856Epoch 00005: val_loss improved from 0.03824 to 0.03795, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0379 - val_acc: 0.9848\n",
      "Epoch 6/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9861Epoch 00006: val_loss improved from 0.03795 to 0.03782, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0347 - acc: 0.9861 - val_loss: 0.0378 - val_acc: 0.9848\n",
      "Epoch 7/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0387 - val_acc: 0.9845\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9825Epoch 00001: val_loss improved from inf to 0.04198, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0481 - acc: 0.9825 - val_loss: 0.0420 - val_acc: 0.9843\n",
      "Epoch 2/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9845Epoch 00002: val_loss improved from 0.04198 to 0.03957, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 153s 1ms/step - loss: 0.0401 - acc: 0.9845 - val_loss: 0.0396 - val_acc: 0.9843\n",
      "Epoch 3/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9849Epoch 00003: val_loss improved from 0.03957 to 0.03837, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0382 - acc: 0.9849 - val_loss: 0.0384 - val_acc: 0.9851\n",
      "Epoch 4/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0385 - val_acc: 0.9849\n",
      "Epoch 5/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 152s 1ms/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0399 - val_acc: 0.9846\n",
      "Epoch 6/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0345 - acc: 0.9862 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 7/7\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 151s 1ms/step - loss: 0.0333 - acc: 0.9866 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "-------------------------------\n",
      "0 0.0807858389937395 0.968985592620213\n",
      "1 0.02188139795791187 0.9904305920248667\n",
      "2 0.04204937278981892 0.9832175019270418\n",
      "3 0.007957250472187682 0.9972300731335895\n",
      "4 0.05679544601393856 0.9763365523810718\n",
      "5 0.01872501955842561 0.9931441176654906\n",
      "final 0.03803238763100369 0.9848907382920457\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999078      0.566135  0.987943  0.199553  0.916229   \n",
      "1  0000247867823ef7  0.001194      0.000057  0.000261  0.000020  0.000209   \n",
      "2  00013b17ad220c46  0.000783      0.000088  0.000382  0.000042  0.000215   \n",
      "3  00017563c3f7919a  0.000368      0.000039  0.000157  0.000045  0.000194   \n",
      "4  00017695ad8997eb  0.003161      0.000119  0.000592  0.000105  0.000265   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.539241  \n",
      "1       0.000035  \n",
      "2       0.000116  \n",
      "3       0.000016  \n",
      "4       0.000053  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "\n",
    "\n",
    "# fold4 output feat\n",
    "# final 0.0401879770356 0.984252568867, pre model, not good\n",
    "\n",
    "# test add other feat, NON-NN feat to nn\n",
    "# add first 3 files, 3891 1st fold\n",
    "# add more non-nn feats, 3883 1st fold\n",
    "# final 0.03855334827205233 0.9847883805119143\n",
    "\n",
    "# len 250, max word 160000, more text clean\n",
    "# 1st fold, 3876\n",
    "# final 0.038284512738446075 0.9849492284521206\n",
    "\n",
    "# add more other feat, lr1, mnb1, ridge, 1st fold 3831, 3824, 3729, 3804, 3839\n",
    "# final 0.03805630898087786 0.9848301591976821\n",
    "\n",
    "# add no pretrained, bottom dr 0.1\n",
    "# 1st fold 3908\n",
    "\n",
    "# rm bottom dr, 1st fold 3796 , later worse\n",
    "\n",
    "# add fasttext nn other model, 3821, 3846, 3726\n",
    "\n",
    "# add 3 conv, 1st fold 3912\n",
    "# change 256 to 64 conv kernel, 1st fold 3928\n",
    "# add dr after conv, 64 to 128 kernel, 3847, 3829, 3761,\n",
    "\n",
    "# rm nn other model, conv layers, add rf, gbrt other feat\n",
    "# 1st fold 3926\n",
    "\n",
    "# add nn fasttext model, 3811, 3827, 3758, 3793,3855, final 0.0380903419194057 0.9850568085679728\n",
    "\n",
    "# change to 9865 version, add ridge v2\n",
    "# 3882, 3835,3680,3782,3837, final 0.03803238763100369 0.9848907382920457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03912, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 163s 1ms/step - loss: 0.0471 - acc: 0.9827 - val_loss: 0.0391 - val_acc: 0.9846\n",
      "Epoch 2/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00002: val_loss improved from 0.03912 to 0.03808, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 167s 1ms/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0381 - val_acc: 0.9848\n",
      "Epoch 3/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143613/143613 [==============================] - 170s 1ms/step - loss: 0.0378 - acc: 0.9851 - val_loss: 0.0381 - val_acc: 0.9847\n",
      "Epoch 4/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "143613/143613 [==============================] - 169s 1ms/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0389 - val_acc: 0.9843\n",
      "Epoch 5/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 170s 1ms/step - loss: 0.0351 - acc: 0.9859 - val_loss: 0.0409 - val_acc: 0.9842\n",
      "Epoch 6/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 170s 1ms/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 7/7\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 174s 1ms/step - loss: 0.0334 - acc: 0.9865 - val_loss: 0.0395 - val_acc: 0.9845\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9828Epoch 00001: val_loss improved from inf to 0.04044, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 173s 1ms/step - loss: 0.0468 - acc: 0.9828 - val_loss: 0.0404 - val_acc: 0.9835\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9846Epoch 00002: val_loss improved from 0.04044 to 0.03867, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 172s 1ms/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0387 - val_acc: 0.9842\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 171s 1ms/step - loss: 0.0379 - acc: 0.9852 - val_loss: 0.0396 - val_acc: 0.9842\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855Epoch 00004: val_loss improved from 0.03867 to 0.03813, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 172s 1ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0381 - val_acc: 0.9845\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 172s 1ms/step - loss: 0.0356 - acc: 0.9858 - val_loss: 0.0395 - val_acc: 0.9841\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 177s 1ms/step - loss: 0.0344 - acc: 0.9862 - val_loss: 0.0396 - val_acc: 0.9838\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0336 - acc: 0.9864 - val_loss: 0.0394 - val_acc: 0.9839\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03728, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 174s 1ms/step - loss: 0.0472 - acc: 0.9827 - val_loss: 0.0373 - val_acc: 0.9852\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9846Epoch 00002: val_loss did not improve\n",
      "143614/143614 [==============================] - 185s 1ms/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0384 - val_acc: 0.9848\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9851Epoch 00003: val_loss improved from 0.03728 to 0.03641, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0380 - acc: 0.9851 - val_loss: 0.0364 - val_acc: 0.9855\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00004: val_loss improved from 0.03641 to 0.03628, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 186s 1ms/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0363 - val_acc: 0.9856\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 186s 1ms/step - loss: 0.0355 - acc: 0.9859 - val_loss: 0.0375 - val_acc: 0.9851\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0343 - acc: 0.9863 - val_loss: 0.0376 - val_acc: 0.9847\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 185s 1ms/step - loss: 0.0334 - acc: 0.9867 - val_loss: 0.0389 - val_acc: 0.9841\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.04114, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0469 - acc: 0.9827 - val_loss: 0.0411 - val_acc: 0.9841\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9845Epoch 00002: val_loss improved from 0.04114 to 0.03974, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 186s 1ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0397 - val_acc: 0.9847\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 174s 1ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0401 - val_acc: 0.9843\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 177s 1ms/step - loss: 0.0359 - acc: 0.9857 - val_loss: 0.0418 - val_acc: 0.9837\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9859Epoch 00006: val_loss improved from 0.03974 to 0.03893, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0349 - acc: 0.9859 - val_loss: 0.0389 - val_acc: 0.9850\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9863Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0409 - val_acc: 0.9846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9829Epoch 00001: val_loss improved from inf to 0.03935, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0466 - acc: 0.9829 - val_loss: 0.0393 - val_acc: 0.9848\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9846Epoch 00002: val_loss improved from 0.03935 to 0.03777, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0378 - val_acc: 0.9853\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0378 - acc: 0.9850 - val_loss: 0.0378 - val_acc: 0.9853\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0379 - val_acc: 0.9852\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0351 - acc: 0.9859 - val_loss: 0.0385 - val_acc: 0.9851\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9862Epoch 00006: val_loss improved from 0.03777 to 0.03755, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0375 - val_acc: 0.9855\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9865Epoch 00007: val_loss improved from 0.03755 to 0.03750, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0332 - acc: 0.9865 - val_loss: 0.0375 - val_acc: 0.9854\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03840, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0469 - acc: 0.9827 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9846Epoch 00002: val_loss improved from 0.03840 to 0.03704, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0396 - acc: 0.9846 - val_loss: 0.0370 - val_acc: 0.9854\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0371 - val_acc: 0.9853\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9855Epoch 00004: val_loss improved from 0.03704 to 0.03703, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0370 - val_acc: 0.9853\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0375 - val_acc: 0.9851\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0338 - acc: 0.9863 - val_loss: 0.0383 - val_acc: 0.9847\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0330 - acc: 0.9868 - val_loss: 0.0379 - val_acc: 0.9848\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03839, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0469 - acc: 0.9827 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9845Epoch 00002: val_loss improved from 0.03839 to 0.03759, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0394 - acc: 0.9845 - val_loss: 0.0376 - val_acc: 0.9847\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0404 - val_acc: 0.9839\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9855Epoch 00004: val_loss improved from 0.03759 to 0.03705, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0370 - val_acc: 0.9846\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0352 - acc: 0.9858 - val_loss: 0.0388 - val_acc: 0.9844\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0386 - val_acc: 0.9847\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0391 - val_acc: 0.9842\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.04030, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0470 - acc: 0.9827 - val_loss: 0.0403 - val_acc: 0.9843\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9846Epoch 00002: val_loss improved from 0.04030 to 0.03953, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0398 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9842\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00003: val_loss improved from 0.03953 to 0.03929, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854Epoch 00004: val_loss improved from 0.03929 to 0.03873, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0387 - val_acc: 0.9845\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9858Epoch 00005: val_loss improved from 0.03873 to 0.03855, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0385 - val_acc: 0.9845\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0346 - acc: 0.9862 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0336 - acc: 0.9865 - val_loss: 0.0387 - val_acc: 0.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9829Epoch 00001: val_loss improved from inf to 0.03833, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0466 - acc: 0.9829 - val_loss: 0.0383 - val_acc: 0.9852\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9847Epoch 00002: val_loss improved from 0.03833 to 0.03727, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0373 - val_acc: 0.9852\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9852Epoch 00003: val_loss improved from 0.03727 to 0.03621, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 163s 1ms/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0362 - val_acc: 0.9857\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0363 - acc: 0.9854 - val_loss: 0.0369 - val_acc: 0.9852\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9861Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0349 - acc: 0.9861 - val_loss: 0.0372 - val_acc: 0.9852\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9858Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0353 - acc: 0.9858 - val_loss: 0.0368 - val_acc: 0.9857\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9858Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0353 - acc: 0.9858 - val_loss: 0.0392 - val_acc: 0.9849\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9828Epoch 00001: val_loss improved from inf to 0.04150, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0468 - acc: 0.9828 - val_loss: 0.0415 - val_acc: 0.9843\n",
      "Epoch 2/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9847Epoch 00002: val_loss improved from 0.04150 to 0.03957, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0396 - val_acc: 0.9845\n",
      "Epoch 3/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9851Epoch 00003: val_loss improved from 0.03957 to 0.03954, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0375 - acc: 0.9851 - val_loss: 0.0395 - val_acc: 0.9844\n",
      "Epoch 4/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9856Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0400 - val_acc: 0.9846\n",
      "Epoch 5/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9859Epoch 00005: val_loss improved from 0.03954 to 0.03941, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0394 - val_acc: 0.9846\n",
      "Epoch 6/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 161s 1ms/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0403 - val_acc: 0.9844\n",
      "Epoch 7/7\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 162s 1ms/step - loss: 0.0331 - acc: 0.9868 - val_loss: 0.0432 - val_acc: 0.9839\n",
      "-------------------------------\n",
      "0 0.0803797404189732 0.9690733278603255\n",
      "1 0.020945358330325254 0.9908254006053732\n",
      "2 0.04115263353761608 0.9832989703642893\n",
      "3 0.00757851832850118 0.9974055436138145\n",
      "4 0.05739991694886228 0.9762676175495547\n",
      "5 0.01883788353934886 0.993087716439704\n",
      "final 0.03771567518393781 0.984993096072177\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999219      0.533268  0.985399  0.232914  0.956773   \n",
      "1  0000247867823ef7  0.000521      0.000028  0.000122  0.000012  0.000134   \n",
      "2  00013b17ad220c46  0.000444      0.000044  0.000219  0.000031  0.000198   \n",
      "3  00017563c3f7919a  0.000183      0.000024  0.000089  0.000025  0.000092   \n",
      "4  00017695ad8997eb  0.002322      0.000091  0.000233  0.000083  0.000182   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.694634  \n",
      "1       0.000016  \n",
      "2       0.000071  \n",
      "3       0.000008  \n",
      "4       0.000033  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "# final 0.037889400552850935 0.9849074497663528 PUB 9865\n",
    "\n",
    "# add more other feat, lr1, mnb1, ridge\n",
    "# final 0.03770786578985529 0.9850045852107631\n",
    "\n",
    "# add rf , gbrt\n",
    "# final 0.03795258578516857 0.9850484528308193 PUB 9861\n",
    "\n",
    "# change to 9865 version, add ridge v2\n",
    "# final 0.03771567518393781 0.984993096072177 PUB 9864"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
