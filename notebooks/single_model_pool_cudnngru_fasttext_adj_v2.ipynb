{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 67)\n",
      "[5.00000000e+01 4.60000000e+01 2.64000000e+02 0.00000000e+00\n",
      " 2.00000000e+01 0.00000000e+00 3.00000000e+00 1.20000000e+01\n",
      " 4.24000000e+00 0.00000000e+00 6.43939394e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 1.51515152e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 4.60000000e+01 3.40000000e+02 2.50000000e+01\n",
      " 2.50000000e-01 9.20000000e-01 5.00000000e+01 1.00000000e+00\n",
      " 4.00000000e-01 3.00000000e+01 6.00000000e-01 6.00000000e-02\n",
      " 2.40000000e-01 4.97183589e-03 2.19470500e-04 1.91532348e-03\n",
      " 1.12080601e-04 6.59428638e-04 2.94189463e-04 9.43022186e-03\n",
      " 6.50492942e-04 2.44203987e-03 3.31069919e-04 3.26886404e-03\n",
      " 6.48667070e-04 4.30602517e-04 7.68448229e-09 2.80973771e-05\n",
      " 9.47565532e-09 2.08501852e-05 1.59291015e-07 4.78512980e-01\n",
      " 4.94473060e-01 4.93796720e-01 4.86044940e-01 4.89667930e-01\n",
      " 4.97940090e-01 4.37223070e-03 7.44488508e-03 1.37984240e-02\n",
      " 2.68202183e-03 1.16276474e-02 6.19806917e-03]\n",
      "[3.49500713e-02 9.33609959e-02 5.16619944e-02 0.00000000e+00\n",
      " 2.09863589e-02 0.00000000e+00 2.21893491e-03 1.20000000e-02\n",
      " 2.61501211e-03 0.00000000e+00 6.45107832e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.19159566e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.20970043e-02 4.39760604e-02 8.88888889e-01\n",
      " 2.64705882e-01 9.19935949e-01 3.49500713e-02 0.00000000e+00\n",
      " 4.00000000e-01 2.40000000e-02 6.00000000e-01 6.00000000e-02\n",
      " 2.40000000e-01 4.83638733e-03 2.17394968e-04 1.79814374e-03\n",
      " 1.11942435e-04 5.97145528e-04 2.77291187e-04 9.39025052e-03\n",
      " 5.99427416e-04 2.30538448e-03 3.11601646e-04 3.20398478e-03\n",
      " 6.23104892e-04 4.30602517e-04 7.68448229e-09 2.80973771e-05\n",
      " 9.47565544e-09 2.08501852e-05 1.59291015e-07 6.12307004e-02\n",
      " 9.00218375e-02 5.42136083e-02 5.61924912e-02 4.89451054e-02\n",
      " 8.05734814e-02 4.32942913e-03 7.05352530e-03 1.33625856e-02\n",
      " 2.61513462e-03 1.13988430e-02 6.09049778e-03]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import string\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "# prepare other feat\n",
    "fl = [\n",
    "    '../features/other_feat.pkl',\n",
    "    '../features/lgb1_feat.pkl',\n",
    "    '../features/lr_feat2.pkl',\n",
    "    '../features/mnb_feat2.pkl',\n",
    "    '../features/wordbatch_feat.pkl',\n",
    "    '../features/tilli_lr_feat.pkl',\n",
    "]\n",
    "def get_feat(f):\n",
    "    with open(f,'rb') as fin:\n",
    "        a,b = pickle.load(fin)\n",
    "        return a,b\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in fl:\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "print(train_x[0])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 250) (153164, 250)\n",
      "dorm 18265\n",
      "iup 29568\n",
      "linksearch 30224\n",
      "lewisham 71281\n",
      "racoler 60543\n",
      "lucien 18005\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 66521\n"
     ]
    }
   ],
   "source": [
    "max_features = 160000\n",
    "maxlen = 250\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "    (b'&lt;3', b' heart '),\n",
    "    (b':d', b' smile '),\n",
    "    (b':dd', b' smile '),\n",
    "    (b':p', b' smile '),\n",
    "    (b'8\\)', b' smile '),\n",
    "    (b':-\\)', b' smile '),\n",
    "    (b':\\)', b' smile '),\n",
    "    (b';\\)', b' smile '),\n",
    "    (b'\\(-:', b' smile '),\n",
    "    (b'\\(:', b' smile '),\n",
    "    (b'yay!', b' good '),\n",
    "    (b'yay', b' good '),\n",
    "    (b'yaay', b' good '),\n",
    "    (b':/', b' worry '),\n",
    "    (b':&gt;', b' angry '),\n",
    "    (b\":'\\)\", b' sad '),\n",
    "    (b':-\\(', b' sad '),\n",
    "    (b':\\(', b' sad '),\n",
    "    (b':s', b' sad '),\n",
    "    (b':-s', b' sad '),\n",
    "    (b'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', b' '),\n",
    "    (b'(\\[[\\s\\S]*\\])', b' '),\n",
    "    (b'[\\s]*?(www.[\\S]*)', b' ')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def new_clean(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    \n",
    "    # replace words like hhhhhhhhhhhhhhi with hi\n",
    "    for ch in string.ascii_lowercase:\n",
    "        pattern = bytes(ch+'{3,}', encoding=\"utf-8\")\n",
    "        clean = re.sub(pattern, bytes(ch, encoding=\"utf-8\"), clean)\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return new_clean(text)\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model(comp):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    inp_2 = Input(shape=[train_x.shape[1]], name=\"other\")\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool,inp_2])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp,inp_2], outputs=outp)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     48000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 256)     330240      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          506         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "other (InputLayer)              (None, 67)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 835)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 other[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            5016        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 48,335,762\n",
      "Trainable params: 335,762\n",
      "Non-trainable params: 48,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = get_model(False)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        curr_other_x = train_x[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        hold_out_other_x = train_x[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model(True)\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit([curr_x,curr_other_x], curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=([hold_out_x,hold_out_other_x],hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict([X_test,test_x])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x,hold_out_other_x])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.04018, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 157s 1ms/step - loss: 0.0470 - acc: 0.9827 - val_loss: 0.0402 - val_acc: 0.9840\n",
      "Epoch 2/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00002: val_loss improved from 0.04018 to 0.03876, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 158s 1ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Epoch 3/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9853Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 159s 1ms/step - loss: 0.0377 - acc: 0.9853 - val_loss: 0.0396 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9857Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 162s 1ms/step - loss: 0.0365 - acc: 0.9857 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 5/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9862Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0351 - acc: 0.9862 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0340 - acc: 0.9864 - val_loss: 0.0406 - val_acc: 0.9838\n",
      "Epoch 7/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0331 - acc: 0.9868 - val_loss: 0.0399 - val_acc: 0.9841\n",
      "Epoch 8/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9873Epoch 00008: val_loss did not improve\n",
      "127656/127656 [==============================] - 162s 1ms/step - loss: 0.0319 - acc: 0.9873 - val_loss: 0.0409 - val_acc: 0.9839\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9824Epoch 00001: val_loss improved from inf to 0.04495, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 169s 1ms/step - loss: 0.0477 - acc: 0.9824 - val_loss: 0.0450 - val_acc: 0.9825\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9845Epoch 00002: val_loss improved from 0.04495 to 0.04039, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 168s 1ms/step - loss: 0.0399 - acc: 0.9845 - val_loss: 0.0404 - val_acc: 0.9842\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9850Epoch 00003: val_loss improved from 0.04039 to 0.03950, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 168s 1ms/step - loss: 0.0384 - acc: 0.9850 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9852Epoch 00004: val_loss improved from 0.03950 to 0.03929, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 168s 1ms/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0393 - val_acc: 0.9845\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9856Epoch 00005: val_loss improved from 0.03929 to 0.03890, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 168s 1ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0389 - val_acc: 0.9847\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9859Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 168s 1ms/step - loss: 0.0349 - acc: 0.9859 - val_loss: 0.0404 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9862Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 168s 1ms/step - loss: 0.0340 - acc: 0.9862 - val_loss: 0.0389 - val_acc: 0.9848\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9867Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 167s 1ms/step - loss: 0.0331 - acc: 0.9866 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9824Epoch 00001: val_loss improved from inf to 0.03849, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 170s 1ms/step - loss: 0.0478 - acc: 0.9824 - val_loss: 0.0385 - val_acc: 0.9853\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9845Epoch 00002: val_loss improved from 0.03849 to 0.03746, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 172s 1ms/step - loss: 0.0400 - acc: 0.9845 - val_loss: 0.0375 - val_acc: 0.9853\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9849Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 169s 1ms/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0381 - val_acc: 0.9853\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 167s 1ms/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0432 - val_acc: 0.9825\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0358 - acc: 0.9857 - val_loss: 0.0383 - val_acc: 0.9848\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9860Epoch 00006: val_loss improved from 0.03746 to 0.03730, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 169s 1ms/step - loss: 0.0347 - acc: 0.9860 - val_loss: 0.0373 - val_acc: 0.9854\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0336 - acc: 0.9865 - val_loss: 0.0375 - val_acc: 0.9855\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9868Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0328 - acc: 0.9868 - val_loss: 0.0383 - val_acc: 0.9853\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9825Epoch 00001: val_loss improved from inf to 0.03952, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 169s 1ms/step - loss: 0.0477 - acc: 0.9825 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00002: val_loss improved from 0.03952 to 0.03772, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 166s 1ms/step - loss: 0.0397 - acc: 0.9846 - val_loss: 0.0377 - val_acc: 0.9849\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0378 - acc: 0.9851 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0383 - val_acc: 0.9847\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 156s 1ms/step - loss: 0.0353 - acc: 0.9860 - val_loss: 0.0381 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 156s 1ms/step - loss: 0.0341 - acc: 0.9862 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 156s 1ms/step - loss: 0.0332 - acc: 0.9866 - val_loss: 0.0391 - val_acc: 0.9843\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 158s 1ms/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03914, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0473 - acc: 0.9826 - val_loss: 0.0391 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9846Epoch 00002: val_loss improved from 0.03914 to 0.03874, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 157s 1ms/step - loss: 0.0394 - acc: 0.9846 - val_loss: 0.0387 - val_acc: 0.9852\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0400 - val_acc: 0.9844\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 156s 1ms/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0396 - val_acc: 0.9848\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 155s 1ms/step - loss: 0.0351 - acc: 0.9860 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 155s 1ms/step - loss: 0.0338 - acc: 0.9864 - val_loss: 0.0399 - val_acc: 0.9849\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 155s 1ms/step - loss: 0.0329 - acc: 0.9868 - val_loss: 0.0405 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9872Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 155s 1ms/step - loss: 0.0319 - acc: 0.9872 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "-------------------------------\n",
      "0 0.08250731717959926 0.9683025111079081\n",
      "1 0.02141532987947001 0.9909131358454857\n",
      "2 0.04178438009440388 0.9831172330811989\n",
      "3 0.007662522162494782 0.9974306108252753\n",
      "4 0.057296930738458665 0.9766373589186005\n",
      "5 0.01904059637624983 0.993294520934255\n",
      "final 0.038284512738446075 0.9849492284521206\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999094      0.566806  0.980147  0.151731  0.938186   \n",
      "1  0000247867823ef7  0.001555      0.000078  0.000234  0.000044  0.000260   \n",
      "2  00013b17ad220c46  0.000713      0.000090  0.000222  0.000068  0.000263   \n",
      "3  00017563c3f7919a  0.000344      0.000037  0.000113  0.000064  0.000124   \n",
      "4  00017695ad8997eb  0.005532      0.000133  0.000454  0.000105  0.000326   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.650009  \n",
      "1       0.000038  \n",
      "2       0.000087  \n",
      "3       0.000015  \n",
      "4       0.000061  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "\n",
    "\n",
    "# fold4 output feat\n",
    "# final 0.0401879770356 0.984252568867, pre model, not good\n",
    "\n",
    "# test add other feat, NON-NN feat to nn\n",
    "# add first 3 files, 3891 1st fold\n",
    "# add more non-nn feats, 3883 1st fold\n",
    "# final 0.03855334827205233 0.9847883805119143\n",
    "\n",
    "# len 250, max word 160000, more text clean\n",
    "# 1st fold, 3876\n",
    "# final 0.038284512738446075 0.9849492284521206"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9828Epoch 00001: val_loss improved from inf to 0.03932, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 167s 1ms/step - loss: 0.0469 - acc: 0.9828 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 2/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00002: val_loss improved from 0.03932 to 0.03920, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 169s 1ms/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 3/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9851Epoch 00003: val_loss improved from 0.03920 to 0.03850, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 174s 1ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0385 - val_acc: 0.9849\n",
      "Epoch 4/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9857Epoch 00004: val_loss improved from 0.03850 to 0.03776, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 195s 1ms/step - loss: 0.0363 - acc: 0.9857 - val_loss: 0.0378 - val_acc: 0.9849\n",
      "Epoch 5/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 175s 1ms/step - loss: 0.0349 - acc: 0.9860 - val_loss: 0.0381 - val_acc: 0.9850\n",
      "Epoch 6/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 172s 1ms/step - loss: 0.0338 - acc: 0.9864 - val_loss: 0.0388 - val_acc: 0.9847\n",
      "Epoch 7/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 172s 1ms/step - loss: 0.0329 - acc: 0.9867 - val_loss: 0.0407 - val_acc: 0.9838\n",
      "Epoch 8/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9872Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 170s 1ms/step - loss: 0.0318 - acc: 0.9872 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.04094, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 182s 1ms/step - loss: 0.0469 - acc: 0.9827 - val_loss: 0.0409 - val_acc: 0.9837\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9846Epoch 00002: val_loss improved from 0.04094 to 0.03950, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 191s 1ms/step - loss: 0.0398 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9842\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850Epoch 00003: val_loss improved from 0.03950 to 0.03888, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 184s 1ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0389 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0371 - acc: 0.9853 - val_loss: 0.0391 - val_acc: 0.9842\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0394 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0348 - acc: 0.9860 - val_loss: 0.0430 - val_acc: 0.9827\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0340 - acc: 0.9864 - val_loss: 0.0403 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0397 - val_acc: 0.9839\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03945, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0468 - acc: 0.9827 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00002: val_loss improved from 0.03945 to 0.03855, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 184s 1ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0385 - val_acc: 0.9847\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9852Epoch 00003: val_loss improved from 0.03855 to 0.03804, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 189s 1ms/step - loss: 0.0380 - acc: 0.9852 - val_loss: 0.0380 - val_acc: 0.9848\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00004: val_loss improved from 0.03804 to 0.03729, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 193s 1ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0373 - val_acc: 0.9852\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9859Epoch 00005: val_loss improved from 0.03729 to 0.03721, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 183s 1ms/step - loss: 0.0355 - acc: 0.9859 - val_loss: 0.0372 - val_acc: 0.9851\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0343 - acc: 0.9862 - val_loss: 0.0395 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0333 - acc: 0.9867 - val_loss: 0.0376 - val_acc: 0.9850\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0400 - val_acc: 0.9842\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9828Epoch 00001: val_loss improved from inf to 0.04124, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 182s 1ms/step - loss: 0.0465 - acc: 0.9828 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9848Epoch 00002: val_loss improved from 0.04124 to 0.03929, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 179s 1ms/step - loss: 0.0392 - acc: 0.9849 - val_loss: 0.0393 - val_acc: 0.9847\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0374 - acc: 0.9852 - val_loss: 0.0402 - val_acc: 0.9846\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9857Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 169s 1ms/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0410 - val_acc: 0.9841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0349 - acc: 0.9860 - val_loss: 0.0396 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0339 - acc: 0.9864 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0328 - acc: 0.9867 - val_loss: 0.0408 - val_acc: 0.9845\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0332 - acc: 0.9866 - val_loss: 0.0423 - val_acc: 0.9844\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03926, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 180s 1ms/step - loss: 0.0472 - acc: 0.9826 - val_loss: 0.0393 - val_acc: 0.9851\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00002: val_loss improved from 0.03926 to 0.03792, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 173s 1ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0379 - val_acc: 0.9853\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00003: val_loss improved from 0.03792 to 0.03756, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 179s 1ms/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0376 - val_acc: 0.9852\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0381 - val_acc: 0.9852\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0354 - acc: 0.9858 - val_loss: 0.0391 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0343 - acc: 0.9862 - val_loss: 0.0383 - val_acc: 0.9851\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0333 - acc: 0.9865 - val_loss: 0.0390 - val_acc: 0.9850\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0388 - val_acc: 0.9849\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03916, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0475 - acc: 0.9826 - val_loss: 0.0392 - val_acc: 0.9850\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9846Epoch 00002: val_loss improved from 0.03916 to 0.03688, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 178s 1ms/step - loss: 0.0396 - acc: 0.9846 - val_loss: 0.0369 - val_acc: 0.9851\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00003: val_loss improved from 0.03688 to 0.03679, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 181s 1ms/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0368 - val_acc: 0.9851\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0383 - val_acc: 0.9849\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.0375 - val_acc: 0.9853\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0343 - acc: 0.9863 - val_loss: 0.0394 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 170s 1ms/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0383 - val_acc: 0.9851\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9829Epoch 00001: val_loss improved from inf to 0.03898, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0469 - acc: 0.9829 - val_loss: 0.0390 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00002: val_loss improved from 0.03898 to 0.03736, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 186s 1ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0374 - val_acc: 0.9848\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 172s 1ms/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0384 - val_acc: 0.9848\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00004: val_loss improved from 0.03736 to 0.03694, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 180s 1ms/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0369 - val_acc: 0.9850\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0353 - acc: 0.9860 - val_loss: 0.0376 - val_acc: 0.9850\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0339 - acc: 0.9864 - val_loss: 0.0376 - val_acc: 0.9849\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 168s 1ms/step - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9869Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0324 - acc: 0.9869 - val_loss: 0.0397 - val_acc: 0.9840\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03985, saving model to weights_base.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143614/143614 [==============================] - 180s 1ms/step - loss: 0.0471 - acc: 0.9827 - val_loss: 0.0398 - val_acc: 0.9840\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00002: val_loss did not improve\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0405 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9851Epoch 00003: val_loss improved from 0.03985 to 0.03979, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 188s 1ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9856Epoch 00004: val_loss improved from 0.03979 to 0.03843, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 185s 1ms/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0384 - val_acc: 0.9846\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0353 - acc: 0.9859 - val_loss: 0.0397 - val_acc: 0.9839\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0392 - val_acc: 0.9842\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0403 - val_acc: 0.9839\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 167s 1ms/step - loss: 0.0323 - acc: 0.9870 - val_loss: 0.0408 - val_acc: 0.9839\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03755, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 191s 1ms/step - loss: 0.0471 - acc: 0.9826 - val_loss: 0.0375 - val_acc: 0.9854\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00002: val_loss improved from 0.03755 to 0.03693, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0369 - val_acc: 0.9857\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 174s 1ms/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0372 - val_acc: 0.9852\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 176s 1ms/step - loss: 0.0363 - acc: 0.9854 - val_loss: 0.0372 - val_acc: 0.9857\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9860Epoch 00005: val_loss improved from 0.03693 to 0.03655, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 177s 1ms/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0365 - val_acc: 0.9857\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 174s 1ms/step - loss: 0.0339 - acc: 0.9862 - val_loss: 0.0376 - val_acc: 0.9852\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 176s 1ms/step - loss: 0.0330 - acc: 0.9867 - val_loss: 0.0380 - val_acc: 0.9851\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9871Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 175s 1ms/step - loss: 0.0321 - acc: 0.9871 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.04068, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 186s 1ms/step - loss: 0.0468 - acc: 0.9827 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9847Epoch 00002: val_loss improved from 0.04068 to 0.04017, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 180s 1ms/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0402 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00003: val_loss improved from 0.04017 to 0.03949, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 178s 1ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0395 - val_acc: 0.9846\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 175s 1ms/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0397 - val_acc: 0.9844\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 176s 1ms/step - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0408 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 176s 1ms/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 176s 1ms/step - loss: 0.0332 - acc: 0.9866 - val_loss: 0.0421 - val_acc: 0.9839\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9869Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 176s 1ms/step - loss: 0.0325 - acc: 0.9869 - val_loss: 0.0425 - val_acc: 0.9834\n",
      "-------------------------------\n",
      "0 0.08082543653391008 0.9688226557457182\n",
      "1 0.021397893897689292 0.9907564657738561\n",
      "2 0.041551766082576304 0.9831799011098508\n",
      "3 0.007702592090216501 0.9973616759937582\n",
      "4 0.05684692948295613 0.9763741531982628\n",
      "5 0.0190117852297573 0.99294984677667\n",
      "final 0.037889400552850935 0.9849074497663528\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998906      0.415498  0.988035  0.169566  0.948082   \n",
      "1  0000247867823ef7  0.000631      0.000018  0.000143  0.000021  0.000142   \n",
      "2  00013b17ad220c46  0.000435      0.000033  0.000256  0.000061  0.000262   \n",
      "3  00017563c3f7919a  0.000205      0.000014  0.000095  0.000045  0.000104   \n",
      "4  00017695ad8997eb  0.003361      0.000057  0.000298  0.000110  0.000269   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.695658  \n",
      "1       0.000022  \n",
      "2       0.000130  \n",
      "3       0.000012  \n",
      "4       0.000055  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "# final 0.037889400552850935 0.9849074497663528"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
