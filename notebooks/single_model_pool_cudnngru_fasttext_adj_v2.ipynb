{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn2d_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cnn_v2_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_cudnn_gru_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_gru_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/fasttext_lstm_v1_5_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 133)\n",
      "[ 5.00000000e+01  4.60000000e+01  2.64000000e+02  0.00000000e+00\n",
      "  2.00000000e+01  0.00000000e+00  3.00000000e+00  1.20000000e+01\n",
      "  4.24000000e+00  0.00000000e+00  6.43939394e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.51515152e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.60000000e+01  3.40000000e+02  2.50000000e+01\n",
      "  2.50000000e-01  9.20000000e-01  5.00000000e+01  1.00000000e+00\n",
      "  4.00000000e-01  3.00000000e+01  6.00000000e-01  6.00000000e-02\n",
      "  2.40000000e-01  4.97183589e-03  2.19470500e-04  1.91532348e-03\n",
      "  1.12080601e-04  6.59428638e-04  2.94189463e-04  6.48717407e-03\n",
      "  1.43058692e-03  3.50386039e-03  7.49363315e-04  2.99510552e-03\n",
      "  1.17916128e-03  7.35962419e-03  6.12739154e-04  2.56212582e-03\n",
      "  3.21850807e-04  2.84586342e-03  5.57503823e-04 -2.61802127e-02\n",
      " -3.55490310e-03 -2.24073743e-02 -2.24863418e-03 -3.27414011e-02\n",
      " -9.82143021e-03 -8.71819733e-03 -1.09741811e-02 -1.36728622e-02\n",
      " -4.26841221e-03 -2.65620055e-02 -1.09669200e-02  1.47045578e-02\n",
      "  2.72551819e-04  4.38906331e-03  6.81379250e-05  4.40233785e-03\n",
      "  2.41395784e-04  3.02297447e-04  5.99867599e-09  2.28718188e-05\n",
      "  1.14941809e-08  1.50047469e-05  1.12412318e-07  4.78512980e-01\n",
      "  4.94473060e-01  4.93796720e-01  4.86044940e-01  4.89667930e-01\n",
      "  4.97940090e-01  4.09644022e-03  6.09518792e-03  1.30472148e-02\n",
      "  2.28091293e-03  1.25153108e-02  6.42610836e-03  3.20521576e-05\n",
      "  3.98428599e-12  1.90362641e-07  4.36426484e-15  3.14909670e-07\n",
      "  2.49261084e-10  1.03984428e-04  1.01192825e-06  7.71425694e-05\n",
      "  5.70658983e-07  2.76282262e-05  4.82240830e-06  1.07179466e-03\n",
      "  4.30504834e-13  4.95859013e-06  3.81039023e-09  2.17764386e-06\n",
      "  4.04727665e-08  2.31894222e-03  1.04079954e-08  7.93577201e-05\n",
      "  3.33727321e-08  2.54135884e-05  6.86043165e-07  3.92362796e-04\n",
      "  8.51056670e-10  7.99019381e-06  8.22520896e-09  3.10496898e-06\n",
      "  5.43770611e-08  1.43150217e-04  8.10625309e-08  1.20443437e-05\n",
      "  7.84154608e-09  4.47905268e-06  1.02983597e-06  2.64098839e-04\n",
      "  6.69312954e-07  2.83593632e-04  1.90346142e-07  3.36135199e-05\n",
      "  5.04876652e-06]\n",
      "[3.49500713e-02 9.33609959e-02 5.16619944e-02 0.00000000e+00\n",
      " 2.09863589e-02 0.00000000e+00 2.21893491e-03 1.20000000e-02\n",
      " 2.61501211e-03 0.00000000e+00 6.45107832e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.19159566e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.20970043e-02 4.39760604e-02 8.88888889e-01\n",
      " 2.64705882e-01 9.19935949e-01 3.49500713e-02 0.00000000e+00\n",
      " 4.00000000e-01 2.40000000e-02 6.00000000e-01 6.00000000e-02\n",
      " 2.40000000e-01 4.83638733e-03 2.17394968e-04 1.79814374e-03\n",
      " 1.11942435e-04 5.97145528e-04 2.77291187e-04 6.41563293e-03\n",
      " 1.38321079e-03 3.37450246e-03 7.06767259e-04 2.88629059e-03\n",
      " 1.12457600e-03 7.31546044e-03 5.68872108e-04 2.40054267e-03\n",
      " 3.05619484e-04 2.77494446e-03 5.34113253e-04 6.67202266e-02\n",
      " 4.87881259e-02 2.28364572e-02 3.87381735e-02 3.56464940e-02\n",
      " 2.62395352e-02 6.39271377e-02 4.78549963e-02 2.95811533e-02\n",
      " 4.17371945e-02 3.53564038e-02 3.71946016e-02 1.47043943e-02\n",
      " 2.72562789e-04 4.38900502e-03 6.98790411e-05 4.40231664e-03\n",
      " 2.41533516e-04 3.02297447e-04 5.99867599e-09 2.28718188e-05\n",
      " 1.14941809e-08 1.50047469e-05 1.12412318e-07 6.12307004e-02\n",
      " 9.00218375e-02 5.42136083e-02 5.61924912e-02 4.89451054e-02\n",
      " 8.05734814e-02 4.06211702e-03 5.70668569e-03 1.26388567e-02\n",
      " 2.22039166e-03 1.22442901e-02 6.32540379e-03 3.20521454e-05\n",
      " 4.89555083e-12 1.90365273e-07 4.64348831e-15 3.15957909e-07\n",
      " 2.59789556e-10 9.16498370e-05 1.13355713e-06 7.24869416e-05\n",
      " 5.81399818e-07 2.43423178e-05 4.81439416e-06 1.07179477e-03\n",
      " 4.88805527e-13 4.95880057e-06 3.81615681e-09 2.18490891e-06\n",
      " 4.19219603e-08 2.31894219e-03 1.29394237e-08 7.93581931e-05\n",
      " 3.53740753e-08 2.54700022e-05 7.03738971e-07 3.88531271e-04\n",
      " 9.67030148e-10 7.92067129e-06 8.69702087e-09 3.12861708e-06\n",
      " 6.02284241e-08 1.43142182e-04 8.13033035e-08 1.20445149e-05\n",
      " 7.93952987e-09 4.49932945e-06 1.05974129e-06 2.64098439e-04\n",
      " 6.85468495e-07 2.83630892e-04 2.00738290e-07 3.37293069e-05\n",
      " 5.24582129e-06]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import string\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "# prepare other feat\n",
    "fl = [\n",
    "    '../features/other_feat.pkl',\n",
    "    '../features/lgb1_feat.pkl',\n",
    "    '../features/lr_feat1.pkl',\n",
    "    '../features/lr_feat2.pkl',\n",
    "    '../features/ridge_feat1.pkl',\n",
    "    '../features/ridge_feat2.pkl',\n",
    "    '../features/mnb_feat1.pkl',\n",
    "    '../features/mnb_feat2.pkl',\n",
    "    '../features/wordbatch_feat.pkl',\n",
    "    '../features/tilli_lr_feat.pkl',\n",
    "    '../features/fasttext_cnn2d_5_feat.pkl',\n",
    "'../features/fasttext_cnn_gru_5_feat.pkl',\n",
    "'../features/fasttext_cnn_v1_5_feat.pkl',\n",
    "'../features/fasttext_cnn_v2_5_feat.pkl',\n",
    "'../features/fasttext_cudnn_gru_5_feat.pkl',\n",
    "'../features/fasttext_gru_v1_5_feat.pkl',\n",
    "'../features/fasttext_lstm_v1_5_feat.pkl',\n",
    "\n",
    "]\n",
    "def get_feat(f):\n",
    "    with open(f,'rb') as fin:\n",
    "        a,b = pickle.load(fin)\n",
    "        return a,b\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in fl:\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "print(train_x[0])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 250) (153164, 250)\n",
      "rebelion 71283\n",
      "proverb 21036\n",
      "ecn 70849\n",
      "diamter 135075\n",
      "jerkwads 107383\n",
      "libeling 32578\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 66521\n"
     ]
    }
   ],
   "source": [
    "max_features = 160000\n",
    "maxlen = 250\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "    (b'&lt;3', b' heart '),\n",
    "    (b':d', b' smile '),\n",
    "    (b':dd', b' smile '),\n",
    "    (b':p', b' smile '),\n",
    "    (b'8\\)', b' smile '),\n",
    "    (b':-\\)', b' smile '),\n",
    "    (b':\\)', b' smile '),\n",
    "    (b';\\)', b' smile '),\n",
    "    (b'\\(-:', b' smile '),\n",
    "    (b'\\(:', b' smile '),\n",
    "    (b'yay!', b' good '),\n",
    "    (b'yay', b' good '),\n",
    "    (b'yaay', b' good '),\n",
    "    (b':/', b' worry '),\n",
    "    (b':&gt;', b' angry '),\n",
    "    (b\":'\\)\", b' sad '),\n",
    "    (b':-\\(', b' sad '),\n",
    "    (b':\\(', b' sad '),\n",
    "    (b':s', b' sad '),\n",
    "    (b':-s', b' sad '),\n",
    "    (b'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', b' '),\n",
    "    (b'(\\[[\\s\\S]*\\])', b' '),\n",
    "    (b'[\\s]*?(www.[\\S]*)', b' ')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def new_clean(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    \n",
    "    # replace words like hhhhhhhhhhhhhhi with hi\n",
    "    for ch in string.ascii_lowercase:\n",
    "        pattern = bytes(ch+'{3,}', encoding=\"utf-8\")\n",
    "        clean = re.sub(pattern, bytes(ch, encoding=\"utf-8\"), clean)\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return new_clean(text)\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model(comp):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    inp_2 = Input(shape=[train_x.shape[1]], name=\"other\")\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool,inp_2])\n",
    "    # conc = Dropout(0.1)(conc)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp,inp_2], outputs=outp)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     48000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 256)     330240      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          506         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "other (InputLayer)              (None, 133)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 901)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 other[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            5412        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 48,336,158\n",
      "Trainable params: 336,158\n",
      "Non-trainable params: 48,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = get_model(False)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        curr_other_x = train_x[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        hold_out_other_x = train_x[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model(True)\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit([curr_x,curr_other_x], curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=([hold_out_x,hold_out_other_x],hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict([X_test,test_x])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x,hold_out_other_x])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9835Epoch 00001: val_loss improved from inf to 0.03980, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 155s 1ms/step - loss: 0.0454 - acc: 0.9835 - val_loss: 0.0398 - val_acc: 0.9845\n",
      "Epoch 2/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9851Epoch 00002: val_loss improved from 0.03980 to 0.03860, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 155s 1ms/step - loss: 0.0388 - acc: 0.9851 - val_loss: 0.0386 - val_acc: 0.9847\n",
      "Epoch 3/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9855Epoch 00003: val_loss improved from 0.03860 to 0.03821, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 159s 1ms/step - loss: 0.0374 - acc: 0.9855 - val_loss: 0.0382 - val_acc: 0.9849\n",
      "Epoch 4/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9857Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0363 - acc: 0.9857 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 5/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0352 - acc: 0.9860 - val_loss: 0.0412 - val_acc: 0.9835\n",
      "Epoch 6/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0341 - acc: 0.9864 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 7/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9869Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0331 - acc: 0.9869 - val_loss: 0.0404 - val_acc: 0.9842\n",
      "Epoch 8/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9871Epoch 00008: val_loss did not improve\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0322 - acc: 0.9871 - val_loss: 0.0408 - val_acc: 0.9837\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9834Epoch 00001: val_loss improved from inf to 0.03977, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0454 - acc: 0.9834 - val_loss: 0.0398 - val_acc: 0.9846\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9850Epoch 00002: val_loss improved from 0.03977 to 0.03890, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0390 - acc: 0.9850 - val_loss: 0.0389 - val_acc: 0.9847\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9853Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0378 - acc: 0.9853 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9855Epoch 00004: val_loss improved from 0.03890 to 0.03846, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0385 - val_acc: 0.9849\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0387 - val_acc: 0.9847\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0346 - acc: 0.9862 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0337 - acc: 0.9866 - val_loss: 0.0391 - val_acc: 0.9845\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9865Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 160s 1ms/step - loss: 0.0338 - acc: 0.9865 - val_loss: 0.0393 - val_acc: 0.9848\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9833Epoch 00001: val_loss improved from inf to 0.03842, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0453 - acc: 0.9833 - val_loss: 0.0384 - val_acc: 0.9852\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9848Epoch 00002: val_loss did not improve\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0394 - acc: 0.9848 - val_loss: 0.0392 - val_acc: 0.9852\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9851Epoch 00003: val_loss improved from 0.03842 to 0.03726, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 164s 1ms/step - loss: 0.0381 - acc: 0.9851 - val_loss: 0.0373 - val_acc: 0.9855\n",
      "Epoch 4/8\n",
      " 11392/127657 [=>............................] - ETA: 2:15 - loss: 0.0357 - acc: 0.9860"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "\n",
    "\n",
    "# fold4 output feat\n",
    "# final 0.0401879770356 0.984252568867, pre model, not good\n",
    "\n",
    "# test add other feat, NON-NN feat to nn\n",
    "# add first 3 files, 3891 1st fold\n",
    "# add more non-nn feats, 3883 1st fold\n",
    "# final 0.03855334827205233 0.9847883805119143\n",
    "\n",
    "# len 250, max word 160000, more text clean\n",
    "# 1st fold, 3876\n",
    "# final 0.038284512738446075 0.9849492284521206\n",
    "\n",
    "# add more other feat, lr1, mnb1, ridge, 1st fold 3831, 3824, 3729, 3804, 3839\n",
    "# final 0.03805630898087786 0.9848301591976821\n",
    "\n",
    "# add no pretrained, bottom dr 0.1\n",
    "# 1st fold 3908\n",
    "\n",
    "# rm bottom dr, 1st fold 3796 , later worse\n",
    "\n",
    "# add fasttext nn other model, 3821, 3846, 3726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "# final 0.037889400552850935 0.9849074497663528 PUB 9865\n",
    "\n",
    "# add more other feat, lr1, mnb1, ridge\n",
    "# final 0.03770786578985529 0.9850045852107631"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
