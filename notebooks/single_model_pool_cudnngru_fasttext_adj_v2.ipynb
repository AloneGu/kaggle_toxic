{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "curso 86457\n",
      "jibbering 111182\n",
      "sapphire 46228\n",
      "evenly 18861\n",
      "kellers 26834\n",
      "executive 4211\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 79399\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model(comp=True):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    gru_out = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    \n",
    "    x = Conv1D(64, kernel_size = 2, padding = \"valid\")(gru_out)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(gru_out)\n",
    "    conc = concatenate([att,avg_pool, max_pool])\n",
    "    \n",
    "    # dr and BN\n",
    "    conc = Dropout(0.2)(conc)\n",
    "    conc = BatchNormalization()(conc)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 150, 300)     54000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 150, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 150, 256)     330240      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 149, 64)      32832       bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          406         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 384)          1536        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            2310        batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 54,367,324\n",
      "Trainable params: 366,556\n",
      "Non-trainable params: 54,000,768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = get_model(False)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "#         del model\n",
    "#         gc.collect()\n",
    "#         K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9737Epoch 00001: val_loss improved from inf to 0.04705, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 143s 1ms/step - loss: 0.0738 - acc: 0.9737 - val_loss: 0.0470 - val_acc: 0.9826\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9816Epoch 00002: val_loss improved from 0.04705 to 0.04250, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 139s 1ms/step - loss: 0.0501 - acc: 0.9816 - val_loss: 0.0425 - val_acc: 0.9835\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9830Epoch 00003: val_loss improved from 0.04250 to 0.04073, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 140s 1ms/step - loss: 0.0450 - acc: 0.9830 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9836Epoch 00004: val_loss improved from 0.04073 to 0.04051, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0405 - val_acc: 0.9837\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9843Epoch 00005: val_loss improved from 0.04051 to 0.03976, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0405 - acc: 0.9843 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9849Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0390 - acc: 0.9849 - val_loss: 0.0420 - val_acc: 0.9831\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9855Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0373 - acc: 0.9855 - val_loss: 0.0409 - val_acc: 0.9839\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9859Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0361 - acc: 0.9859 - val_loss: 0.0422 - val_acc: 0.9837\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0729 - acc: 0.9738Epoch 00001: val_loss improved from inf to 0.04540, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0729 - acc: 0.9738 - val_loss: 0.0454 - val_acc: 0.9830\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9817Epoch 00002: val_loss improved from 0.04540 to 0.04533, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0490 - acc: 0.9817 - val_loss: 0.0453 - val_acc: 0.9823\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9829Epoch 00003: val_loss improved from 0.04533 to 0.04134, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0413 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9834Epoch 00004: val_loss improved from 0.04134 to 0.04132, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0428 - acc: 0.9834 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9840Epoch 00005: val_loss improved from 0.04132 to 0.04075, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0412 - acc: 0.9840 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00006: val_loss improved from 0.04075 to 0.04063, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0406 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9852Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0379 - acc: 0.9852 - val_loss: 0.0414 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9856Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 141s 1ms/step - loss: 0.0366 - acc: 0.9856 - val_loss: 0.0413 - val_acc: 0.9842\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9739Epoch 00001: val_loss improved from inf to 0.04307, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 143s 1ms/step - loss: 0.0733 - acc: 0.9739 - val_loss: 0.0431 - val_acc: 0.9837\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9820Epoch 00002: val_loss improved from 0.04307 to 0.04158, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 143s 1ms/step - loss: 0.0487 - acc: 0.9820 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9827Epoch 00003: val_loss improved from 0.04158 to 0.04098, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0450 - acc: 0.9827 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9837Epoch 00004: val_loss improved from 0.04098 to 0.03946, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0424 - acc: 0.9837 - val_loss: 0.0395 - val_acc: 0.9847\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9841Epoch 00005: val_loss improved from 0.03946 to 0.03917, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 143s 1ms/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0392 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9848Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0390 - acc: 0.9848 - val_loss: 0.0400 - val_acc: 0.9845\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0400 - val_acc: 0.9844\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9858Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 142s 1ms/step - loss: 0.0363 - acc: 0.9858 - val_loss: 0.0403 - val_acc: 0.9839\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0734 - acc: 0.9741Epoch 00001: val_loss improved from inf to 0.04564, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 144s 1ms/step - loss: 0.0733 - acc: 0.9741 - val_loss: 0.0456 - val_acc: 0.9832\n",
      "Epoch 2/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9816Epoch 00002: val_loss improved from 0.04564 to 0.04407, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 143s 1ms/step - loss: 0.0490 - acc: 0.9816 - val_loss: 0.0441 - val_acc: 0.9835\n",
      "Epoch 3/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9828Epoch 00003: val_loss improved from 0.04407 to 0.04221, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 143s 1ms/step - loss: 0.0452 - acc: 0.9828 - val_loss: 0.0422 - val_acc: 0.9835\n",
      "Epoch 4/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9835Epoch 00004: val_loss did not improve\n",
      "119679/119679 [==============================] - 142s 1ms/step - loss: 0.0425 - acc: 0.9835 - val_loss: 0.0438 - val_acc: 0.9834\n",
      "Epoch 5/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9842Epoch 00005: val_loss improved from 0.04221 to 0.04119, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 143s 1ms/step - loss: 0.0408 - acc: 0.9842 - val_loss: 0.0412 - val_acc: 0.9841\n",
      "Epoch 6/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9849Epoch 00006: val_loss did not improve\n",
      "119679/119679 [==============================] - 142s 1ms/step - loss: 0.0389 - acc: 0.9849 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9853Epoch 00007: val_loss did not improve\n",
      "119679/119679 [==============================] - 142s 1ms/step - loss: 0.0374 - acc: 0.9853 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 8/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9858Epoch 00008: val_loss did not improve\n",
      "119679/119679 [==============================] - 142s 1ms/step - loss: 0.0361 - acc: 0.9858 - val_loss: 0.0442 - val_acc: 0.9827\n",
      "-------------------------------\n",
      "0 0.0877463395806 0.96635980222\n",
      "1 0.022590711267 0.990324056376\n",
      "2 0.0439554470249 0.982728691304\n",
      "3 0.0081749894525 0.997374209599\n",
      "4 0.0586310490062 0.975960544209\n",
      "5 0.0200293258827 0.992768109494\n",
      "final 0.0401879770356 0.984252568867\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.996905      0.446136  0.986337  0.106733  0.944002   \n",
      "1  0000247867823ef7  0.000604      0.000032  0.000161  0.000008  0.000152   \n",
      "2  00013b17ad220c46  0.000906      0.000108  0.000276  0.000049  0.000239   \n",
      "3  00017563c3f7919a  0.000346      0.000026  0.000097  0.000015  0.000173   \n",
      "4  00017695ad8997eb  0.004371      0.000158  0.000637  0.000118  0.000477   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.691290  \n",
      "1       0.000019  \n",
      "2       0.000073  \n",
      "3       0.000016  \n",
      "4       0.000093  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_4.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_4_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "\n",
    "# fold10 test\n",
    "# check fisrt 1 fold\n",
    "# gru  conv val_loss\n",
    "# pre       3865 3960\n",
    "# 128  128  3954 \n",
    "# 128  64   3901\n",
    "# 96   64   3928\n",
    "\n",
    "# dr    val_loss\n",
    "# 0.4   3881\n",
    "# 0.3   3976\n",
    "# 0.45  3988\n",
    "\n",
    "# add dr and BN at bottom\n",
    "# bottom dr  val_loss\n",
    "# 0.3        3900\n",
    "# 0.2 nadam  3865 4090 seems good\n",
    "# 0.1 nadam  3921\n",
    "\n",
    "# fold4 output feat\n",
    "# final 0.0401879770356 0.984252568867"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
