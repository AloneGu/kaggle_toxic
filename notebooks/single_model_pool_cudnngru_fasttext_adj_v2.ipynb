{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 91)\n",
      "[ 5.00000000e+01  4.60000000e+01  2.64000000e+02  0.00000000e+00\n",
      "  2.00000000e+01  0.00000000e+00  3.00000000e+00  1.20000000e+01\n",
      "  4.24000000e+00  0.00000000e+00  6.43939394e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.51515152e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.60000000e+01  3.40000000e+02  2.50000000e+01\n",
      "  2.50000000e-01  9.20000000e-01  5.00000000e+01  1.00000000e+00\n",
      "  4.00000000e-01  3.00000000e+01  6.00000000e-01  6.00000000e-02\n",
      "  2.40000000e-01  4.97183589e-03  2.19470500e-04  1.91532348e-03\n",
      "  1.12080601e-04  6.59428638e-04  2.94189463e-04  6.48717407e-03\n",
      "  1.43058692e-03  3.50386039e-03  7.49363315e-04  2.99510552e-03\n",
      "  1.17916128e-03  7.35962419e-03  6.12739154e-04  2.56212582e-03\n",
      "  3.21850807e-04  2.84586342e-03  5.57503823e-04 -2.61802127e-02\n",
      " -3.55490310e-03 -2.24073743e-02 -2.24863418e-03 -3.27414011e-02\n",
      " -9.82143021e-03 -8.71819733e-03 -1.09741811e-02 -1.36728622e-02\n",
      " -4.26841221e-03 -2.65620055e-02 -1.09669200e-02  1.47045578e-02\n",
      "  2.72551819e-04  4.38906331e-03  6.81379250e-05  4.40233785e-03\n",
      "  2.41395784e-04  3.02297447e-04  5.99867599e-09  2.28718188e-05\n",
      "  1.14941809e-08  1.50047469e-05  1.12412318e-07  4.78512980e-01\n",
      "  4.94473060e-01  4.93796720e-01  4.86044940e-01  4.89667930e-01\n",
      "  4.97940090e-01  4.09644022e-03  6.09518792e-03  1.30472148e-02\n",
      "  2.28091293e-03  1.25153108e-02  6.42610836e-03]\n",
      "[3.49500713e-02 9.33609959e-02 5.16619944e-02 0.00000000e+00\n",
      " 2.09863589e-02 0.00000000e+00 2.21893491e-03 1.20000000e-02\n",
      " 2.61501211e-03 0.00000000e+00 6.45107832e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.19159566e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.20970043e-02 4.39760604e-02 8.88888889e-01\n",
      " 2.64705882e-01 9.19935949e-01 3.49500713e-02 0.00000000e+00\n",
      " 4.00000000e-01 2.40000000e-02 6.00000000e-01 6.00000000e-02\n",
      " 2.40000000e-01 4.83638733e-03 2.17394968e-04 1.79814374e-03\n",
      " 1.11942435e-04 5.97145528e-04 2.77291187e-04 6.41563293e-03\n",
      " 1.38321079e-03 3.37450246e-03 7.06767259e-04 2.88629059e-03\n",
      " 1.12457600e-03 7.31546044e-03 5.68872108e-04 2.40054267e-03\n",
      " 3.05619484e-04 2.77494446e-03 5.34113253e-04 6.67202266e-02\n",
      " 4.87881259e-02 2.28364572e-02 3.87381735e-02 3.56464940e-02\n",
      " 2.62395352e-02 6.39271377e-02 4.78549963e-02 2.95811533e-02\n",
      " 4.17371945e-02 3.53564038e-02 3.71946016e-02 1.47043943e-02\n",
      " 2.72562789e-04 4.38900502e-03 6.98790411e-05 4.40231664e-03\n",
      " 2.41533516e-04 3.02297447e-04 5.99867599e-09 2.28718188e-05\n",
      " 1.14941809e-08 1.50047469e-05 1.12412318e-07 6.12307004e-02\n",
      " 9.00218375e-02 5.42136083e-02 5.61924912e-02 4.89451054e-02\n",
      " 8.05734814e-02 4.06211702e-03 5.70668569e-03 1.26388567e-02\n",
      " 2.22039166e-03 1.22442901e-02 6.32540379e-03]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import string\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "# prepare other feat\n",
    "fl = [\n",
    "    '../features/other_feat.pkl',\n",
    "    '../features/lgb1_feat.pkl',\n",
    "    '../features/lr_feat1.pkl',\n",
    "    '../features/lr_feat2.pkl',\n",
    "    '../features/ridge_feat1.pkl',\n",
    "    '../features/ridge_feat2.pkl',\n",
    "    '../features/mnb_feat1.pkl',\n",
    "    '../features/mnb_feat2.pkl',\n",
    "    '../features/wordbatch_feat.pkl',\n",
    "    '../features/tilli_lr_feat.pkl',\n",
    "]\n",
    "def get_feat(f):\n",
    "    with open(f,'rb') as fin:\n",
    "        a,b = pickle.load(fin)\n",
    "        return a,b\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in fl:\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "print(train_x[0])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 250) (153164, 250)\n",
      "citet 37052\n",
      "wondring 136474\n",
      "maximum 4937\n",
      "muscles 16908\n",
      "beforehand 8844\n",
      "madeleine 22563\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 66521\n"
     ]
    }
   ],
   "source": [
    "max_features = 160000\n",
    "maxlen = 250\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "    (b'&lt;3', b' heart '),\n",
    "    (b':d', b' smile '),\n",
    "    (b':dd', b' smile '),\n",
    "    (b':p', b' smile '),\n",
    "    (b'8\\)', b' smile '),\n",
    "    (b':-\\)', b' smile '),\n",
    "    (b':\\)', b' smile '),\n",
    "    (b';\\)', b' smile '),\n",
    "    (b'\\(-:', b' smile '),\n",
    "    (b'\\(:', b' smile '),\n",
    "    (b'yay!', b' good '),\n",
    "    (b'yay', b' good '),\n",
    "    (b'yaay', b' good '),\n",
    "    (b':/', b' worry '),\n",
    "    (b':&gt;', b' angry '),\n",
    "    (b\":'\\)\", b' sad '),\n",
    "    (b':-\\(', b' sad '),\n",
    "    (b':\\(', b' sad '),\n",
    "    (b':s', b' sad '),\n",
    "    (b':-s', b' sad '),\n",
    "    (b'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', b' '),\n",
    "    (b'(\\[[\\s\\S]*\\])', b' '),\n",
    "    (b'[\\s]*?(www.[\\S]*)', b' ')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def new_clean(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    \n",
    "    # replace words like hhhhhhhhhhhhhhi with hi\n",
    "    for ch in string.ascii_lowercase:\n",
    "        pattern = bytes(ch+'{3,}', encoding=\"utf-8\")\n",
    "        clean = re.sub(pattern, bytes(ch, encoding=\"utf-8\"), clean)\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return new_clean(text)\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model(comp):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    inp_2 = Input(shape=[train_x.shape[1]], name=\"other\")\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool,inp_2])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp,inp_2], outputs=outp)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 250)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 250, 300)     48000000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 250, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 250, 256)     330240      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "attention_1 (Attention)         (None, 256)          506         bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "other (InputLayer)              (None, 91)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 859)          0           attention_1[0][0]                \n",
      "                                                                 global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 other[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            5160        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 48,335,906\n",
      "Trainable params: 335,906\n",
      "Non-trainable params: 48,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m = get_model(False)\n",
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        curr_other_x = train_x[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        hold_out_other_x = train_x[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model(True)\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit([curr_x,curr_other_x], curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=([hold_out_x,hold_out_other_x],hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict([X_test,test_x])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x,hold_out_other_x])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9829Epoch 00001: val_loss improved from inf to 0.04031, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 156s 1ms/step - loss: 0.0471 - acc: 0.9829 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 2/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9847Epoch 00002: val_loss improved from 0.04031 to 0.03956, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 156s 1ms/step - loss: 0.0395 - acc: 0.9847 - val_loss: 0.0396 - val_acc: 0.9841\n",
      "Epoch 3/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00003: val_loss improved from 0.03956 to 0.03919, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 160s 1ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0392 - val_acc: 0.9839\n",
      "Epoch 4/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9857Epoch 00004: val_loss improved from 0.03919 to 0.03879, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 163s 1ms/step - loss: 0.0363 - acc: 0.9857 - val_loss: 0.0388 - val_acc: 0.9842\n",
      "Epoch 5/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9860Epoch 00005: val_loss improved from 0.03879 to 0.03831, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 165s 1ms/step - loss: 0.0351 - acc: 0.9860 - val_loss: 0.0383 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0338 - acc: 0.9865 - val_loss: 0.0414 - val_acc: 0.9835\n",
      "Epoch 7/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9870Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0327 - acc: 0.9870 - val_loss: 0.0430 - val_acc: 0.9828\n",
      "Epoch 8/8\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9873Epoch 00008: val_loss did not improve\n",
      "127656/127656 [==============================] - 161s 1ms/step - loss: 0.0317 - acc: 0.9873 - val_loss: 0.0417 - val_acc: 0.9834\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.04009, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0472 - acc: 0.9826 - val_loss: 0.0401 - val_acc: 0.9845\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9846Epoch 00002: val_loss improved from 0.04009 to 0.03945, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0397 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0403 - val_acc: 0.9844\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00004: val_loss improved from 0.03945 to 0.03824, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0382 - val_acc: 0.9849\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0359 - acc: 0.9858 - val_loss: 0.0406 - val_acc: 0.9837\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0347 - acc: 0.9862 - val_loss: 0.0392 - val_acc: 0.9846\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0335 - acc: 0.9867 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0327 - acc: 0.9870 - val_loss: 0.0408 - val_acc: 0.9843\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03952, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0478 - acc: 0.9826 - val_loss: 0.0395 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00002: val_loss improved from 0.03952 to 0.03729, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0373 - val_acc: 0.9854\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0390 - val_acc: 0.9844\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0367 - acc: 0.9855 - val_loss: 0.0380 - val_acc: 0.9852\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0375 - val_acc: 0.9854\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0378 - val_acc: 0.9853\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0332 - acc: 0.9868 - val_loss: 0.0379 - val_acc: 0.9850\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9872Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0319 - acc: 0.9872 - val_loss: 0.0401 - val_acc: 0.9845\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0471 - acc: 0.9828Epoch 00001: val_loss improved from inf to 0.03994, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0471 - acc: 0.9828 - val_loss: 0.0399 - val_acc: 0.9842\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9847Epoch 00002: val_loss improved from 0.03994 to 0.03875, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0396 - acc: 0.9847 - val_loss: 0.0387 - val_acc: 0.9846\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00003: val_loss improved from 0.03875 to 0.03835, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0383 - val_acc: 0.9842\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9856Epoch 00004: val_loss improved from 0.03835 to 0.03804, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 163s 1ms/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0380 - val_acc: 0.9845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 154s 1ms/step - loss: 0.0352 - acc: 0.9860 - val_loss: 0.0385 - val_acc: 0.9844\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 154s 1ms/step - loss: 0.0341 - acc: 0.9863 - val_loss: 0.0388 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 154s 1ms/step - loss: 0.0330 - acc: 0.9868 - val_loss: 0.0395 - val_acc: 0.9846\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9872Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 154s 1ms/step - loss: 0.0319 - acc: 0.9872 - val_loss: 0.0412 - val_acc: 0.9835\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03985, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 198s 2ms/step - loss: 0.0472 - acc: 0.9826 - val_loss: 0.0399 - val_acc: 0.9845\n",
      "Epoch 2/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9847Epoch 00002: val_loss improved from 0.03985 to 0.03950, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 160s 1ms/step - loss: 0.0396 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9847\n",
      "Epoch 3/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9851Epoch 00003: val_loss improved from 0.03950 to 0.03843, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0384 - val_acc: 0.9850\n",
      "Epoch 4/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9856Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0392 - val_acc: 0.9846\n",
      "Epoch 5/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0350 - acc: 0.9859 - val_loss: 0.0395 - val_acc: 0.9849\n",
      "Epoch 6/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9863Epoch 00006: val_loss improved from 0.03843 to 0.03839, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 162s 1ms/step - loss: 0.0338 - acc: 0.9863 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0327 - acc: 0.9868 - val_loss: 0.0414 - val_acc: 0.9837\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0318 - acc: 0.9871Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 161s 1ms/step - loss: 0.0318 - acc: 0.9871 - val_loss: 0.0395 - val_acc: 0.9846\n",
      "-------------------------------\n",
      "0 0.08083834971274949 0.9685218492081895\n",
      "1 0.021157384924307507 0.9908755350282946\n",
      "2 0.042058079633024256 0.9829668298124346\n",
      "3 0.007994329490344555 0.9974431444310057\n",
      "4 0.056991027521593046 0.976242550338094\n",
      "5 0.01929868260324831 0.9929310463680744\n",
      "final 0.03805630898087786 0.9848301591976821\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998827      0.504171  0.977602  0.078174  0.931503   \n",
      "1  0000247867823ef7  0.000885      0.000032  0.000138  0.000010  0.000273   \n",
      "2  00013b17ad220c46  0.000464      0.000056  0.000185  0.000017  0.000268   \n",
      "3  00017563c3f7919a  0.000346      0.000039  0.000117  0.000037  0.000201   \n",
      "4  00017695ad8997eb  0.005627      0.000115  0.000491  0.000098  0.000463   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.625789  \n",
      "1       0.000036  \n",
      "2       0.000089  \n",
      "3       0.000019  \n",
      "4       0.000062  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "\n",
    "\n",
    "# fold4 output feat\n",
    "# final 0.0401879770356 0.984252568867, pre model, not good\n",
    "\n",
    "# test add other feat, NON-NN feat to nn\n",
    "# add first 3 files, 3891 1st fold\n",
    "# add more non-nn feats, 3883 1st fold\n",
    "# final 0.03855334827205233 0.9847883805119143\n",
    "\n",
    "# len 250, max word 160000, more text clean\n",
    "# 1st fold, 3876\n",
    "# final 0.038284512738446075 0.9849492284521206\n",
    "\n",
    "# add more other feat, lr1, mnb1, ridge\n",
    "# final 0.03805630898087786 0.9848301591976821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9830Epoch 00001: val_loss improved from inf to 0.03993, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0463 - acc: 0.9830 - val_loss: 0.0399 - val_acc: 0.9843\n",
      "Epoch 2/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9846Epoch 00002: val_loss improved from 0.03993 to 0.03951, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0396 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852Epoch 00003: val_loss improved from 0.03951 to 0.03904, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0390 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9855Epoch 00004: val_loss improved from 0.03904 to 0.03810, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0365 - acc: 0.9855 - val_loss: 0.0381 - val_acc: 0.9850\n",
      "Epoch 5/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.0385 - val_acc: 0.9851\n",
      "Epoch 6/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0339 - acc: 0.9864 - val_loss: 0.0397 - val_acc: 0.9847\n",
      "Epoch 7/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0330 - acc: 0.9867 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 8/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9871Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 165s 1ms/step - loss: 0.0321 - acc: 0.9871 - val_loss: 0.0398 - val_acc: 0.9850\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.04078, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0471 - acc: 0.9826 - val_loss: 0.0408 - val_acc: 0.9838\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9846Epoch 00002: val_loss improved from 0.04078 to 0.03956, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0398 - acc: 0.9846 - val_loss: 0.0396 - val_acc: 0.9841\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850Epoch 00003: val_loss improved from 0.03956 to 0.03832, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0383 - val_acc: 0.9842\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0368 - acc: 0.9855 - val_loss: 0.0391 - val_acc: 0.9838\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0357 - acc: 0.9859 - val_loss: 0.0402 - val_acc: 0.9838\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0344 - acc: 0.9864 - val_loss: 0.0396 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0335 - acc: 0.9866 - val_loss: 0.0400 - val_acc: 0.9842\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0325 - acc: 0.9870 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03784, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0469 - acc: 0.9826 - val_loss: 0.0378 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9845Epoch 00002: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0400 - val_acc: 0.9844\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852Epoch 00003: val_loss improved from 0.03784 to 0.03648, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0365 - val_acc: 0.9853\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0414 - val_acc: 0.9834\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0354 - acc: 0.9859 - val_loss: 0.0398 - val_acc: 0.9838\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0343 - acc: 0.9863 - val_loss: 0.0384 - val_acc: 0.9846\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0333 - acc: 0.9866 - val_loss: 0.0376 - val_acc: 0.9849\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0325 - acc: 0.9870 - val_loss: 0.0402 - val_acc: 0.9836\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9829Epoch 00001: val_loss improved from inf to 0.04229, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0464 - acc: 0.9829 - val_loss: 0.0423 - val_acc: 0.9838\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00002: val_loss improved from 0.04229 to 0.03911, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0391 - val_acc: 0.9848\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0407 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0395 - val_acc: 0.9848\n",
      "Epoch 5/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0353 - acc: 0.9859 - val_loss: 0.0395 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0342 - acc: 0.9863 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0332 - acc: 0.9866 - val_loss: 0.0412 - val_acc: 0.9836\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0323 - acc: 0.9870 - val_loss: 0.0406 - val_acc: 0.9840\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9826Epoch 00001: val_loss improved from inf to 0.03967, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0472 - acc: 0.9826 - val_loss: 0.0397 - val_acc: 0.9847\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9844Epoch 00002: val_loss improved from 0.03967 to 0.03883, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0403 - acc: 0.9844 - val_loss: 0.0388 - val_acc: 0.9850\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9848Epoch 00003: val_loss improved from 0.03883 to 0.03725, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0384 - acc: 0.9848 - val_loss: 0.0373 - val_acc: 0.9857\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0391 - val_acc: 0.9852\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9855Epoch 00005: val_loss improved from 0.03725 to 0.03709, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0361 - acc: 0.9855 - val_loss: 0.0371 - val_acc: 0.9856\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9858Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0351 - acc: 0.9858 - val_loss: 0.0383 - val_acc: 0.9852\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9861Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0346 - acc: 0.9861 - val_loss: 0.0376 - val_acc: 0.9855\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9864Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0337 - acc: 0.9864 - val_loss: 0.0387 - val_acc: 0.9847\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9827Epoch 00001: val_loss improved from inf to 0.03862, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0470 - acc: 0.9827 - val_loss: 0.0386 - val_acc: 0.9850\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9846Epoch 00002: val_loss improved from 0.03862 to 0.03734, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0399 - acc: 0.9846 - val_loss: 0.0373 - val_acc: 0.9854\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9849Epoch 00003: val_loss improved from 0.03734 to 0.03665, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0366 - val_acc: 0.9857\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0381 - val_acc: 0.9844\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0372 - val_acc: 0.9851\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9859Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0350 - acc: 0.9859 - val_loss: 0.0376 - val_acc: 0.9849\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0341 - acc: 0.9863 - val_loss: 0.0411 - val_acc: 0.9832\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0334 - acc: 0.9866 - val_loss: 0.0393 - val_acc: 0.9843\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9828Epoch 00001: val_loss improved from inf to 0.03977, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0469 - acc: 0.9828 - val_loss: 0.0398 - val_acc: 0.9843\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9847Epoch 00002: val_loss improved from 0.03977 to 0.03796, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0380 - val_acc: 0.9848\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0393 - val_acc: 0.9848\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9856Epoch 00004: val_loss improved from 0.03796 to 0.03731, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0373 - val_acc: 0.9847\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0383 - val_acc: 0.9849\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 164s 1ms/step - loss: 0.0341 - acc: 0.9863 - val_loss: 0.0394 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0331 - acc: 0.9868 - val_loss: 0.0401 - val_acc: 0.9839\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9871Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0323 - acc: 0.9871 - val_loss: 0.0402 - val_acc: 0.9836\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9829Epoch 00001: val_loss improved from inf to 0.03978, saving model to weights_base.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0466 - acc: 0.9829 - val_loss: 0.0398 - val_acc: 0.9846\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00002: val_loss improved from 0.03978 to 0.03935, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0393 - val_acc: 0.9847\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9851Epoch 00003: val_loss improved from 0.03935 to 0.03867, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0387 - val_acc: 0.9850\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00004: val_loss improved from 0.03867 to 0.03808, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0381 - val_acc: 0.9850\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0356 - acc: 0.9857 - val_loss: 0.0385 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0346 - acc: 0.9861 - val_loss: 0.0400 - val_acc: 0.9843\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0336 - acc: 0.9866 - val_loss: 0.0404 - val_acc: 0.9837\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9867Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0327 - acc: 0.9867 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9830Epoch 00001: val_loss improved from inf to 0.03757, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0465 - acc: 0.9830 - val_loss: 0.0376 - val_acc: 0.9855\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9846Epoch 00002: val_loss improved from 0.03757 to 0.03654, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0395 - acc: 0.9846 - val_loss: 0.0365 - val_acc: 0.9854\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0378 - acc: 0.9851 - val_loss: 0.0376 - val_acc: 0.9852\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9856Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0371 - val_acc: 0.9854\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0374 - val_acc: 0.9851\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0372 - val_acc: 0.9849\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0334 - acc: 0.9866 - val_loss: 0.0375 - val_acc: 0.9855\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9869Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0322 - acc: 0.9869 - val_loss: 0.0394 - val_acc: 0.9844\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9830Epoch 00001: val_loss improved from inf to 0.04106, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0462 - acc: 0.9830 - val_loss: 0.0411 - val_acc: 0.9842\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00002: val_loss improved from 0.04106 to 0.03987, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0399 - val_acc: 0.9846\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9851Epoch 00003: val_loss improved from 0.03987 to 0.03948, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0375 - acc: 0.9851 - val_loss: 0.0395 - val_acc: 0.9842\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9856Epoch 00004: val_loss improved from 0.03948 to 0.03940, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 166s 1ms/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0394 - val_acc: 0.9846\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0396 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0338 - acc: 0.9863 - val_loss: 0.0396 - val_acc: 0.9847\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0329 - acc: 0.9868 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9869Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 165s 1ms/step - loss: 0.0323 - acc: 0.9869 - val_loss: 0.0429 - val_acc: 0.9834\n",
      "-------------------------------\n",
      "0 0.08016755562228536 0.969261331946281\n",
      "1 0.021478888638900042 0.9905433944764399\n",
      "2 0.04117612929849737 0.9832739031528285\n",
      "3 0.007932077770759013 0.9974807452481967\n",
      "4 0.05677132836710548 0.9762864179581503\n",
      "5 0.01872121504158447 0.9931817184826817\n",
      "final 0.03770786578985529 0.9850045852107631\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.999288      0.498148  0.980321  0.113601  0.928467   \n",
      "1  0000247867823ef7  0.000737      0.000029  0.000216  0.000017  0.000197   \n",
      "2  00013b17ad220c46  0.000494      0.000057  0.000319  0.000059  0.000303   \n",
      "3  00017563c3f7919a  0.000275      0.000021  0.000111  0.000050  0.000126   \n",
      "4  00017695ad8997eb  0.003101      0.000072  0.000309  0.000104  0.000240   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.585109  \n",
      "1       0.000021  \n",
      "2       0.000088  \n",
      "3       0.000011  \n",
      "4       0.000040  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj2_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj2_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "# final 0.037889400552850935 0.9849074497663528 PUB 9865\n",
    "\n",
    "# add more other feat, lr1, mnb1, ridge\n",
    "# final 0.03770786578985529 0.9850045852107631"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
