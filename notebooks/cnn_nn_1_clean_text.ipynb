{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "def clean_text( text ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^A-za-z0-9^,?!.\\/'+-=]\",\" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" _exclamationmark_ \", text)\n",
    "    text = re.sub(r\"\\?\", \" _questionmark_ \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Nonsense _questionmark_   kiss off  geek  what I said is true   I will  have your account terminated ',\n",
       "       '    Please do not vandalize pages  as you did with this edit to W  S  Merwin  If you continue to do so  you will be blocked from editing      ',\n",
       "       '      Points of interest     I removed the   points of interest   section you added because it seemed kind of spammy  I know you probably did not  mean to disobey the rules  but generally  a point of interest tends to be rather touristy  and quite irrelevant to an area culture  That  just my opinion  though   If you want to reply  just put your reply here and add   talkback Jamiegraham08   on my talkpage     ',\n",
       "       'Asking some his nationality is a Racial offence  Wow was not  aware of it   Blocking me has shown your support towards your community  Thanku for that',\n",
       "       'The reader here is not going by my say so for ethereal vocal style and dark lyrical content  The cited sources in the External Links are saying those things  If you feel the sources are unreliable or I did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 100) (226998, 100)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Conv1D(64,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/5\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0924 - acc: 0.9715Epoch 00001: val_loss improved from inf to 0.05371, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 15s 235us/step - loss: 0.0924 - acc: 0.9715 - val_loss: 0.0537 - val_acc: 0.9807\n",
      "Epoch 2/5\n",
      "63680/63900 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9817Epoch 00002: val_loss improved from 0.05371 to 0.05198, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 12s 191us/step - loss: 0.0493 - acc: 0.9817 - val_loss: 0.0520 - val_acc: 0.9813\n",
      "Epoch 3/5\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9844Epoch 00003: val_loss did not improve\n",
      "63900/63900 [==============================] - 13s 196us/step - loss: 0.0407 - acc: 0.9844 - val_loss: 0.0538 - val_acc: 0.9812\n",
      "Epoch 4/5\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9867Epoch 00004: val_loss did not improve\n",
      "63900/63900 [==============================] - 12s 183us/step - loss: 0.0345 - acc: 0.9867 - val_loss: 0.0581 - val_acc: 0.9811\n",
      "Epoch 5/5\n",
      "63616/63900 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9886Epoch 00005: val_loss did not improve\n",
      "63900/63900 [==============================] - 12s 184us/step - loss: 0.0291 - acc: 0.9887 - val_loss: 0.0612 - val_acc: 0.9805\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9681Epoch 00001: val_loss improved from inf to 0.05377, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 12s 190us/step - loss: 0.0980 - acc: 0.9681 - val_loss: 0.0538 - val_acc: 0.9802\n",
      "Epoch 2/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0506 - acc: 0.9816Epoch 00002: val_loss improved from 0.05377 to 0.05339, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 11s 178us/step - loss: 0.0506 - acc: 0.9816 - val_loss: 0.0534 - val_acc: 0.9811\n",
      "Epoch 3/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9839Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 11s 179us/step - loss: 0.0424 - acc: 0.9839 - val_loss: 0.0580 - val_acc: 0.9810\n",
      "Epoch 4/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9860Epoch 00004: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 183us/step - loss: 0.0361 - acc: 0.9860 - val_loss: 0.0565 - val_acc: 0.9802\n",
      "Epoch 5/5\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9877Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 184us/step - loss: 0.0311 - acc: 0.9877 - val_loss: 0.0609 - val_acc: 0.9810\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/5\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0927 - acc: 0.9704Epoch 00001: val_loss improved from inf to 0.05433, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 12s 186us/step - loss: 0.0926 - acc: 0.9704 - val_loss: 0.0543 - val_acc: 0.9804\n",
      "Epoch 2/5\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9819Epoch 00002: val_loss improved from 0.05433 to 0.05330, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 11s 179us/step - loss: 0.0490 - acc: 0.9819 - val_loss: 0.0533 - val_acc: 0.9807\n",
      "Epoch 3/5\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9844Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 182us/step - loss: 0.0409 - acc: 0.9844 - val_loss: 0.0549 - val_acc: 0.9806\n",
      "Epoch 4/5\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9866Epoch 00004: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 180us/step - loss: 0.0346 - acc: 0.9866 - val_loss: 0.0585 - val_acc: 0.9804\n",
      "Epoch 5/5\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9886Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 185us/step - loss: 0.0290 - acc: 0.9886 - val_loss: 0.0637 - val_acc: 0.9797\n",
      "-------------------------------\n",
      "0 0.115455622116 0.958216398368\n",
      "1 0.0242955305497 0.990203545086\n",
      "2 0.0584712332487 0.978602205506\n",
      "3 0.0148655987401 0.996817977903\n",
      "4 0.072685647532 0.970902755318\n",
      "5 0.0315687888399 0.991507652502\n",
      "final 0.0528904035044 0.981041755781\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 5\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_clean_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_clean_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.047,  0.   ,  0.005,  0.001,  0.006,  0.002],\n",
       "       [ 0.004,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.01 ,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.193,  0.003,  0.028,  0.006,  0.042,  0.012],\n",
       "       [ 0.005,  0.   ,  0.   ,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.04 ,  0.   ,  0.002,  0.   ,  0.007,  0.001],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.016,  0.   ,  0.002,  0.   ,  0.002,  0.001]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "def get_nn_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/6\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.1291 - acc: 0.9652Epoch 00001: val_loss improved from inf to 0.07444, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 13s 197us/step - loss: 0.1291 - acc: 0.9652 - val_loss: 0.0744 - val_acc: 0.9754\n",
      "Epoch 2/6\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9779Epoch 00002: val_loss improved from 0.07444 to 0.05663, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 12s 195us/step - loss: 0.0653 - acc: 0.9779 - val_loss: 0.0566 - val_acc: 0.9803\n",
      "Epoch 3/6\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9810Epoch 00003: val_loss improved from 0.05663 to 0.05323, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 12s 184us/step - loss: 0.0530 - acc: 0.9810 - val_loss: 0.0532 - val_acc: 0.9812\n",
      "Epoch 4/6\n",
      "63680/63900 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9826Epoch 00004: val_loss improved from 0.05323 to 0.05275, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 12s 189us/step - loss: 0.0465 - acc: 0.9826 - val_loss: 0.0527 - val_acc: 0.9810\n",
      "Epoch 5/6\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9839Epoch 00005: val_loss improved from 0.05275 to 0.05260, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 13s 201us/step - loss: 0.0424 - acc: 0.9839 - val_loss: 0.0526 - val_acc: 0.9814\n",
      "Epoch 6/6\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9850Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 13s 199us/step - loss: 0.0383 - acc: 0.9850 - val_loss: 0.0541 - val_acc: 0.9814\n",
      "temp eval\n",
      "0 0.11792112978 0.958999718319\n",
      "1 0.0238699308046 0.990391537041\n",
      "2 0.0559499751372 0.979155581985\n",
      "3 0.0161412494379 0.996432036556\n",
      "4 0.0714463858267 0.971550186223\n",
      "5 0.0302419350807 0.991643454039\n",
      "final 0.0525951010111 0.981362085694\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/6\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.1314 - acc: 0.9634Epoch 00001: val_loss improved from inf to 0.06723, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 13s 200us/step - loss: 0.1312 - acc: 0.9635 - val_loss: 0.0672 - val_acc: 0.9777\n",
      "Epoch 2/6\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9790Epoch 00002: val_loss improved from 0.06723 to 0.05407, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 11s 180us/step - loss: 0.0615 - acc: 0.9790 - val_loss: 0.0541 - val_acc: 0.9805\n",
      "Epoch 3/6\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9815Epoch 00003: val_loss improved from 0.05407 to 0.05129, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 13s 205us/step - loss: 0.0504 - acc: 0.9815 - val_loss: 0.0513 - val_acc: 0.9816\n",
      "Epoch 4/6\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9829Epoch 00004: val_loss improved from 0.05129 to 0.05059, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 13s 197us/step - loss: 0.0453 - acc: 0.9829 - val_loss: 0.0506 - val_acc: 0.9816\n",
      "Epoch 5/6\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9843Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 194us/step - loss: 0.0410 - acc: 0.9843 - val_loss: 0.0514 - val_acc: 0.9817\n",
      "Epoch 6/6\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9854Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 13s 202us/step - loss: 0.0377 - acc: 0.9854 - val_loss: 0.0520 - val_acc: 0.9816\n",
      "temp eval\n",
      "0 0.10917605005 0.960594679186\n",
      "1 0.0241937974378 0.99048513302\n",
      "2 0.0568499432306 0.978748043818\n",
      "3 0.0127460827178 0.997276995305\n",
      "4 0.0711169344063 0.970860719875\n",
      "5 0.0294311291198 0.99186228482\n",
      "final 0.0505856561603 0.981637976004\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/6\n",
      "63552/63901 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.9628Epoch 00001: val_loss improved from inf to 0.11148, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 13s 200us/step - loss: 0.1405 - acc: 0.9628 - val_loss: 0.1115 - val_acc: 0.9637\n",
      "Epoch 2/6\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0716 - acc: 0.9765Epoch 00002: val_loss improved from 0.11148 to 0.05963, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 12s 183us/step - loss: 0.0716 - acc: 0.9765 - val_loss: 0.0596 - val_acc: 0.9796\n",
      "Epoch 3/6\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9820Epoch 00003: val_loss improved from 0.05963 to 0.05547, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 12s 186us/step - loss: 0.0504 - acc: 0.9820 - val_loss: 0.0555 - val_acc: 0.9799\n",
      "Epoch 4/6\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9837Epoch 00004: val_loss improved from 0.05547 to 0.05452, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 12s 190us/step - loss: 0.0437 - acc: 0.9837 - val_loss: 0.0545 - val_acc: 0.9808\n",
      "Epoch 5/6\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9849Epoch 00005: val_loss improved from 0.05452 to 0.05317, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 12s 184us/step - loss: 0.0393 - acc: 0.9849 - val_loss: 0.0532 - val_acc: 0.9810\n",
      "Epoch 6/6\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 12s 183us/step - loss: 0.0357 - acc: 0.9863 - val_loss: 0.0542 - val_acc: 0.9812\n",
      "temp eval\n",
      "0 0.117316906772 0.957902973396\n",
      "1 0.0247662053652 0.990359937402\n",
      "2 0.0590583338486 0.978153364632\n",
      "3 0.0134243947022 0.996744913928\n",
      "4 0.0746400145591 0.971768388106\n",
      "5 0.0298241119777 0.99117370892\n",
      "final 0.0531716612041 0.981017214397\n",
      "-------------------------------\n",
      "0 0.114804728079 0.959165788568\n",
      "1 0.0242766401818 0.990412202272\n",
      "2 0.0572860702011 0.978685668381\n",
      "3 0.0141039301409 0.996817977903\n",
      "4 0.0724011016754 0.971393099707\n",
      "5 0.0298323963606 0.991559816799\n",
      "final 0.0521174777731 0.981339092272\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n"
     ]
    }
   ],
   "source": [
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_nn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 6\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, \n",
    "                                     save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        print('temp eval')\n",
    "        eval_val(hold_out_y,hold_out_pred)\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/nn_clean_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/nn_clean_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
