{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cinematic 24524\n",
      "linus 28854\n",
      "jobs 4488\n",
      "alterna 47982\n",
      "akerans 40338\n",
      "limon 125179\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3819\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "# print(word_vec_dict['is'])\n",
    "# print(word_vec_dict['are'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76680 samples, validate on 19171 samples\n",
      "Epoch 1/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9771Epoch 00001: val_loss improved from inf to 0.04860, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 415s 5ms/step - loss: 0.0669 - acc: 0.9771 - val_loss: 0.0486 - val_acc: 0.9822\n",
      "Epoch 2/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9815Epoch 00002: val_loss improved from 0.04860 to 0.04475, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0495 - acc: 0.9815 - val_loss: 0.0447 - val_acc: 0.9828\n",
      "Epoch 3/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9825Epoch 00003: val_loss improved from 0.04475 to 0.04275, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0458 - acc: 0.9825 - val_loss: 0.0428 - val_acc: 0.9835\n",
      "Epoch 4/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9832Epoch 00004: val_loss did not improve\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0433 - acc: 0.9832 - val_loss: 0.0429 - val_acc: 0.9834\n",
      "Epoch 5/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9836Epoch 00005: val_loss did not improve\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0418 - acc: 0.9836 - val_loss: 0.0430 - val_acc: 0.9832\n",
      "Epoch 6/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9842Epoch 00006: val_loss improved from 0.04275 to 0.04234, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0400 - acc: 0.9842 - val_loss: 0.0423 - val_acc: 0.9842\n",
      "Epoch 7/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9845Epoch 00007: val_loss did not improve\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0390 - acc: 0.9845 - val_loss: 0.0449 - val_acc: 0.9824\n",
      "Epoch 8/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9848Epoch 00008: val_loss improved from 0.04234 to 0.04211, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 412s 5ms/step - loss: 0.0380 - acc: 0.9848 - val_loss: 0.0421 - val_acc: 0.9836\n",
      "Epoch 9/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9852Epoch 00009: val_loss did not improve\n",
      "76680/76680 [==============================] - 411s 5ms/step - loss: 0.0369 - acc: 0.9852 - val_loss: 0.0423 - val_acc: 0.9836\n",
      "Epoch 10/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9854Epoch 00010: val_loss did not improve\n",
      "76680/76680 [==============================] - 411s 5ms/step - loss: 0.0363 - acc: 0.9854 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "Epoch 11/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9860Epoch 00011: val_loss did not improve\n",
      "76680/76680 [==============================] - 411s 5ms/step - loss: 0.0349 - acc: 0.9860 - val_loss: 0.0440 - val_acc: 0.9821\n",
      "Epoch 12/12\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9860Epoch 00012: val_loss did not improve\n",
      "76680/76680 [==============================] - 411s 5ms/step - loss: 0.0344 - acc: 0.9860 - val_loss: 0.0436 - val_acc: 0.9830\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0660 - acc: 0.9774Epoch 00001: val_loss improved from inf to 0.04701, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 413s 5ms/step - loss: 0.0660 - acc: 0.9774 - val_loss: 0.0470 - val_acc: 0.9822\n",
      "Epoch 2/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0484 - acc: 0.9818Epoch 00002: val_loss improved from 0.04701 to 0.04467, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0484 - acc: 0.9818 - val_loss: 0.0447 - val_acc: 0.9830\n",
      "Epoch 3/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9828Epoch 00003: val_loss improved from 0.04467 to 0.04231, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0449 - acc: 0.9828 - val_loss: 0.0423 - val_acc: 0.9835\n",
      "Epoch 4/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9832Epoch 00004: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0432 - acc: 0.9832 - val_loss: 0.0423 - val_acc: 0.9837\n",
      "Epoch 5/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9836Epoch 00005: val_loss improved from 0.04231 to 0.04107, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0414 - acc: 0.9836 - val_loss: 0.0411 - val_acc: 0.9838\n",
      "Epoch 6/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9841Epoch 00006: val_loss did not improve\n",
      "76681/76681 [==============================] - 411s 5ms/step - loss: 0.0400 - acc: 0.9841 - val_loss: 0.0422 - val_acc: 0.9833\n",
      "Epoch 7/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9845Epoch 00007: val_loss did not improve\n",
      "76681/76681 [==============================] - 411s 5ms/step - loss: 0.0386 - acc: 0.9845 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 8/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9849Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0377 - acc: 0.9849 - val_loss: 0.0416 - val_acc: 0.9835\n",
      "Epoch 9/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00009: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0367 - acc: 0.9853 - val_loss: 0.0423 - val_acc: 0.9831\n",
      "Epoch 10/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9855Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0359 - acc: 0.9855 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 11/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9859Epoch 00011: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0349 - acc: 0.9858 - val_loss: 0.0419 - val_acc: 0.9835\n",
      "Epoch 12/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9861Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 411s 5ms/step - loss: 0.0344 - acc: 0.9861 - val_loss: 0.0430 - val_acc: 0.9838\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9775Epoch 00001: val_loss improved from inf to 0.04803, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 413s 5ms/step - loss: 0.0657 - acc: 0.9775 - val_loss: 0.0480 - val_acc: 0.9815\n",
      "Epoch 2/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9820Epoch 00002: val_loss improved from 0.04803 to 0.04468, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0480 - acc: 0.9820 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "Epoch 3/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9826Epoch 00003: val_loss improved from 0.04468 to 0.04320, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0451 - acc: 0.9826 - val_loss: 0.0432 - val_acc: 0.9834\n",
      "Epoch 4/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9834Epoch 00004: val_loss improved from 0.04320 to 0.04270, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.0427 - val_acc: 0.9837\n",
      "Epoch 5/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9838Epoch 00005: val_loss improved from 0.04270 to 0.04249, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0425 - val_acc: 0.9837\n",
      "Epoch 6/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9845Epoch 00006: val_loss improved from 0.04249 to 0.04201, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0398 - acc: 0.9845 - val_loss: 0.0420 - val_acc: 0.9838\n",
      "Epoch 7/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9848Epoch 00007: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0422 - val_acc: 0.9835\n",
      "Epoch 8/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9853Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0424 - val_acc: 0.9834\n",
      "Epoch 9/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9854Epoch 00009: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0366 - acc: 0.9854 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Epoch 10/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9857Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0356 - acc: 0.9857 - val_loss: 0.0428 - val_acc: 0.9836\n",
      "Epoch 11/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9858Epoch 00011: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0352 - acc: 0.9859 - val_loss: 0.0429 - val_acc: 0.9833\n",
      "Epoch 12/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9863Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 412s 5ms/step - loss: 0.0342 - acc: 0.9863 - val_loss: 0.0433 - val_acc: 0.9828\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0666 - acc: 0.9770Epoch 00001: val_loss improved from inf to 0.04937, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 418s 5ms/step - loss: 0.0666 - acc: 0.9770 - val_loss: 0.0494 - val_acc: 0.9815\n",
      "Epoch 2/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9818Epoch 00002: val_loss improved from 0.04937 to 0.04575, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0482 - acc: 0.9818 - val_loss: 0.0458 - val_acc: 0.9824\n",
      "Epoch 3/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9829Epoch 00003: val_loss did not improve\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0449 - acc: 0.9829 - val_loss: 0.0468 - val_acc: 0.9818\n",
      "Epoch 4/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9833Epoch 00004: val_loss improved from 0.04575 to 0.04478, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0429 - acc: 0.9833 - val_loss: 0.0448 - val_acc: 0.9827\n",
      "Epoch 5/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9840Epoch 00005: val_loss improved from 0.04478 to 0.04371, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0409 - acc: 0.9840 - val_loss: 0.0437 - val_acc: 0.9832\n",
      "Epoch 6/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9845Epoch 00006: val_loss improved from 0.04371 to 0.04321, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0395 - acc: 0.9845 - val_loss: 0.0432 - val_acc: 0.9832\n",
      "Epoch 7/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849Epoch 00007: val_loss did not improve\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0437 - val_acc: 0.9831\n",
      "Epoch 8/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0433 - val_acc: 0.9831\n",
      "Epoch 9/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9854Epoch 00009: val_loss improved from 0.04321 to 0.04286, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0361 - acc: 0.9854 - val_loss: 0.0429 - val_acc: 0.9836\n",
      "Epoch 10/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9859Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0350 - acc: 0.9859 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "Epoch 11/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9862Epoch 00011: val_loss did not improve\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0344 - acc: 0.9862 - val_loss: 0.0449 - val_acc: 0.9829\n",
      "Epoch 12/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9862Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0338 - acc: 0.9862 - val_loss: 0.0438 - val_acc: 0.9831\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9779Epoch 00001: val_loss improved from inf to 0.04647, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 416s 5ms/step - loss: 0.0639 - acc: 0.9779 - val_loss: 0.0465 - val_acc: 0.9822\n",
      "Epoch 2/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9818Epoch 00002: val_loss improved from 0.04647 to 0.04356, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0482 - acc: 0.9818 - val_loss: 0.0436 - val_acc: 0.9829\n",
      "Epoch 3/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9828Epoch 00003: val_loss improved from 0.04356 to 0.04216, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0448 - acc: 0.9828 - val_loss: 0.0422 - val_acc: 0.9834\n",
      "Epoch 4/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9832Epoch 00004: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0431 - acc: 0.9832 - val_loss: 0.0425 - val_acc: 0.9834\n",
      "Epoch 5/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9839Epoch 00005: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0415 - acc: 0.9839 - val_loss: 0.0424 - val_acc: 0.9835\n",
      "Epoch 6/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9842Epoch 00006: val_loss improved from 0.04216 to 0.04120, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0401 - acc: 0.9842 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 7/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9846Epoch 00007: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0426 - val_acc: 0.9832\n",
      "Epoch 8/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9851Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0376 - acc: 0.9851 - val_loss: 0.0425 - val_acc: 0.9831\n",
      "Epoch 9/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00009: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0413 - val_acc: 0.9837\n",
      "Epoch 10/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9856Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0416 - val_acc: 0.9836\n",
      "Epoch 11/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9860Epoch 00011: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0418 - val_acc: 0.9836\n",
      "Epoch 12/12\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9861Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 414s 5ms/step - loss: 0.0344 - acc: 0.9861 - val_loss: 0.0417 - val_acc: 0.9840\n",
      "-------------------------------\n",
      "0 0.0910250195272 0.965383772731\n",
      "1 0.0222024922164 0.99028700796\n",
      "2 0.0474044767524 0.981951153353\n",
      "3 0.00970413317373 0.996546723561\n",
      "4 0.0605058338699 0.975274123379\n",
      "5 0.0202593307345 0.993020417106\n",
      "final 0.041850214379 0.983743866348\n",
      "all eval None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 12\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_glove_1_fold5_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_glove_1_fold5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "\n",
    "# pre 4166, 4161, 4300\n",
    "# dropout embed, rate to 0.2\n",
    "# new 4175, 4217, 4264"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.266,  0.002,  0.035,  0.01 ,  0.04 ,  0.001],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.034,  0.   ,  0.003,  0.   ,  0.003,  0.006],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.19 ,  0.   ,  0.019,  0.   ,  0.042,  0.003],\n",
       "       [ 0.007,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.336,  0.001,  0.016,  0.068,  0.019,  0.005],\n",
       "       [ 0.056,  0.   ,  0.004,  0.   ,  0.003,  0.001],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
