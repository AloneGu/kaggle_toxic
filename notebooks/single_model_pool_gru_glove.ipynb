{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "mcgeddon 15239\n",
      "clean 1471\n",
      "yanking 56920\n",
      "palatines 125209\n",
      "rulebreaker 180184\n",
      "correlating 46296\n",
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9800Epoch 00001: val_loss improved from inf to 0.04576, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 489s 4ms/step - loss: 0.0579 - acc: 0.9800 - val_loss: 0.0458 - val_acc: 0.9824\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9833Epoch 00002: val_loss improved from 0.04576 to 0.04244, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 490s 4ms/step - loss: 0.0438 - acc: 0.9833 - val_loss: 0.0424 - val_acc: 0.9836\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9842Epoch 00003: val_loss improved from 0.04244 to 0.04208, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 485s 4ms/step - loss: 0.0410 - acc: 0.9842 - val_loss: 0.0421 - val_acc: 0.9832\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9849Epoch 00004: val_loss improved from 0.04208 to 0.04132, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 487s 4ms/step - loss: 0.0389 - acc: 0.9849 - val_loss: 0.0413 - val_acc: 0.9837\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9855Epoch 00005: val_loss improved from 0.04132 to 0.04064, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 488s 4ms/step - loss: 0.0372 - acc: 0.9855 - val_loss: 0.0406 - val_acc: 0.9839\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9861Epoch 00006: val_loss improved from 0.04064 to 0.04060, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 489s 4ms/step - loss: 0.0356 - acc: 0.9861 - val_loss: 0.0406 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 491s 4ms/step - loss: 0.0339 - acc: 0.9867 - val_loss: 0.0421 - val_acc: 0.9834\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9873Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 492s 4ms/step - loss: 0.0325 - acc: 0.9873 - val_loss: 0.0425 - val_acc: 0.9836\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0577 - acc: 0.9795Epoch 00001: val_loss improved from inf to 0.04424, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 495s 4ms/step - loss: 0.0577 - acc: 0.9795 - val_loss: 0.0442 - val_acc: 0.9832\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9834Epoch 00002: val_loss improved from 0.04424 to 0.04226, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 492s 4ms/step - loss: 0.0433 - acc: 0.9834 - val_loss: 0.0423 - val_acc: 0.9837\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9843Epoch 00003: val_loss improved from 0.04226 to 0.04135, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 490s 4ms/step - loss: 0.0405 - acc: 0.9843 - val_loss: 0.0414 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9848Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 497s 4ms/step - loss: 0.0385 - acc: 0.9848 - val_loss: 0.0418 - val_acc: 0.9841\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 493s 4ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0414 - val_acc: 0.9841\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 491s 4ms/step - loss: 0.0351 - acc: 0.9860 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9866Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 491s 4ms/step - loss: 0.0336 - acc: 0.9866 - val_loss: 0.0421 - val_acc: 0.9841\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9872Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 491s 4ms/step - loss: 0.0322 - acc: 0.9872 - val_loss: 0.0429 - val_acc: 0.9835\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0584 - acc: 0.9795Epoch 00001: val_loss improved from inf to 0.04305, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 493s 4ms/step - loss: 0.0584 - acc: 0.9795 - val_loss: 0.0430 - val_acc: 0.9838\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9833Epoch 00002: val_loss improved from 0.04305 to 0.04150, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 492s 4ms/step - loss: 0.0438 - acc: 0.9833 - val_loss: 0.0415 - val_acc: 0.9840\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9841Epoch 00003: val_loss improved from 0.04150 to 0.04050, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 493s 4ms/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0405 - val_acc: 0.9845\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9848Epoch 00004: val_loss improved from 0.04050 to 0.04030, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 494s 4ms/step - loss: 0.0388 - acc: 0.9848 - val_loss: 0.0403 - val_acc: 0.9840\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9856Epoch 00005: val_loss improved from 0.04030 to 0.03975, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 493s 4ms/step - loss: 0.0371 - acc: 0.9856 - val_loss: 0.0397 - val_acc: 0.9842\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 484s 4ms/step - loss: 0.0354 - acc: 0.9861 - val_loss: 0.0399 - val_acc: 0.9847\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 484s 4ms/step - loss: 0.0335 - acc: 0.9868 - val_loss: 0.0417 - val_acc: 0.9836\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9874Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 485s 4ms/step - loss: 0.0322 - acc: 0.9874 - val_loss: 0.0430 - val_acc: 0.9835\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9796Epoch 00001: val_loss improved from inf to 0.04417, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 490s 4ms/step - loss: 0.0573 - acc: 0.9796 - val_loss: 0.0442 - val_acc: 0.9833\n",
      "Epoch 2/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9837Epoch 00002: val_loss improved from 0.04417 to 0.04298, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 494s 4ms/step - loss: 0.0425 - acc: 0.9837 - val_loss: 0.0430 - val_acc: 0.9837\n",
      "Epoch 3/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9846Epoch 00003: val_loss improved from 0.04298 to 0.04154, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 500s 4ms/step - loss: 0.0399 - acc: 0.9846 - val_loss: 0.0415 - val_acc: 0.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9852Epoch 00004: val_loss improved from 0.04154 to 0.04104, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 484s 4ms/step - loss: 0.0376 - acc: 0.9852 - val_loss: 0.0410 - val_acc: 0.9844\n",
      "Epoch 5/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "119679/119679 [==============================] - 484s 4ms/step - loss: 0.0357 - acc: 0.9860 - val_loss: 0.0412 - val_acc: 0.9843\n",
      "Epoch 6/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "119679/119679 [==============================] - 485s 4ms/step - loss: 0.0341 - acc: 0.9865 - val_loss: 0.0418 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9869Epoch 00007: val_loss did not improve\n",
      "119679/119679 [==============================] - 486s 4ms/step - loss: 0.0327 - acc: 0.9869 - val_loss: 0.0421 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9876Epoch 00008: val_loss did not improve\n",
      "119679/119679 [==============================] - 489s 4ms/step - loss: 0.0313 - acc: 0.9876 - val_loss: 0.0432 - val_acc: 0.9838\n",
      "-------------------------------\n",
      "0 0.0894374356979 0.965921126019\n",
      "1 0.021876810766 0.990549661279\n",
      "2 0.0451803694333 0.982634689261\n",
      "3 0.00841699935096 0.997317808374\n",
      "4 0.0594638434591 0.975709872095\n",
      "5 0.0197360570308 0.992855844734\n",
      "final 0.040685252623 0.984164833627\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: \"../results/pool_gru1_glove_sample_<_io.TextIOWrapper name='../glove.840B.300d.txt' mode='r' encoding='UTF-8'>.gz\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-84ba7925ebbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0msample_submission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../results/pool_gru1_glove_sample_{}.gz\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gzip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../features/pool_gru_{}_feat.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[1;32m   1522\u001b[0m                                      \u001b[0mdoublequote\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                                      escapechar=escapechar, decimal=decimal)\n\u001b[0;32m-> 1524\u001b[0;31m         \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/formats/format.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1635\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[1;32m   1636\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1637\u001b[0;31m                                      compression=self.compression)\n\u001b[0m\u001b[1;32m   1638\u001b[0m             \u001b[0mclose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/gzip.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"write\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'b'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \"../results/pool_gru1_glove_sample_<_io.TextIOWrapper name='../glove.840B.300d.txt' mode='r' encoding='UTF-8'>.gz\""
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "\n",
    "# 40000, 150\n",
    "# final 0.040685252623 0.984164833627, pub 9845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.996088      0.612122  0.979455  0.129746  0.948955   \n",
      "1  0000247867823ef7  0.000551      0.000014  0.000254  0.000006  0.000210   \n",
      "2  00013b17ad220c46  0.001690      0.000379  0.002866  0.000068  0.000768   \n",
      "3  00017563c3f7919a  0.000229      0.000010  0.000132  0.000032  0.000126   \n",
      "4  00017695ad8997eb  0.007934      0.000175  0.000948  0.000172  0.000653   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.663246  \n",
      "1       0.000029  \n",
      "2       0.000111  \n",
      "3       0.000009  \n",
      "4       0.000080  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_glove_sample_4.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_glove_4_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
