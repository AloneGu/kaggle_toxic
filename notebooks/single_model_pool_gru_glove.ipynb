{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "sorensen 61060\n",
      "diffrences 66417\n",
      "neareast 115448\n",
      "frenchies 99341\n",
      "nclined 76256\n",
      "raccoons 70422\n",
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 76050\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import gc\n",
    "from keras import backend as K\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(CuDNNGRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0593 - acc: 0.9789Epoch 00001: val_loss improved from inf to 0.04380, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 72s 605us/step - loss: 0.0593 - acc: 0.9789 - val_loss: 0.0438 - val_acc: 0.9828\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9832Epoch 00002: val_loss improved from 0.04380 to 0.04135, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 595us/step - loss: 0.0444 - acc: 0.9832 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00003: val_loss improved from 0.04135 to 0.04062, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 70s 584us/step - loss: 0.0417 - acc: 0.9839 - val_loss: 0.0406 - val_acc: 0.9840\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9845Epoch 00004: val_loss improved from 0.04062 to 0.04038, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 70s 584us/step - loss: 0.0400 - acc: 0.9845 - val_loss: 0.0404 - val_acc: 0.9840\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9850Epoch 00005: val_loss improved from 0.04038 to 0.04008, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 592us/step - loss: 0.0385 - acc: 0.9850 - val_loss: 0.0401 - val_acc: 0.9839\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9854Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 69s 579us/step - loss: 0.0373 - acc: 0.9854 - val_loss: 0.0406 - val_acc: 0.9838\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9857Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 69s 578us/step - loss: 0.0365 - acc: 0.9857 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9861Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 69s 579us/step - loss: 0.0355 - acc: 0.9861 - val_loss: 0.0438 - val_acc: 0.9825\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0595 - acc: 0.9792Epoch 00001: val_loss improved from inf to 0.04605, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 594us/step - loss: 0.0595 - acc: 0.9792 - val_loss: 0.0461 - val_acc: 0.9826\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9829Epoch 00002: val_loss improved from 0.04605 to 0.04211, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 597us/step - loss: 0.0449 - acc: 0.9829 - val_loss: 0.0421 - val_acc: 0.9836\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9836Epoch 00003: val_loss improved from 0.04211 to 0.04164, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 72s 598us/step - loss: 0.0421 - acc: 0.9836 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9842Epoch 00004: val_loss improved from 0.04164 to 0.04093, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 597us/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9848Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 71s 592us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0431 - val_acc: 0.9833\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00006: val_loss improved from 0.04093 to 0.03988, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 597us/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0399 - val_acc: 0.9846\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 71s 592us/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0399 - val_acc: 0.9847\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9857Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 71s 592us/step - loss: 0.0359 - acc: 0.9857 - val_loss: 0.0419 - val_acc: 0.9834\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0597 - acc: 0.9785Epoch 00001: val_loss improved from inf to 0.04349, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 72s 599us/step - loss: 0.0597 - acc: 0.9785 - val_loss: 0.0435 - val_acc: 0.9835\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9827Epoch 00002: val_loss improved from 0.04349 to 0.04142, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 71s 597us/step - loss: 0.0452 - acc: 0.9827 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9837Epoch 00003: val_loss improved from 0.04142 to 0.04046, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 74s 619us/step - loss: 0.0422 - acc: 0.9837 - val_loss: 0.0405 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9842Epoch 00004: val_loss improved from 0.04046 to 0.03988, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 75s 624us/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0399 - val_acc: 0.9841\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9846Epoch 00005: val_loss improved from 0.03988 to 0.03950, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 75s 627us/step - loss: 0.0390 - acc: 0.9846 - val_loss: 0.0395 - val_acc: 0.9841\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9851Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 74s 618us/step - loss: 0.0380 - acc: 0.9851 - val_loss: 0.0399 - val_acc: 0.9840\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9853Epoch 00007: val_loss improved from 0.03950 to 0.03922, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 75s 623us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0392 - val_acc: 0.9845\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9858Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 74s 618us/step - loss: 0.0361 - acc: 0.9858 - val_loss: 0.0400 - val_acc: 0.9838\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9788Epoch 00001: val_loss improved from inf to 0.04438, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 75s 625us/step - loss: 0.0601 - acc: 0.9788 - val_loss: 0.0444 - val_acc: 0.9830\n",
      "Epoch 2/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9830Epoch 00002: val_loss improved from 0.04438 to 0.04220, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 75s 623us/step - loss: 0.0444 - acc: 0.9830 - val_loss: 0.0422 - val_acc: 0.9835\n",
      "Epoch 3/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9840Epoch 00003: val_loss improved from 0.04220 to 0.04128, saving model to weights_base.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119679/119679 [==============================] - 74s 621us/step - loss: 0.0415 - acc: 0.9840 - val_loss: 0.0413 - val_acc: 0.9839\n",
      "Epoch 4/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9844Epoch 00004: val_loss did not improve\n",
      "119679/119679 [==============================] - 70s 584us/step - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "Epoch 5/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9848Epoch 00005: val_loss improved from 0.04128 to 0.04112, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 71s 590us/step - loss: 0.0387 - acc: 0.9848 - val_loss: 0.0411 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9853Epoch 00006: val_loss did not improve\n",
      "119679/119679 [==============================] - 70s 589us/step - loss: 0.0374 - acc: 0.9853 - val_loss: 0.0416 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9856Epoch 00007: val_loss did not improve\n",
      "119679/119679 [==============================] - 71s 593us/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0416 - val_acc: 0.9834\n",
      "Epoch 8/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9861Epoch 00008: val_loss improved from 0.04112 to 0.04096, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 71s 597us/step - loss: 0.0354 - acc: 0.9861 - val_loss: 0.0410 - val_acc: 0.9841\n",
      "-------------------------------\n",
      "0 0.08807299605732495 0.9660213948649817\n",
      "1 0.02182123717149164 0.9905997957022266\n",
      "2 0.04421876039196561 0.9827098908949621\n",
      "3 0.00822132744954071 0.9971235374848814\n",
      "4 0.05848472510437603 0.976173615506577\n",
      "5 0.019401227099465106 0.993087716439704\n",
      "final 0.040036712212360666 0.9842859918155554\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "\n",
    "# 40000, 150\n",
    "# final 0.040685252623 0.984164833627, pub 9845\n",
    "\n",
    "# 180000, 150, dr rate 0.4\n",
    "# final 0.040036712212360666 0.9842859918155554"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998300      0.533777  0.983988  0.252903  0.952430   \n",
      "1  0000247867823ef7  0.000542      0.000020  0.000191  0.000006  0.000115   \n",
      "2  00013b17ad220c46  0.002197      0.000110  0.000638  0.000062  0.000471   \n",
      "3  00017563c3f7919a  0.000203      0.000010  0.000056  0.000033  0.000055   \n",
      "4  00017695ad8997eb  0.002871      0.000077  0.000317  0.000063  0.000184   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.487567  \n",
      "1       0.000016  \n",
      "2       0.000103  \n",
      "3       0.000004  \n",
      "4       0.000022  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_glove_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_glove_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
