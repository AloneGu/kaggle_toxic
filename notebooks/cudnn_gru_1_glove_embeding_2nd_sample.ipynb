{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyhows 146576\n",
      "sdl 131088\n",
      "teutones 130857\n",
      "9575 147566\n",
      "disaffiliating 90938\n",
      "substatiantion 170646\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "# print(word_vec_dict['is'])\n",
    "# print(word_vec_dict['are'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.layers import Bidirectional, Dropout, CuDNNGRU\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "# https://github.com/PavelOstyakov/toxic/blob/master/toxic/model.py\n",
    "def get_model(comp=True):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=False))(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 256)          330240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               296448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 12,635,110\n",
      "Trainable params: 635,110\n",
      "Non-trainable params: 12,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_m = get_model(False)\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127656, 150)\n",
      "[    10     13     23 ..., 127626 127631 127639]\n",
      "(125221, 150) 12171\n",
      "(31915, 150)\n",
      "[    6    12    16 ..., 31893 31901 31906]\n",
      "(31290, 150) 3123\n",
      "Train on 125221 samples, validate on 31290 samples\n",
      "Epoch 1/6\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9802Epoch 00001: val_loss improved from inf to 0.04131, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 105s 839us/step - loss: 0.0610 - acc: 0.9802 - val_loss: 0.0413 - val_acc: 0.9842\n",
      "Epoch 2/6\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9853Epoch 00002: val_loss improved from 0.04131 to 0.03744, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 104s 829us/step - loss: 0.0395 - acc: 0.9853 - val_loss: 0.0374 - val_acc: 0.9855\n",
      "Epoch 3/6\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9863Epoch 00003: val_loss did not improve\n",
      "125221/125221 [==============================] - 105s 835us/step - loss: 0.0361 - acc: 0.9863 - val_loss: 0.0381 - val_acc: 0.9854\n",
      "Epoch 4/6\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9868Epoch 00004: val_loss improved from 0.03744 to 0.03580, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 105s 836us/step - loss: 0.0340 - acc: 0.9868 - val_loss: 0.0358 - val_acc: 0.9860\n",
      "Epoch 5/6\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9876Epoch 00005: val_loss did not improve\n",
      "125221/125221 [==============================] - 105s 837us/step - loss: 0.0319 - acc: 0.9876 - val_loss: 0.0372 - val_acc: 0.9856\n",
      "Epoch 6/6\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9884Epoch 00006: val_loss did not improve\n",
      "125221/125221 [==============================] - 105s 840us/step - loss: 0.0295 - acc: 0.9884 - val_loss: 0.0361 - val_acc: 0.9860\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127627 127632 127640]\n",
      "(125211, 150) 12229\n",
      "(31914, 150)\n",
      "[   10    13    23 ..., 31894 31907 31908]\n",
      "(31301, 150) 3065\n",
      "Train on 125211 samples, validate on 31301 samples\n",
      "Epoch 1/6\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9795Epoch 00001: val_loss improved from inf to 0.03935, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 106s 843us/step - loss: 0.0601 - acc: 0.9795 - val_loss: 0.0393 - val_acc: 0.9856\n",
      "Epoch 2/6\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9855Epoch 00002: val_loss improved from 0.03935 to 0.03664, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 105s 838us/step - loss: 0.0392 - acc: 0.9855 - val_loss: 0.0366 - val_acc: 0.9861\n",
      "Epoch 3/6\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9864Epoch 00003: val_loss improved from 0.03664 to 0.03629, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 105s 838us/step - loss: 0.0360 - acc: 0.9864 - val_loss: 0.0363 - val_acc: 0.9862\n",
      "Epoch 4/6\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9869Epoch 00004: val_loss improved from 0.03629 to 0.03610, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 105s 836us/step - loss: 0.0339 - acc: 0.9869 - val_loss: 0.0361 - val_acc: 0.9862\n",
      "Epoch 5/6\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9875Epoch 00005: val_loss improved from 0.03610 to 0.03516, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 106s 843us/step - loss: 0.0317 - acc: 0.9875 - val_loss: 0.0352 - val_acc: 0.9866\n",
      "Epoch 6/6\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9882Epoch 00006: val_loss did not improve\n",
      "125211/125211 [==============================] - 105s 837us/step - loss: 0.0295 - acc: 0.9882 - val_loss: 0.0384 - val_acc: 0.9864\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127627 127632 127640]\n",
      "(125203, 150) 12267\n",
      "(31914, 150)\n",
      "[    7    17    26 ..., 31903 31904 31912]\n",
      "(31308, 150) 3027\n",
      "Train on 125203 samples, validate on 31308 samples\n",
      "Epoch 1/6\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9790Epoch 00001: val_loss improved from inf to 0.03930, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 106s 844us/step - loss: 0.0645 - acc: 0.9790 - val_loss: 0.0393 - val_acc: 0.9856\n",
      "Epoch 2/6\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9850Epoch 00002: val_loss improved from 0.03930 to 0.03704, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 105s 835us/step - loss: 0.0405 - acc: 0.9850 - val_loss: 0.0370 - val_acc: 0.9860\n",
      "Epoch 3/6\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9859Epoch 00003: val_loss did not improve\n",
      "125203/125203 [==============================] - 105s 835us/step - loss: 0.0372 - acc: 0.9859 - val_loss: 0.0375 - val_acc: 0.9859\n",
      "Epoch 4/6\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9865Epoch 00004: val_loss improved from 0.03704 to 0.03658, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 105s 836us/step - loss: 0.0350 - acc: 0.9865 - val_loss: 0.0366 - val_acc: 0.9865\n",
      "Epoch 5/6\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9867Epoch 00005: val_loss improved from 0.03658 to 0.03553, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 105s 836us/step - loss: 0.0337 - acc: 0.9867 - val_loss: 0.0355 - val_acc: 0.9866\n",
      "Epoch 6/6\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0315 - acc: 0.9875Epoch 00006: val_loss improved from 0.03553 to 0.03518, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 105s 838us/step - loss: 0.0315 - acc: 0.9875 - val_loss: 0.0352 - val_acc: 0.9867\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127627 127632 127640]\n",
      "(125206, 150) 12252\n",
      "(31914, 150)\n",
      "[   10    23    58 ..., 31886 31906 31911]\n",
      "(31305, 150) 3042\n",
      "Train on 125206 samples, validate on 31305 samples\n",
      "Epoch 1/6\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0616 - acc: 0.9800Epoch 00001: val_loss improved from inf to 0.04079, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 106s 848us/step - loss: 0.0616 - acc: 0.9800 - val_loss: 0.0408 - val_acc: 0.9851\n",
      "Epoch 2/6\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9856Epoch 00002: val_loss improved from 0.04079 to 0.03629, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 105s 839us/step - loss: 0.0388 - acc: 0.9856 - val_loss: 0.0363 - val_acc: 0.9862\n",
      "Epoch 3/6\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9865Epoch 00003: val_loss improved from 0.03629 to 0.03576, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 105s 838us/step - loss: 0.0358 - acc: 0.9865 - val_loss: 0.0358 - val_acc: 0.9859\n",
      "Epoch 4/6\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9869Epoch 00004: val_loss did not improve\n",
      "125206/125206 [==============================] - 105s 838us/step - loss: 0.0337 - acc: 0.9869 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "Epoch 5/6\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9877Epoch 00005: val_loss did not improve\n",
      "125206/125206 [==============================] - 105s 837us/step - loss: 0.0314 - acc: 0.9877 - val_loss: 0.0365 - val_acc: 0.9862\n",
      "Epoch 6/6\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9884Epoch 00006: val_loss did not improve\n",
      "125206/125206 [==============================] - 105s 840us/step - loss: 0.0293 - acc: 0.9884 - val_loss: 0.0373 - val_acc: 0.9863\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127629 127649 127654]\n",
      "(125205, 150) 12257\n",
      "(31914, 150)\n",
      "[   25    26    34 ..., 31884 31889 31897]\n",
      "(31306, 150) 3037\n",
      "Train on 125205 samples, validate on 31306 samples\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125184/125205 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9803Epoch 00001: val_loss improved from inf to 0.04053, saving model to weights_base.best.h5\n",
      "125205/125205 [==============================] - 104s 833us/step - loss: 0.0611 - acc: 0.9803 - val_loss: 0.0405 - val_acc: 0.9851\n",
      "Epoch 2/6\n",
      "125184/125205 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9853Epoch 00002: val_loss improved from 0.04053 to 0.03930, saving model to weights_base.best.h5\n",
      "125205/125205 [==============================] - 103s 822us/step - loss: 0.0393 - acc: 0.9853 - val_loss: 0.0393 - val_acc: 0.9851\n",
      "Epoch 3/6\n",
      "125184/125205 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9862Epoch 00003: val_loss improved from 0.03930 to 0.03598, saving model to weights_base.best.h5\n",
      "125205/125205 [==============================] - 103s 822us/step - loss: 0.0361 - acc: 0.9862 - val_loss: 0.0360 - val_acc: 0.9864\n",
      "Epoch 4/6\n",
      "125184/125205 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9869Epoch 00004: val_loss improved from 0.03598 to 0.03566, saving model to weights_base.best.h5\n",
      "125205/125205 [==============================] - 104s 834us/step - loss: 0.0336 - acc: 0.9869 - val_loss: 0.0357 - val_acc: 0.9866\n",
      "Epoch 5/6\n",
      "125184/125205 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9874Epoch 00005: val_loss did not improve\n",
      "125205/125205 [==============================] - 105s 836us/step - loss: 0.0322 - acc: 0.9874 - val_loss: 0.0358 - val_acc: 0.9866\n",
      "Epoch 6/6\n",
      "125184/125205 [============================>.] - ETA: 0s - loss: 0.0295 - acc: 0.9883Epoch 00006: val_loss did not improve\n",
      "125205/125205 [==============================] - 105s 835us/step - loss: 0.0295 - acc: 0.9883 - val_loss: 0.0368 - val_acc: 0.9860\n",
      "-------------------------------\n",
      "0 0.0905073830327 0.965833390779\n",
      "1 0.0219169597443 0.990668730534\n",
      "2 0.0461252664947 0.98210827782\n",
      "3 0.00896275124672 0.997280207557\n",
      "4 0.0597781160487 0.975296263105\n",
      "5 0.0202313809194 0.992736775479\n",
      "final 0.0412536429144 0.983987274212\n",
      "all eval None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def del_data_ratio(x,y,ratio=0.8):\n",
    "    print(x.shape)\n",
    "    pos_index = np.where(y[:,0]==1)[0]\n",
    "    neg_index = np.where(y[:,0]==0)[0]\n",
    "    print(pos_index)\n",
    "    data_cnt = len(pos_index)\n",
    "    add_cnt = int(data_cnt*ratio)\n",
    "    add_index = pos_index[:add_cnt]\n",
    "    add_x = np.concatenate([x[add_index],x[neg_index]])\n",
    "    add_y = np.concatenate([y[add_index],y[neg_index]])\n",
    "    print(add_x.shape,data_cnt)\n",
    "    add_x,add_y = shuffle(add_x,add_y,random_state=666)\n",
    "    return add_x,add_y\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # change pos ratio\n",
    "        new_curr_x,new_curr_y = del_data_ratio(curr_x,curr_y)\n",
    "        new_hold_x,new_hold_y = del_data_ratio(hold_out_x,hold_out_y)\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 256\n",
    "        epochs = 6\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(new_curr_x, new_curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(new_hold_x,new_hold_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6) (153164, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.994163</td>\n",
       "      <td>3.585157e-01</td>\n",
       "      <td>0.943492</td>\n",
       "      <td>0.113758</td>\n",
       "      <td>0.886089</td>\n",
       "      <td>0.353726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>1.994826e-07</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>8.709250e-07</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>3.256011e-07</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>4.849345e-06</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.994163  3.585157e-01  0.943492  0.113758  0.886089   \n",
       "1  0000247867823ef7  0.000202  1.994826e-07  0.000031  0.000002  0.000014   \n",
       "2  00013b17ad220c46  0.000315  8.709250e-07  0.000130  0.000007  0.000036   \n",
       "3  00017563c3f7919a  0.000297  3.256011e-07  0.000034  0.000005  0.000027   \n",
       "4  00017695ad8997eb  0.004468  4.849345e-06  0.000465  0.000035  0.000293   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.353726  \n",
       "1       0.000006  \n",
       "2       0.000012  \n",
       "3       0.000005  \n",
       "4       0.000040  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cudnn_gru_glove_1_csv_sample.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cudnn_gru_glove_1_sample_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "sample_submission.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.828469</td>\n",
       "      <td>2.987631e-01</td>\n",
       "      <td>0.786243</td>\n",
       "      <td>0.094798</td>\n",
       "      <td>0.738407</td>\n",
       "      <td>0.294772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>1.662355e-07</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>7.257708e-07</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>2.713343e-07</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>4.041121e-06</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.828469  2.987631e-01  0.786243  0.094798  0.738407   \n",
       "1  0000247867823ef7  0.000168  1.662355e-07  0.000026  0.000002  0.000012   \n",
       "2  00013b17ad220c46  0.000262  7.257708e-07  0.000109  0.000006  0.000030   \n",
       "3  00017563c3f7919a  0.000247  2.713343e-07  0.000028  0.000004  0.000023   \n",
       "4  00017695ad8997eb  0.003723  4.041121e-06  0.000388  0.000029  0.000244   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.294772  \n",
       "1       0.000005  \n",
       "2       0.000010  \n",
       "3       0.000004  \n",
       "4       0.000033  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred/1.2\n",
    "sample_submission.to_csv(\"../results/cudnn_gru_glove_1_csv_sample_div2.gz\", index=False, compression='gzip')\n",
    "sample_submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
