{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indications 12616\n",
      "9058989 120503\n",
      "272632936 138645\n",
      "hypno 45999\n",
      "smsmo 175998\n",
      "ratite 105916\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "# print(word_vec_dict['is'])\n",
    "# print(word_vec_dict['are'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.layers import Bidirectional, Dropout, CuDNNGRU\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "# https://github.com/PavelOstyakov/toxic/blob/master/toxic/model.py\n",
    "def get_model(comp=True):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=False))(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 256)          330240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               296448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 12,635,110\n",
      "Trainable params: 635,110\n",
      "Non-trainable params: 12,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_m = get_model(False)\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def del_data_ratio(x,y,ratio=0.8):\n",
    "    print(x.shape)\n",
    "    pos_index = np.where(y[:,0]==1)[0]\n",
    "    neg_index = np.where(y[:,0]==0)[0]\n",
    "    print(pos_index)\n",
    "    data_cnt = len(pos_index)\n",
    "    add_cnt = int(data_cnt*ratio)\n",
    "    add_index = pos_index[:add_cnt]\n",
    "    add_x = np.concatenate([x[add_index],x[neg_index]])\n",
    "    add_y = np.concatenate([y[add_index],y[neg_index]])\n",
    "    print(add_x.shape,data_cnt)\n",
    "    add_x,add_y = shuffle(add_x,add_y,random_state=666)\n",
    "    return add_x,add_y\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # change pos ratio\n",
    "        new_curr_x,new_curr_y = del_data_ratio(curr_x,curr_y)\n",
    "        new_hold_x,new_hold_y = del_data_ratio(hold_out_x,hold_out_y)\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 256\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(new_curr_x, new_curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(new_hold_x,new_hold_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106380, 150)\n",
      "[     0      6     11 ..., 106350 106355 106363]\n",
      "(104358, 150) 10110\n",
      "(53191, 150)\n",
      "[    6    12    16 ..., 53167 53181 53182]\n",
      "(52154, 150) 5184\n",
      "Train on 104358 samples, validate on 52154 samples\n",
      "Epoch 1/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9788Epoch 00001: val_loss improved from inf to 0.04070, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 93s 894us/step - loss: 0.0638 - acc: 0.9788 - val_loss: 0.0407 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9852Epoch 00002: val_loss improved from 0.04070 to 0.03843, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 93s 894us/step - loss: 0.0402 - acc: 0.9852 - val_loss: 0.0384 - val_acc: 0.9852\n",
      "Epoch 3/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9858Epoch 00003: val_loss improved from 0.03843 to 0.03655, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 94s 901us/step - loss: 0.0374 - acc: 0.9858 - val_loss: 0.0365 - val_acc: 0.9858\n",
      "Epoch 4/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9866Epoch 00004: val_loss did not improve\n",
      "104358/104358 [==============================] - 94s 905us/step - loss: 0.0349 - acc: 0.9866 - val_loss: 0.0384 - val_acc: 0.9858\n",
      "Epoch 5/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9873Epoch 00005: val_loss improved from 0.03655 to 0.03568, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 95s 910us/step - loss: 0.0329 - acc: 0.9873 - val_loss: 0.0357 - val_acc: 0.9862\n",
      "Epoch 6/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9879Epoch 00006: val_loss did not improve\n",
      "104358/104358 [==============================] - 95s 911us/step - loss: 0.0307 - acc: 0.9879 - val_loss: 0.0358 - val_acc: 0.9863\n",
      "Epoch 7/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0282 - acc: 0.9888Epoch 00007: val_loss did not improve\n",
      "104358/104358 [==============================] - 95s 907us/step - loss: 0.0282 - acc: 0.9888 - val_loss: 0.0372 - val_acc: 0.9859\n",
      "Epoch 8/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0257 - acc: 0.9896Epoch 00008: val_loss did not improve\n",
      "104358/104358 [==============================] - 94s 904us/step - loss: 0.0257 - acc: 0.9896 - val_loss: 0.0384 - val_acc: 0.9853\n",
      "(106381, 150)\n",
      "[     6     12     16 ..., 106351 106356 106364]\n",
      "(104330, 150) 10252\n",
      "(53190, 150)\n",
      "[    0     6    11 ..., 53156 53171 53182]\n",
      "(52181, 150) 5042\n",
      "Train on 104330 samples, validate on 52181 samples\n",
      "Epoch 1/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0678 - acc: 0.9794Epoch 00001: val_loss improved from inf to 0.04074, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 95s 914us/step - loss: 0.0677 - acc: 0.9795 - val_loss: 0.0407 - val_acc: 0.9851\n",
      "Epoch 2/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9853Epoch 00002: val_loss improved from 0.04074 to 0.03893, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 95s 911us/step - loss: 0.0401 - acc: 0.9853 - val_loss: 0.0389 - val_acc: 0.9850\n",
      "Epoch 3/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9862Epoch 00003: val_loss improved from 0.03893 to 0.03684, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 95s 909us/step - loss: 0.0367 - acc: 0.9863 - val_loss: 0.0368 - val_acc: 0.9860\n",
      "Epoch 4/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9868Epoch 00004: val_loss improved from 0.03684 to 0.03564, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 95s 907us/step - loss: 0.0346 - acc: 0.9868 - val_loss: 0.0356 - val_acc: 0.9867\n",
      "Epoch 5/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9875Epoch 00005: val_loss improved from 0.03564 to 0.03563, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 95s 907us/step - loss: 0.0323 - acc: 0.9875 - val_loss: 0.0356 - val_acc: 0.9863\n",
      "Epoch 6/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0298 - acc: 0.9883Epoch 00006: val_loss did not improve\n",
      "104330/104330 [==============================] - 95s 909us/step - loss: 0.0298 - acc: 0.9883 - val_loss: 0.0389 - val_acc: 0.9862\n",
      "Epoch 7/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9891Epoch 00007: val_loss did not improve\n",
      "104330/104330 [==============================] - 95s 910us/step - loss: 0.0273 - acc: 0.9891 - val_loss: 0.0379 - val_acc: 0.9859\n",
      "Epoch 8/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9900Epoch 00008: val_loss did not improve\n",
      "104330/104330 [==============================] - 95s 909us/step - loss: 0.0248 - acc: 0.9900 - val_loss: 0.0395 - val_acc: 0.9854\n",
      "(106381, 150)\n",
      "[     6     12     16 ..., 106347 106362 106373]\n",
      "(104335, 150) 10226\n",
      "(53190, 150)\n",
      "[   15    21    40 ..., 53160 53165 53173]\n",
      "(52176, 150) 5068\n",
      "Train on 104335 samples, validate on 52176 samples\n",
      "Epoch 1/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9779Epoch 00001: val_loss improved from inf to 0.04385, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 96s 916us/step - loss: 0.0698 - acc: 0.9779 - val_loss: 0.0438 - val_acc: 0.9842\n",
      "Epoch 2/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9851Epoch 00002: val_loss improved from 0.04385 to 0.03836, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 95s 906us/step - loss: 0.0402 - acc: 0.9850 - val_loss: 0.0384 - val_acc: 0.9857\n",
      "Epoch 3/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9861Epoch 00003: val_loss improved from 0.03836 to 0.03689, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 94s 905us/step - loss: 0.0367 - acc: 0.9861 - val_loss: 0.0369 - val_acc: 0.9860\n",
      "Epoch 4/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9867Epoch 00004: val_loss improved from 0.03689 to 0.03667, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 94s 906us/step - loss: 0.0347 - acc: 0.9867 - val_loss: 0.0367 - val_acc: 0.9863\n",
      "Epoch 5/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9873Epoch 00005: val_loss did not improve\n",
      "104335/104335 [==============================] - 95s 908us/step - loss: 0.0324 - acc: 0.9873 - val_loss: 0.0373 - val_acc: 0.9861\n",
      "Epoch 6/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9880Epoch 00006: val_loss did not improve\n",
      "104335/104335 [==============================] - 95s 908us/step - loss: 0.0301 - acc: 0.9880 - val_loss: 0.0383 - val_acc: 0.9863\n",
      "Epoch 7/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9890Epoch 00007: val_loss did not improve\n",
      "104335/104335 [==============================] - 95s 908us/step - loss: 0.0275 - acc: 0.9890 - val_loss: 0.0392 - val_acc: 0.9861\n",
      "Epoch 8/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9900Epoch 00008: val_loss did not improve\n",
      "104335/104335 [==============================] - 95s 907us/step - loss: 0.0249 - acc: 0.9900 - val_loss: 0.0398 - val_acc: 0.9860\n",
      "-------------------------------\n",
      "0 0.0915107369822 0.965827123976\n",
      "1 0.0222986968416 0.990098451473\n",
      "2 0.0466807070363 0.981995475368\n",
      "3 0.00999819986863 0.997067136259\n",
      "4 0.0606844434025 0.974982922962\n",
      "5 0.0205892022559 0.992968647185\n",
      "final 0.0419603310645 0.983823292871\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "(119678, 150)\n",
      "[    32     33     50 ..., 119648 119653 119661]\n",
      "(117406, 150) 11359\n",
      "(39893, 150)\n",
      "[    6    12    16 ..., 39870 39882 39886]\n",
      "(39106, 150) 3935\n",
      "Train on 117406 samples, validate on 39106 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9798Epoch 00001: val_loss improved from inf to 0.04157, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 100s 851us/step - loss: 0.0645 - acc: 0.9798 - val_loss: 0.0416 - val_acc: 0.9845\n",
      "Epoch 2/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9853Epoch 00002: val_loss improved from 0.04157 to 0.03735, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 99s 845us/step - loss: 0.0402 - acc: 0.9853 - val_loss: 0.0373 - val_acc: 0.9856\n",
      "Epoch 3/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9863Epoch 00003: val_loss improved from 0.03735 to 0.03637, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 99s 846us/step - loss: 0.0368 - acc: 0.9863 - val_loss: 0.0364 - val_acc: 0.9858\n",
      "Epoch 4/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9869Epoch 00004: val_loss improved from 0.03637 to 0.03631, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 99s 846us/step - loss: 0.0345 - acc: 0.9869 - val_loss: 0.0363 - val_acc: 0.9860\n",
      "Epoch 5/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9876Epoch 00005: val_loss did not improve\n",
      "117406/117406 [==============================] - 100s 851us/step - loss: 0.0323 - acc: 0.9876 - val_loss: 0.0370 - val_acc: 0.9859\n",
      "Epoch 6/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9882Epoch 00006: val_loss improved from 0.03631 to 0.03629, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 100s 848us/step - loss: 0.0302 - acc: 0.9882 - val_loss: 0.0363 - val_acc: 0.9860\n",
      "Epoch 7/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0274 - acc: 0.9891Epoch 00007: val_loss did not improve\n",
      "117406/117406 [==============================] - 100s 849us/step - loss: 0.0274 - acc: 0.9892 - val_loss: 0.0375 - val_acc: 0.9857\n",
      "Epoch 8/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9900Epoch 00008: val_loss did not improve\n",
      "117406/117406 [==============================] - 101s 859us/step - loss: 0.0249 - acc: 0.9900 - val_loss: 0.0390 - val_acc: 0.9855\n",
      "(119678, 150)\n",
      "[     6     12     16 ..., 119648 119653 119661]\n",
      "(117381, 150) 11482\n",
      "(39893, 150)\n",
      "[   32    33    50 ..., 39877 39881 39888]\n",
      "(39130, 150) 3812\n",
      "Train on 117381 samples, validate on 39130 samples\n",
      "Epoch 1/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0672 - acc: 0.9779Epoch 00001: val_loss improved from inf to 0.04113, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 103s 875us/step - loss: 0.0671 - acc: 0.9780 - val_loss: 0.0411 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9851Epoch 00002: val_loss improved from 0.04113 to 0.03912, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 101s 862us/step - loss: 0.0404 - acc: 0.9851 - val_loss: 0.0391 - val_acc: 0.9855\n",
      "Epoch 3/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9861Epoch 00003: val_loss improved from 0.03912 to 0.03673, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 101s 863us/step - loss: 0.0371 - acc: 0.9861 - val_loss: 0.0367 - val_acc: 0.9862\n",
      "Epoch 4/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9867Epoch 00004: val_loss did not improve\n",
      "117381/117381 [==============================] - 101s 863us/step - loss: 0.0344 - acc: 0.9867 - val_loss: 0.0380 - val_acc: 0.9853\n",
      "Epoch 5/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9873Epoch 00005: val_loss improved from 0.03673 to 0.03631, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 102s 869us/step - loss: 0.0321 - acc: 0.9873 - val_loss: 0.0363 - val_acc: 0.9865\n",
      "Epoch 6/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9881Epoch 00006: val_loss did not improve\n",
      "117381/117381 [==============================] - 101s 864us/step - loss: 0.0300 - acc: 0.9881 - val_loss: 0.0371 - val_acc: 0.9865\n",
      "Epoch 7/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9890Epoch 00007: val_loss did not improve\n",
      "117381/117381 [==============================] - 101s 862us/step - loss: 0.0272 - acc: 0.9890 - val_loss: 0.0405 - val_acc: 0.9863\n",
      "Epoch 8/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0250 - acc: 0.9900Epoch 00008: val_loss did not improve\n",
      "117381/117381 [==============================] - 101s 860us/step - loss: 0.0250 - acc: 0.9900 - val_loss: 0.0396 - val_acc: 0.9859\n",
      "(119678, 150)\n",
      "[     6     12     16 ..., 119648 119653 119661]\n",
      "(117363, 150) 11575\n",
      "(39893, 150)\n",
      "[    5    20    41 ..., 39850 39874 39883]\n",
      "(39149, 150) 3719\n",
      "Train on 117363 samples, validate on 39149 samples\n",
      "Epoch 1/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.03941, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 103s 876us/step - loss: 0.0663 - acc: 0.9787 - val_loss: 0.0394 - val_acc: 0.9854\n",
      "Epoch 2/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9851Epoch 00002: val_loss improved from 0.03941 to 0.03611, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 101s 861us/step - loss: 0.0401 - acc: 0.9851 - val_loss: 0.0361 - val_acc: 0.9864\n",
      "Epoch 3/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9859Epoch 00003: val_loss improved from 0.03611 to 0.03544, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 101s 862us/step - loss: 0.0370 - acc: 0.9859 - val_loss: 0.0354 - val_acc: 0.9865\n",
      "Epoch 4/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9868Epoch 00004: val_loss improved from 0.03544 to 0.03517, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 101s 863us/step - loss: 0.0344 - acc: 0.9868 - val_loss: 0.0352 - val_acc: 0.9863\n",
      "Epoch 5/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9874Epoch 00005: val_loss did not improve\n",
      "117363/117363 [==============================] - 101s 862us/step - loss: 0.0324 - acc: 0.9874 - val_loss: 0.0353 - val_acc: 0.9861\n",
      "Epoch 6/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9882Epoch 00006: val_loss did not improve\n",
      "117363/117363 [==============================] - 101s 862us/step - loss: 0.0299 - acc: 0.9882 - val_loss: 0.0368 - val_acc: 0.9862\n",
      "Epoch 7/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9891Epoch 00007: val_loss did not improve\n",
      "117363/117363 [==============================] - 101s 861us/step - loss: 0.0272 - acc: 0.9891 - val_loss: 0.0391 - val_acc: 0.9866\n",
      "Epoch 8/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9900Epoch 00008: val_loss did not improve\n",
      "117363/117363 [==============================] - 101s 861us/step - loss: 0.0246 - acc: 0.9901 - val_loss: 0.0380 - val_acc: 0.9859\n",
      "(119679, 150)\n",
      "[     6     12     16 ..., 119636 119660 119669]\n",
      "(117385, 150) 11466\n",
      "(39892, 150)\n",
      "[    5    31    35 ..., 39862 39867 39875]\n",
      "(39126, 150) 3828\n",
      "Train on 117385 samples, validate on 39126 samples\n",
      "Epoch 1/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9793Epoch 00001: val_loss improved from inf to 0.04028, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 103s 879us/step - loss: 0.0625 - acc: 0.9793 - val_loss: 0.0403 - val_acc: 0.9851\n",
      "Epoch 2/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9855Epoch 00002: val_loss improved from 0.04028 to 0.03892, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 102s 867us/step - loss: 0.0391 - acc: 0.9855 - val_loss: 0.0389 - val_acc: 0.9855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9864Epoch 00003: val_loss improved from 0.03892 to 0.03663, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 100s 851us/step - loss: 0.0354 - acc: 0.9864 - val_loss: 0.0366 - val_acc: 0.9862\n",
      "Epoch 4/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9871Epoch 00004: val_loss did not improve\n",
      "117385/117385 [==============================] - 99s 845us/step - loss: 0.0332 - acc: 0.9871 - val_loss: 0.0368 - val_acc: 0.9864\n",
      "Epoch 5/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0308 - acc: 0.9879Epoch 00005: val_loss did not improve\n",
      "117385/117385 [==============================] - 99s 845us/step - loss: 0.0308 - acc: 0.9879 - val_loss: 0.0368 - val_acc: 0.9865\n",
      "Epoch 6/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9886Epoch 00006: val_loss did not improve\n",
      "117385/117385 [==============================] - 99s 846us/step - loss: 0.0287 - acc: 0.9886 - val_loss: 0.0385 - val_acc: 0.9863\n",
      "Epoch 7/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9895Epoch 00007: val_loss did not improve\n",
      "117385/117385 [==============================] - 99s 846us/step - loss: 0.0263 - acc: 0.9895 - val_loss: 0.0390 - val_acc: 0.9860\n",
      "Epoch 8/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9906Epoch 00008: val_loss did not improve\n",
      "117385/117385 [==============================] - 99s 847us/step - loss: 0.0237 - acc: 0.9906 - val_loss: 0.0417 - val_acc: 0.9856\n",
      "-------------------------------\n",
      "0 0.0912206057271 0.965914859216\n",
      "1 0.0221931961327 0.990549661279\n",
      "2 0.0468433478495 0.981914006931\n",
      "3 0.00911978713362 0.997154871499\n",
      "4 0.0605503388129 0.975120792625\n",
      "5 0.020482230613 0.992830777522\n",
      "final 0.0417349177115 0.983914161512\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "(127656, 150)\n",
      "[    10     13     23 ..., 127626 127631 127639]\n",
      "(125221, 150) 12171\n",
      "(31915, 150)\n",
      "[    6    12    16 ..., 31893 31901 31906]\n",
      "(31290, 150) 3123\n",
      "Train on 125221 samples, validate on 31290 samples\n",
      "Epoch 1/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0620 - acc: 0.9801Epoch 00001: val_loss improved from inf to 0.04077, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 106s 843us/step - loss: 0.0620 - acc: 0.9801 - val_loss: 0.0408 - val_acc: 0.9846\n",
      "Epoch 2/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9857Epoch 00002: val_loss improved from 0.04077 to 0.03687, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 103s 825us/step - loss: 0.0390 - acc: 0.9857 - val_loss: 0.0369 - val_acc: 0.9858\n",
      "Epoch 3/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9864Epoch 00003: val_loss improved from 0.03687 to 0.03669, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 103s 823us/step - loss: 0.0360 - acc: 0.9864 - val_loss: 0.0367 - val_acc: 0.9859\n",
      "Epoch 4/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9871Epoch 00004: val_loss improved from 0.03669 to 0.03657, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 103s 822us/step - loss: 0.0337 - acc: 0.9871 - val_loss: 0.0366 - val_acc: 0.9860\n",
      "Epoch 5/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9877Epoch 00005: val_loss improved from 0.03657 to 0.03568, saving model to weights_base.best.h5\n",
      "125221/125221 [==============================] - 103s 820us/step - loss: 0.0317 - acc: 0.9877 - val_loss: 0.0357 - val_acc: 0.9862\n",
      "Epoch 6/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9884Epoch 00006: val_loss did not improve\n",
      "125221/125221 [==============================] - 103s 820us/step - loss: 0.0294 - acc: 0.9884 - val_loss: 0.0372 - val_acc: 0.9861\n",
      "Epoch 7/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9892Epoch 00007: val_loss did not improve\n",
      "125221/125221 [==============================] - 103s 820us/step - loss: 0.0270 - acc: 0.9892 - val_loss: 0.0382 - val_acc: 0.9859\n",
      "Epoch 8/8\n",
      "125184/125221 [============================>.] - ETA: 0s - loss: 0.0246 - acc: 0.9901Epoch 00008: val_loss did not improve\n",
      "125221/125221 [==============================] - 105s 838us/step - loss: 0.0246 - acc: 0.9901 - val_loss: 0.0399 - val_acc: 0.9858\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127627 127632 127640]\n",
      "(125211, 150) 12229\n",
      "(31914, 150)\n",
      "[   10    13    23 ..., 31894 31907 31908]\n",
      "(31301, 150) 3065\n",
      "Train on 125211 samples, validate on 31301 samples\n",
      "Epoch 1/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9796Epoch 00001: val_loss improved from inf to 0.03905, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 108s 859us/step - loss: 0.0635 - acc: 0.9796 - val_loss: 0.0391 - val_acc: 0.9855\n",
      "Epoch 2/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9855Epoch 00002: val_loss improved from 0.03905 to 0.03696, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 105s 840us/step - loss: 0.0395 - acc: 0.9855 - val_loss: 0.0370 - val_acc: 0.9862\n",
      "Epoch 3/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9863Epoch 00003: val_loss improved from 0.03696 to 0.03564, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 105s 839us/step - loss: 0.0364 - acc: 0.9863 - val_loss: 0.0356 - val_acc: 0.9864\n",
      "Epoch 4/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9868Epoch 00004: val_loss did not improve\n",
      "125211/125211 [==============================] - 105s 841us/step - loss: 0.0341 - acc: 0.9868 - val_loss: 0.0361 - val_acc: 0.9864\n",
      "Epoch 5/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9876Epoch 00005: val_loss improved from 0.03564 to 0.03508, saving model to weights_base.best.h5\n",
      "125211/125211 [==============================] - 105s 840us/step - loss: 0.0319 - acc: 0.9876 - val_loss: 0.0351 - val_acc: 0.9867\n",
      "Epoch 6/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9883Epoch 00006: val_loss did not improve\n",
      "125211/125211 [==============================] - 105s 840us/step - loss: 0.0297 - acc: 0.9883 - val_loss: 0.0356 - val_acc: 0.9866\n",
      "Epoch 7/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0271 - acc: 0.9891Epoch 00007: val_loss did not improve\n",
      "125211/125211 [==============================] - 105s 841us/step - loss: 0.0271 - acc: 0.9891 - val_loss: 0.0365 - val_acc: 0.9866\n",
      "Epoch 8/8\n",
      "125184/125211 [============================>.] - ETA: 0s - loss: 0.0244 - acc: 0.9902Epoch 00008: val_loss did not improve\n",
      "125211/125211 [==============================] - 105s 837us/step - loss: 0.0244 - acc: 0.9902 - val_loss: 0.0388 - val_acc: 0.9861\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127627 127632 127640]\n",
      "(125203, 150) 12267\n",
      "(31914, 150)\n",
      "[    7    17    26 ..., 31903 31904 31912]\n",
      "(31308, 150) 3027\n",
      "Train on 125203 samples, validate on 31308 samples\n",
      "Epoch 1/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9779Epoch 00001: val_loss improved from inf to 0.04150, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 108s 862us/step - loss: 0.0657 - acc: 0.9779 - val_loss: 0.0415 - val_acc: 0.9848\n",
      "Epoch 2/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9855Epoch 00002: val_loss improved from 0.04150 to 0.03671, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 105s 840us/step - loss: 0.0391 - acc: 0.9855 - val_loss: 0.0367 - val_acc: 0.9863\n",
      "Epoch 3/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9863Epoch 00003: val_loss did not improve\n",
      "125203/125203 [==============================] - 105s 836us/step - loss: 0.0359 - acc: 0.9863 - val_loss: 0.0377 - val_acc: 0.9859\n",
      "Epoch 4/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9867Epoch 00004: val_loss improved from 0.03671 to 0.03554, saving model to weights_base.best.h5\n",
      "125203/125203 [==============================] - 103s 821us/step - loss: 0.0342 - acc: 0.9867 - val_loss: 0.0355 - val_acc: 0.9864\n",
      "Epoch 5/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9873Epoch 00005: val_loss did not improve\n",
      "125203/125203 [==============================] - 103s 820us/step - loss: 0.0320 - acc: 0.9873 - val_loss: 0.0366 - val_acc: 0.9864\n",
      "Epoch 6/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.9881Epoch 00006: val_loss did not improve\n",
      "125203/125203 [==============================] - 102s 819us/step - loss: 0.0297 - acc: 0.9881 - val_loss: 0.0364 - val_acc: 0.9863\n",
      "Epoch 7/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0275 - acc: 0.9888Epoch 00007: val_loss did not improve\n",
      "125203/125203 [==============================] - 103s 821us/step - loss: 0.0275 - acc: 0.9888 - val_loss: 0.0394 - val_acc: 0.9866\n",
      "Epoch 8/8\n",
      "125184/125203 [============================>.] - ETA: 0s - loss: 0.0248 - acc: 0.9899Epoch 00008: val_loss did not improve\n",
      "125203/125203 [==============================] - 103s 821us/step - loss: 0.0248 - acc: 0.9899 - val_loss: 0.0387 - val_acc: 0.9864\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127627 127632 127640]\n",
      "(125206, 150) 12252\n",
      "(31914, 150)\n",
      "[   10    23    58 ..., 31886 31906 31911]\n",
      "(31305, 150) 3042\n",
      "Train on 125206 samples, validate on 31305 samples\n",
      "Epoch 1/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9796Epoch 00001: val_loss improved from inf to 0.04060, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 106s 847us/step - loss: 0.0647 - acc: 0.9796 - val_loss: 0.0406 - val_acc: 0.9854\n",
      "Epoch 2/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9853Epoch 00002: val_loss improved from 0.04060 to 0.03717, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 103s 820us/step - loss: 0.0403 - acc: 0.9853 - val_loss: 0.0372 - val_acc: 0.9860\n",
      "Epoch 3/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9861Epoch 00003: val_loss improved from 0.03717 to 0.03651, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 103s 820us/step - loss: 0.0368 - acc: 0.9861 - val_loss: 0.0365 - val_acc: 0.9863\n",
      "Epoch 4/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9868Epoch 00004: val_loss improved from 0.03651 to 0.03573, saving model to weights_base.best.h5\n",
      "125206/125206 [==============================] - 103s 824us/step - loss: 0.0345 - acc: 0.9868 - val_loss: 0.0357 - val_acc: 0.9863\n",
      "Epoch 5/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9874Epoch 00005: val_loss did not improve\n",
      "125206/125206 [==============================] - 103s 821us/step - loss: 0.0323 - acc: 0.9874 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "Epoch 6/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9882Epoch 00006: val_loss did not improve\n",
      "125206/125206 [==============================] - 103s 823us/step - loss: 0.0302 - acc: 0.9882 - val_loss: 0.0366 - val_acc: 0.9864\n",
      "Epoch 7/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9891Epoch 00007: val_loss did not improve\n",
      "125206/125206 [==============================] - 103s 821us/step - loss: 0.0276 - acc: 0.9891 - val_loss: 0.0378 - val_acc: 0.9861\n",
      "Epoch 8/8\n",
      "125184/125206 [============================>.] - ETA: 0s - loss: 0.0252 - acc: 0.9899Epoch 00008: val_loss did not improve\n",
      "125206/125206 [==============================] - 103s 822us/step - loss: 0.0252 - acc: 0.9899 - val_loss: 0.0398 - val_acc: 0.9862\n",
      "(127657, 150)\n",
      "[     6     12     16 ..., 127629 127649 127654]\n",
      "(125205, 150) 12257\n",
      "(31914, 150)\n",
      "[   25    26    34 ..., 31884 31889 31897]\n",
      "(31306, 150) 3037\n",
      "Train on 125205 samples, validate on 31306 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[150,256,128]\n\t [[Node: training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@bidirectional_28/strided_slice_1\"], begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/Shape, bidirectional_28/strided_slice_1/stack, bidirectional_28/strided_slice_1/stack_1, bidirectional_28/strided_slice_1/stack_2, training_23/Adam/gradients/bidirectional_28/concat_2_grad/Slice_1)]]\n\nCaused by op 'training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-6412b2e880c3>\", line 5, in <module>\n    train_pred,test_pred = kf_train(fold_cnt=f)\n  File \"<ipython-input-11-1a59c019f577>\", line 42, in kf_train\n    callbacks=callbacks_list)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1634, in fit\n    self._make_train_function()\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py\", line 243, in _StridedSliceGrad\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3651, in strided_slice_grad\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'bidirectional_28/strided_slice_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-12-6412b2e880c3>\", line 5, in <module>\n    train_pred,test_pred = kf_train(fold_cnt=f)\n  File \"<ipython-input-11-1a59c019f577>\", line 31, in kf_train\n    model = get_model()\n  File \"<ipython-input-7-48e3d8f9f1cf>\", line 21, in get_model\n    x = Bidirectional(CuDNNGRU(128, return_sequences=False))(x)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/layers/wrappers.py\", line 291, in call\n    y_rev = self.backward_layer.call(inputs, **kwargs)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/layers/cudnn_recurrent.py\", line 84, in call\n    output, states = self._process_batch(inputs, initial_state)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/layers/cudnn_recurrent.py\", line 298, in _process_batch\n    output = outputs[-1]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 497, in _SliceHelper\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 655, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3568, in strided_slice\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[150,256,128]\n\t [[Node: training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@bidirectional_28/strided_slice_1\"], begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/Shape, bidirectional_28/strided_slice_1/stack, bidirectional_28/strided_slice_1/stack_1, bidirectional_28/strided_slice_1/stack_2, training_23/Adam/gradients/bidirectional_28/concat_2_grad/Slice_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[150,256,128]\n\t [[Node: training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@bidirectional_28/strided_slice_1\"], begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/Shape, bidirectional_28/strided_slice_1/stack, bidirectional_28/strided_slice_1/stack_1, bidirectional_28/strided_slice_1/stack_2, training_23/Adam/gradients/bidirectional_28/concat_2_grad/Slice_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6412b2e880c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkf_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfold_cnt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0msample_submission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist_classes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-1a59c019f577>\u001b[0m in \u001b[0;36mkf_train\u001b[0;34m(fold_cnt, rnd)\u001b[0m\n\u001b[1;32m     40\u001b[0m                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                   \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_hold_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnew_hold_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                   callbacks=callbacks_list)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[150,256,128]\n\t [[Node: training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@bidirectional_28/strided_slice_1\"], begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/Shape, bidirectional_28/strided_slice_1/stack, bidirectional_28/strided_slice_1/stack_1, bidirectional_28/strided_slice_1/stack_2, training_23/Adam/gradients/bidirectional_28/concat_2_grad/Slice_1)]]\n\nCaused by op 'training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 281, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 232, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 397, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-6412b2e880c3>\", line 5, in <module>\n    train_pred,test_pred = kf_train(fold_cnt=f)\n  File \"<ipython-input-11-1a59c019f577>\", line 42, in kf_train\n    callbacks=callbacks_list)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 1634, in fit\n    self._make_train_function()\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/engine/training.py\", line 990, in _make_train_function\n    loss=self.total_loss)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/legacy/interfaces.py\", line 87, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 415, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/optimizers.py\", line 73, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\", line 2394, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 368, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 560, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_grad.py\", line 243, in _StridedSliceGrad\n    shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3651, in strided_slice_grad\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op 'bidirectional_28/strided_slice_1', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 19 identical lines from previous traceback]\n  File \"<ipython-input-12-6412b2e880c3>\", line 5, in <module>\n    train_pred,test_pred = kf_train(fold_cnt=f)\n  File \"<ipython-input-11-1a59c019f577>\", line 31, in kf_train\n    model = get_model()\n  File \"<ipython-input-7-48e3d8f9f1cf>\", line 21, in get_model\n    x = Bidirectional(CuDNNGRU(128, return_sequences=False))(x)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/layers/wrappers.py\", line 291, in call\n    y_rev = self.backward_layer.call(inputs, **kwargs)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/layers/cudnn_recurrent.py\", line 84, in call\n    output, states = self._process_batch(inputs, initial_state)\n  File \"/home/jac/.local/lib/python3.5/site-packages/keras/layers/cudnn_recurrent.py\", line 298, in _process_batch\n    output = outputs[-1]\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 497, in _SliceHelper\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/array_ops.py\", line 655, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_array_ops.py\", line 3568, in strided_slice\n    shrink_axis_mask=shrink_axis_mask, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[150,256,128]\n\t [[Node: training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/StridedSliceGrad = StridedSliceGrad[Index=DT_INT32, T=DT_FLOAT, _class=[\"loc:@bidirectional_28/strided_slice_1\"], begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](training_23/Adam/gradients/bidirectional_28/strided_slice_1_grad/Shape, bidirectional_28/strided_slice_1/stack, bidirectional_28/strided_slice_1/stack_1, bidirectional_28/strided_slice_1/stack_2, training_23/Adam/gradients/bidirectional_28/concat_2_grad/Slice_1)]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "for f in [3,4]:\n",
    "    train_pred,test_pred = kf_train(fold_cnt=f)\n",
    "    print(train_pred.shape,test_pred.shape)    \n",
    "    sample_submission[list_classes] = test_pred\n",
    "    sample_submission.to_csv(\"../results/cudnn_gru_glove_1_sample_{}.gz\".format(f), index=False, compression='gzip')\n",
    "    with open('../features/cudnn_gru_glove_1_sample_feat_{}.pkl'.format(f),'wb') as fout:\n",
    "        pickle.dump([train_pred,test_pred],fout)\n",
    "    sample_submission.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
