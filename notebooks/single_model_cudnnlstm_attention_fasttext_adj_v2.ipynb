{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "wale 40497\n",
      "hw 20236\n",
      "12c 181409\n",
      "outs 13060\n",
      "fralembert 168203\n",
      "conncerns 93376\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 79399\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "# adj dropout and lstm units, opt use nadam\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool])\n",
    "    x = Dense(256, activation=\"relu\")(conc)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='nadam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval')\n",
    "    eval_val(y,train_pred)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9812Epoch 00001: val_loss improved from inf to 0.04110, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0507 - acc: 0.9812 - val_loss: 0.0411 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9836Epoch 00002: val_loss improved from 0.04110 to 0.03998, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 935us/step - loss: 0.0419 - acc: 0.9836 - val_loss: 0.0400 - val_acc: 0.9840\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9845Epoch 00003: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 941us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0413 - val_acc: 0.9835\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0403 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9855Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0416 - val_acc: 0.9833\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9860Epoch 00006: val_loss improved from 0.03998 to 0.03987, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 114s 951us/step - loss: 0.0348 - acc: 0.9860 - val_loss: 0.0399 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0334 - acc: 0.9865 - val_loss: 0.0418 - val_acc: 0.9830\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9870Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0321 - acc: 0.9870 - val_loss: 0.0423 - val_acc: 0.9832\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9871Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 941us/step - loss: 0.0314 - acc: 0.9871 - val_loss: 0.0414 - val_acc: 0.9836\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0301 - acc: 0.9877Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0301 - acc: 0.9877 - val_loss: 0.0421 - val_acc: 0.9835\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0500 - acc: 0.9811Epoch 00001: val_loss improved from inf to 0.04106, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 114s 951us/step - loss: 0.0500 - acc: 0.9811 - val_loss: 0.0411 - val_acc: 0.9837\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9836Epoch 00002: val_loss improved from 0.04106 to 0.04029, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 114s 950us/step - loss: 0.0416 - acc: 0.9836 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9843Epoch 00003: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0395 - acc: 0.9843 - val_loss: 0.0421 - val_acc: 0.9835\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9848Epoch 00004: val_loss improved from 0.04029 to 0.03988, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 114s 949us/step - loss: 0.0376 - acc: 0.9848 - val_loss: 0.0399 - val_acc: 0.9845\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9854Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 942us/step - loss: 0.0361 - acc: 0.9854 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9859Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 112s 938us/step - loss: 0.0347 - acc: 0.9859 - val_loss: 0.0417 - val_acc: 0.9835\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 944us/step - loss: 0.0333 - acc: 0.9864 - val_loss: 0.0436 - val_acc: 0.9828\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 112s 939us/step - loss: 0.0322 - acc: 0.9866 - val_loss: 0.0438 - val_acc: 0.9826\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9873Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 112s 940us/step - loss: 0.0312 - acc: 0.9872 - val_loss: 0.0452 - val_acc: 0.9824\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9874Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 943us/step - loss: 0.0302 - acc: 0.9874 - val_loss: 0.0440 - val_acc: 0.9834\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0518 - acc: 0.9809Epoch 00001: val_loss improved from inf to 0.04302, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 114s 951us/step - loss: 0.0518 - acc: 0.9809 - val_loss: 0.0430 - val_acc: 0.9827\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9833Epoch 00002: val_loss improved from 0.04302 to 0.03935, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 113s 946us/step - loss: 0.0425 - acc: 0.9833 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9843Epoch 00003: val_loss improved from 0.03935 to 0.03860, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 113s 947us/step - loss: 0.0399 - acc: 0.9843 - val_loss: 0.0386 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9848Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 942us/step - loss: 0.0381 - acc: 0.9848 - val_loss: 0.0413 - val_acc: 0.9832\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9854Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 941us/step - loss: 0.0365 - acc: 0.9854 - val_loss: 0.0405 - val_acc: 0.9834\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9858Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 943us/step - loss: 0.0351 - acc: 0.9858 - val_loss: 0.0400 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 943us/step - loss: 0.0339 - acc: 0.9864 - val_loss: 0.0449 - val_acc: 0.9811\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 942us/step - loss: 0.0327 - acc: 0.9866 - val_loss: 0.0407 - val_acc: 0.9834\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 113s 941us/step - loss: 0.0311 - acc: 0.9872 - val_loss: 0.0410 - val_acc: 0.9835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9876Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 109s 908us/step - loss: 0.0304 - acc: 0.9876 - val_loss: 0.0444 - val_acc: 0.9819\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9812Epoch 00001: val_loss improved from inf to 0.04197, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 110s 918us/step - loss: 0.0499 - acc: 0.9812 - val_loss: 0.0420 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9837Epoch 00002: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 908us/step - loss: 0.0415 - acc: 0.9837 - val_loss: 0.0423 - val_acc: 0.9835\n",
      "Epoch 3/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9843Epoch 00003: val_loss improved from 0.04197 to 0.04107, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 109s 913us/step - loss: 0.0392 - acc: 0.9843 - val_loss: 0.0411 - val_acc: 0.9839\n",
      "Epoch 4/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9851Epoch 00004: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 907us/step - loss: 0.0373 - acc: 0.9851 - val_loss: 0.0435 - val_acc: 0.9823\n",
      "Epoch 5/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 907us/step - loss: 0.0357 - acc: 0.9857 - val_loss: 0.0419 - val_acc: 0.9834\n",
      "Epoch 6/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9860Epoch 00006: val_loss improved from 0.04107 to 0.04070, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 109s 912us/step - loss: 0.0344 - acc: 0.9860 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9865Epoch 00007: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 907us/step - loss: 0.0329 - acc: 0.9865 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "Epoch 8/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9869Epoch 00008: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 908us/step - loss: 0.0321 - acc: 0.9869 - val_loss: 0.0435 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9873Epoch 00009: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 909us/step - loss: 0.0310 - acc: 0.9873 - val_loss: 0.0423 - val_acc: 0.9834\n",
      "Epoch 10/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9877Epoch 00010: val_loss did not improve\n",
      "119679/119679 [==============================] - 109s 908us/step - loss: 0.0299 - acc: 0.9877 - val_loss: 0.0447 - val_acc: 0.9824\n",
      "-------------------------------\n",
      "all eval\n",
      "0 0.0857521960011 0.966516472291\n",
      "1 0.0217033095451 0.990505793659\n",
      "2 0.045240303185 0.982089477411\n",
      "3 0.00799098420974 0.997242606739\n",
      "4 0.0583625763911 0.975672271277\n",
      "5 0.0195297795702 0.992849577931\n",
      "final 0.0397631914837 0.984146033218\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "# adj_v1\n",
    "# 1st epo , 3928, 4 fold: final 0.0393455938053 0.984445795289\n",
    "# 10 fold: final 0.0391567844913 0.984588887287 PUB 9857\n",
    "\n",
    "# adj_v2\n",
    "# 4 fold: final 0.0397631914837 0.984146033218\n",
    "\n",
    "# adj_v2 5 fold\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
      "0  00001cee341fdb12  0.997666  4.752881e-01  0.968059  1.423738e-01  0.929372   \n",
      "1  0000247867823ef7  0.000070  8.890427e-08  0.000120  1.325372e-07  0.000027   \n",
      "2  00013b17ad220c46  0.000497  1.295319e-06  0.000393  1.221707e-06  0.000080   \n",
      "3  00017563c3f7919a  0.000072  1.793499e-07  0.000049  3.736210e-06  0.000032   \n",
      "4  00017695ad8997eb  0.002672  2.239775e-06  0.000707  7.760169e-06  0.000178   \n",
      "\n",
      "   identity_hate  \n",
      "0   6.720545e-01  \n",
      "1   8.526365e-07  \n",
      "2   9.349270e-06  \n",
      "3   1.557055e-06  \n",
      "4   1.377786e-05  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_adj2_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_adj2_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
