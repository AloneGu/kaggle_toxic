{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "steptoe 60842\n",
      "sheriffs 21068\n",
      "nascar1996 178915\n",
      "resertch 111681\n",
      "wringing 42078\n",
      "boyo 61353\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 79399\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "# adj dropout and lstm units\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool])\n",
    "    x = Dense(256, activation=\"relu\")(conc)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval')\n",
    "    eval_val(y,train_pred)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9805Epoch 00001: val_loss improved from inf to 0.04257, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 108s 905us/step - loss: 0.0544 - acc: 0.9805 - val_loss: 0.0426 - val_acc: 0.9828\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9833Epoch 00002: val_loss improved from 0.04257 to 0.04198, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 107s 896us/step - loss: 0.0430 - acc: 0.9833 - val_loss: 0.0420 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9840Epoch 00003: val_loss improved from 0.04198 to 0.03970, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 107s 898us/step - loss: 0.0406 - acc: 0.9840 - val_loss: 0.0397 - val_acc: 0.9840\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9847Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0402 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9852Epoch 00005: val_loss improved from 0.03970 to 0.03887, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 933us/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0389 - val_acc: 0.9843\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9858Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0356 - acc: 0.9858 - val_loss: 0.0427 - val_acc: 0.9829\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9862Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0344 - acc: 0.9862 - val_loss: 0.0460 - val_acc: 0.9814\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9867Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 929us/step - loss: 0.0329 - acc: 0.9867 - val_loss: 0.0434 - val_acc: 0.9824\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 927us/step - loss: 0.0317 - acc: 0.9872 - val_loss: 0.0398 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9877Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 929us/step - loss: 0.0304 - acc: 0.9877 - val_loss: 0.0427 - val_acc: 0.9829\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9805Epoch 00001: val_loss improved from inf to 0.04301, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 934us/step - loss: 0.0539 - acc: 0.9805 - val_loss: 0.0430 - val_acc: 0.9835\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9832Epoch 00002: val_loss improved from 0.04301 to 0.04060, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 933us/step - loss: 0.0430 - acc: 0.9832 - val_loss: 0.0406 - val_acc: 0.9841\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9840Epoch 00003: val_loss improved from 0.04060 to 0.03986, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 111s 931us/step - loss: 0.0403 - acc: 0.9840 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9846Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 926us/step - loss: 0.0385 - acc: 0.9846 - val_loss: 0.0430 - val_acc: 0.9834\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9850Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 925us/step - loss: 0.0370 - acc: 0.9850 - val_loss: 0.0401 - val_acc: 0.9842\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9855Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 927us/step - loss: 0.0358 - acc: 0.9855 - val_loss: 0.0412 - val_acc: 0.9838\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9861Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 927us/step - loss: 0.0344 - acc: 0.9861 - val_loss: 0.0427 - val_acc: 0.9833\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9865Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 925us/step - loss: 0.0331 - acc: 0.9865 - val_loss: 0.0446 - val_acc: 0.9821\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9869Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 927us/step - loss: 0.0321 - acc: 0.9869 - val_loss: 0.0431 - val_acc: 0.9826\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9874Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 927us/step - loss: 0.0305 - acc: 0.9874 - val_loss: 0.0452 - val_acc: 0.9818\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9804Epoch 00001: val_loss improved from inf to 0.04115, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 113s 941us/step - loss: 0.0543 - acc: 0.9804 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9831Epoch 00002: val_loss improved from 0.04115 to 0.04025, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 934us/step - loss: 0.0431 - acc: 0.9831 - val_loss: 0.0403 - val_acc: 0.9837\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9840Epoch 00003: val_loss improved from 0.04025 to 0.03931, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 935us/step - loss: 0.0406 - acc: 0.9840 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9847Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0385 - acc: 0.9847 - val_loss: 0.0446 - val_acc: 0.9822\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9853Epoch 00005: val_loss improved from 0.03931 to 0.03887, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 112s 935us/step - loss: 0.0371 - acc: 0.9853 - val_loss: 0.0389 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9856Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0357 - acc: 0.9856 - val_loss: 0.0434 - val_acc: 0.9820\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9860Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 928us/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0400 - val_acc: 0.9837\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 111s 930us/step - loss: 0.0331 - acc: 0.9866 - val_loss: 0.0408 - val_acc: 0.9835\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9870Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 107s 898us/step - loss: 0.0320 - acc: 0.9870 - val_loss: 0.0427 - val_acc: 0.9823\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9874Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 109s 907us/step - loss: 0.0307 - acc: 0.9874 - val_loss: 0.0422 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "# adj_v1\n",
    "# 1st epo , 3928, 4 fold: final 0.0393455938053 0.984445795289\n",
    "# 10 fold: final 0.0391567844913 0.984588887287 PUB 9857"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_adj2_4.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_adj2_4_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape) \n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_adj2_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_adj2_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
