{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "interessted 64493\n",
      "blissful 74356\n",
      "trnopolje 54751\n",
      "20prizren 127923\n",
      "ocurred 60740\n",
      "b003000shn72 144343\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 79399\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "import gc\n",
    "from keras import backend as K\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Bidirectional(CuDNNGRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9793Epoch 00001: val_loss improved from inf to 0.04360, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 74s 578us/step - loss: 0.0597 - acc: 0.9793 - val_loss: 0.0436 - val_acc: 0.9830\n",
      "Epoch 2/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9830Epoch 00002: val_loss improved from 0.04360 to 0.04164, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 74s 578us/step - loss: 0.0444 - acc: 0.9830 - val_loss: 0.0416 - val_acc: 0.9836\n",
      "Epoch 3/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9839Epoch 00003: val_loss improved from 0.04164 to 0.04031, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 74s 578us/step - loss: 0.0417 - acc: 0.9839 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 4/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9844Epoch 00004: val_loss improved from 0.04031 to 0.03992, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 73s 574us/step - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0399 - val_acc: 0.9841\n",
      "Epoch 5/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 73s 571us/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0412 - val_acc: 0.9835\n",
      "Epoch 6/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9853Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 74s 576us/step - loss: 0.0377 - acc: 0.9853 - val_loss: 0.0402 - val_acc: 0.9841\n",
      "Epoch 7/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9858Epoch 00007: val_loss improved from 0.03992 to 0.03975, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 74s 578us/step - loss: 0.0366 - acc: 0.9858 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 8/8\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9860Epoch 00008: val_loss did not improve\n",
      "127656/127656 [==============================] - 72s 567us/step - loss: 0.0357 - acc: 0.9860 - val_loss: 0.0404 - val_acc: 0.9842\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9785Epoch 00001: val_loss improved from inf to 0.04433, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 578us/step - loss: 0.0607 - acc: 0.9785 - val_loss: 0.0443 - val_acc: 0.9829\n",
      "Epoch 2/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9831Epoch 00002: val_loss improved from 0.04433 to 0.04248, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 73s 575us/step - loss: 0.0441 - acc: 0.9831 - val_loss: 0.0425 - val_acc: 0.9833\n",
      "Epoch 3/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9838Epoch 00003: val_loss improved from 0.04248 to 0.04026, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 579us/step - loss: 0.0416 - acc: 0.9838 - val_loss: 0.0403 - val_acc: 0.9840\n",
      "Epoch 4/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9844Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 576us/step - loss: 0.0399 - acc: 0.9844 - val_loss: 0.0411 - val_acc: 0.9842\n",
      "Epoch 5/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00005: val_loss improved from 0.04026 to 0.03994, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 73s 574us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0399 - val_acc: 0.9842\n",
      "Epoch 6/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9850Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 75s 589us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0406 - val_acc: 0.9838\n",
      "Epoch 7/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 580us/step - loss: 0.0367 - acc: 0.9854 - val_loss: 0.0429 - val_acc: 0.9830\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9859Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 578us/step - loss: 0.0359 - acc: 0.9859 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0611 - acc: 0.9785Epoch 00001: val_loss improved from inf to 0.04252, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 577us/step - loss: 0.0611 - acc: 0.9785 - val_loss: 0.0425 - val_acc: 0.9837\n",
      "Epoch 2/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9830Epoch 00002: val_loss improved from 0.04252 to 0.04212, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 75s 584us/step - loss: 0.0445 - acc: 0.9830 - val_loss: 0.0421 - val_acc: 0.9835\n",
      "Epoch 3/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00003: val_loss improved from 0.04212 to 0.03945, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 75s 585us/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 4/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9842Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 582us/step - loss: 0.0403 - acc: 0.9842 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 5/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9847Epoch 00005: val_loss improved from 0.03945 to 0.03916, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 582us/step - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0392 - val_acc: 0.9847\n",
      "Epoch 6/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9849Epoch 00006: val_loss improved from 0.03916 to 0.03855, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 583us/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0386 - val_acc: 0.9851\n",
      "Epoch 7/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9853Epoch 00007: val_loss improved from 0.03855 to 0.03814, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 581us/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0381 - val_acc: 0.9851\n",
      "Epoch 8/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9857Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 577us/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9790Epoch 00001: val_loss improved from inf to 0.04403, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 583us/step - loss: 0.0606 - acc: 0.9790 - val_loss: 0.0440 - val_acc: 0.9833\n",
      "Epoch 2/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9830Epoch 00002: val_loss improved from 0.04403 to 0.04131, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 581us/step - loss: 0.0445 - acc: 0.9830 - val_loss: 0.0413 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9838Epoch 00003: val_loss improved from 0.04131 to 0.04130, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 581us/step - loss: 0.0421 - acc: 0.9838 - val_loss: 0.0413 - val_acc: 0.9839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9843Epoch 00004: val_loss improved from 0.04130 to 0.03978, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 73s 575us/step - loss: 0.0404 - acc: 0.9843 - val_loss: 0.0398 - val_acc: 0.9842\n",
      "Epoch 5/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9847Epoch 00005: val_loss improved from 0.03978 to 0.03967, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 73s 574us/step - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 6/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9852Epoch 00006: val_loss improved from 0.03967 to 0.03931, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 580us/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0393 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9855Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 576us/step - loss: 0.0369 - acc: 0.9855 - val_loss: 0.0394 - val_acc: 0.9844\n",
      "Epoch 8/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9857Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 74s 579us/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0404 - val_acc: 0.9838\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.04457, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 73s 576us/step - loss: 0.0603 - acc: 0.9791 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Epoch 2/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9828Epoch 00002: val_loss improved from 0.04457 to 0.04266, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 74s 582us/step - loss: 0.0447 - acc: 0.9828 - val_loss: 0.0427 - val_acc: 0.9834\n",
      "Epoch 3/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9836Epoch 00003: val_loss improved from 0.04266 to 0.04115, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 76s 596us/step - loss: 0.0418 - acc: 0.9836 - val_loss: 0.0411 - val_acc: 0.9837\n",
      "Epoch 4/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9844Epoch 00004: val_loss improved from 0.04115 to 0.03998, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 78s 607us/step - loss: 0.0401 - acc: 0.9844 - val_loss: 0.0400 - val_acc: 0.9844\n",
      "Epoch 5/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 620us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0408 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9850Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 80s 624us/step - loss: 0.0377 - acc: 0.9850 - val_loss: 0.0416 - val_acc: 0.9835\n",
      "Epoch 7/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 618us/step - loss: 0.0367 - acc: 0.9855 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 8/8\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9858Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 78s 608us/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0404 - val_acc: 0.9845\n",
      "-------------------------------\n",
      "0 0.08632645256767155 0.9669990161119502\n",
      "1 0.022159135287232103 0.9902425879389112\n",
      "2 0.04293177896516248 0.9829730966152997\n",
      "3 0.007695879350289246 0.997355409190893\n",
      "4 0.05836461469210035 0.976173615506577\n",
      "5 0.01906354428578507 0.9931315840597602\n",
      "final 0.039423567524706805 0.9844792182372318\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "\n",
    "# 40000, 150\n",
    "# final 0.040348153796 0.984148122153, pub 9849\n",
    "\n",
    "# 180000, 150, dr rate 0.4\n",
    "# final 0.039423567524706805 0.9844792182372318"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998185      0.612784  0.988706  0.211715  0.970391   \n",
      "1  0000247867823ef7  0.000664      0.000022  0.000238  0.000010  0.000203   \n",
      "2  00013b17ad220c46  0.001544      0.000087  0.000515  0.000060  0.000628   \n",
      "3  00017563c3f7919a  0.000087      0.000009  0.000039  0.000027  0.000070   \n",
      "4  00017695ad8997eb  0.004227      0.000063  0.000410  0.000077  0.000360   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.601678  \n",
      "1       0.000033  \n",
      "2       0.000215  \n",
      "3       0.000018  \n",
      "4       0.000055  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
