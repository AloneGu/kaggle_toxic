{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "thuranx 29059\n",
      "llibya 139832\n",
      "shenanigans 15810\n",
      "hgilbert 137237\n",
      "lamoron 179605\n",
      "slaughtered 10252\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3760\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9795Epoch 00001: val_loss improved from inf to 0.04435, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 487s 4ms/step - loss: 0.0582 - acc: 0.9795 - val_loss: 0.0443 - val_acc: 0.9827\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9834Epoch 00002: val_loss improved from 0.04435 to 0.04155, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 487s 4ms/step - loss: 0.0434 - acc: 0.9834 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9843Epoch 00003: val_loss improved from 0.04155 to 0.04059, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 489s 4ms/step - loss: 0.0406 - acc: 0.9843 - val_loss: 0.0406 - val_acc: 0.9838\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9850Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 491s 4ms/step - loss: 0.0383 - acc: 0.9850 - val_loss: 0.0411 - val_acc: 0.9837\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 493s 4ms/step - loss: 0.0370 - acc: 0.9856 - val_loss: 0.0410 - val_acc: 0.9837\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 494s 4ms/step - loss: 0.0351 - acc: 0.9862 - val_loss: 0.0409 - val_acc: 0.9839\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 497s 4ms/step - loss: 0.0337 - acc: 0.9867 - val_loss: 0.0415 - val_acc: 0.9836\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 498s 4ms/step - loss: 0.0320 - acc: 0.9875 - val_loss: 0.0413 - val_acc: 0.9837\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9792Epoch 00001: val_loss improved from inf to 0.04356, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 498s 4ms/step - loss: 0.0590 - acc: 0.9792 - val_loss: 0.0436 - val_acc: 0.9835\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9834Epoch 00002: val_loss improved from 0.04356 to 0.04181, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 498s 4ms/step - loss: 0.0428 - acc: 0.9834 - val_loss: 0.0418 - val_acc: 0.9842\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9845Epoch 00003: val_loss improved from 0.04181 to 0.04096, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 499s 4ms/step - loss: 0.0401 - acc: 0.9845 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9850Epoch 00004: val_loss improved from 0.04096 to 0.04037, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 498s 4ms/step - loss: 0.0381 - acc: 0.9850 - val_loss: 0.0404 - val_acc: 0.9845\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 498s 4ms/step - loss: 0.0363 - acc: 0.9856 - val_loss: 0.0404 - val_acc: 0.9845\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 496s 4ms/step - loss: 0.0348 - acc: 0.9863 - val_loss: 0.0412 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 496s 4ms/step - loss: 0.0334 - acc: 0.9868 - val_loss: 0.0414 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9874Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 494s 4ms/step - loss: 0.0317 - acc: 0.9874 - val_loss: 0.0420 - val_acc: 0.9838\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0594 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.04299, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 490s 4ms/step - loss: 0.0594 - acc: 0.9791 - val_loss: 0.0430 - val_acc: 0.9837\n",
      "Epoch 2/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9834Epoch 00002: val_loss improved from 0.04299 to 0.04088, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 488s 4ms/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0409 - val_acc: 0.9843\n",
      "Epoch 3/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9842Epoch 00003: val_loss improved from 0.04088 to 0.03938, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 488s 4ms/step - loss: 0.0406 - acc: 0.9842 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 4/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9850Epoch 00004: val_loss did not improve\n",
      "119678/119678 [==============================] - 488s 4ms/step - loss: 0.0384 - acc: 0.9849 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 5/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9856Epoch 00005: val_loss improved from 0.03938 to 0.03928, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 489s 4ms/step - loss: 0.0367 - acc: 0.9857 - val_loss: 0.0393 - val_acc: 0.9842\n",
      "Epoch 6/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 489s 4ms/step - loss: 0.0350 - acc: 0.9861 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 7/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 489s 4ms/step - loss: 0.0335 - acc: 0.9867 - val_loss: 0.0411 - val_acc: 0.9843\n",
      "Epoch 8/8\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9874Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 488s 4ms/step - loss: 0.0321 - acc: 0.9874 - val_loss: 0.0401 - val_acc: 0.9845\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9798Epoch 00001: val_loss improved from inf to 0.04434, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 492s 4ms/step - loss: 0.0581 - acc: 0.9798 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 2/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9837Epoch 00002: val_loss improved from 0.04434 to 0.04222, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 490s 4ms/step - loss: 0.0425 - acc: 0.9837 - val_loss: 0.0422 - val_acc: 0.9836\n",
      "Epoch 3/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9845Epoch 00003: val_loss improved from 0.04222 to 0.04115, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 491s 4ms/step - loss: 0.0400 - acc: 0.9845 - val_loss: 0.0411 - val_acc: 0.9840\n",
      "Epoch 4/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9851Epoch 00004: val_loss did not improve\n",
      "119679/119679 [==============================] - 491s 4ms/step - loss: 0.0382 - acc: 0.9851 - val_loss: 0.0423 - val_acc: 0.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "119679/119679 [==============================] - 472s 4ms/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0420 - val_acc: 0.9837\n",
      "Epoch 6/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "119679/119679 [==============================] - 471s 4ms/step - loss: 0.0349 - acc: 0.9862 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "Epoch 7/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "119679/119679 [==============================] - 472s 4ms/step - loss: 0.0331 - acc: 0.9871 - val_loss: 0.0418 - val_acc: 0.9838\n",
      "Epoch 8/8\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0317 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "119679/119679 [==============================] - 472s 4ms/step - loss: 0.0317 - acc: 0.9875 - val_loss: 0.0423 - val_acc: 0.9837\n",
      "-------------------------------\n",
      "0 0.0883629048591 0.96644753746\n",
      "1 0.0223739609519 0.990248854742\n",
      "2 0.0449250225548 0.982290015103\n",
      "3 0.00800831313311 0.997280207557\n",
      "4 0.0591202311593 0.975484267191\n",
      "5 0.0192984901181 0.993137850863\n",
      "final 0.040348153796 0.984148122153\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "\n",
    "# 40000, 150\n",
    "# final 0.040348153796 0.984148122153, pub 9849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.994778      0.509820  0.970709  0.308434  0.931687   \n",
      "1  0000247867823ef7  0.000976      0.000090  0.000451  0.000031  0.000396   \n",
      "2  00013b17ad220c46  0.004889      0.000405  0.002275  0.000302  0.001919   \n",
      "3  00017563c3f7919a  0.000238      0.000023  0.000092  0.000049  0.000141   \n",
      "4  00017695ad8997eb  0.006597      0.000163  0.000951  0.000110  0.000669   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.462867  \n",
      "1       0.000082  \n",
      "2       0.000348  \n",
      "3       0.000018  \n",
      "4       0.000103  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_sample_4.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_4_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
