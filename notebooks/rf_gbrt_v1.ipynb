{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cleaning ...\n",
      " Part 1/2 of vectorizing ...\n",
      " Part 2/2 of vectorizing ...\n",
      "(159571, 100000) (153164, 100000)\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# ref: https://www.kaggle.com/tilii7/tuned-logreg-oof-files\n",
    "# https://www.kaggle.com/peterhurford/lightgbm-with-select-k-best-on-tfidf/code\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.metrics import log_loss, matthews_corrcoef, roc_auc_score\n",
    "\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "train = pd.read_csv('../input/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../input/test.csv').fillna(' ')\n",
    "tr_ids = train[['id']]\n",
    "train[class_names] = train[class_names].astype(np.int8)\n",
    "target = train[class_names]\n",
    "\n",
    "print(' Cleaning ...')\n",
    "# PREPROCESSING PART\n",
    "repl = {\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" frown \",\n",
    "    \":(\": \" frown \",\n",
    "    \":s\": \" frown \",\n",
    "    \":-s\": \" frown \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"its\": \"it is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"'s\": \" is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"weren't\": \"were not\",\n",
    "}\n",
    "\n",
    "keys = [i for i in repl.keys()]\n",
    "\n",
    "new_train_data = []\n",
    "new_test_data = []\n",
    "ltr = train[\"comment_text\"].tolist()\n",
    "lte = test[\"comment_text\"].tolist()\n",
    "for i in ltr:\n",
    "    arr = str(i).split()\n",
    "    xx = \"\"\n",
    "    for j in arr:\n",
    "        j = str(j).lower()\n",
    "        if j[:4] == 'http' or j[:3] == 'www':\n",
    "            continue\n",
    "        if j in keys:\n",
    "            # print(\"inn\")\n",
    "            j = repl[j]\n",
    "        xx += j + \" \"\n",
    "    new_train_data.append(xx)\n",
    "for i in lte:\n",
    "    arr = str(i).split()\n",
    "    xx = \"\"\n",
    "    for j in arr:\n",
    "        j = str(j).lower()\n",
    "        if j[:4] == 'http' or j[:3] == 'www':\n",
    "            continue\n",
    "        if j in keys:\n",
    "            # print(\"inn\")\n",
    "            j = repl[j]\n",
    "        xx += j + \" \"\n",
    "    new_test_data.append(xx)\n",
    "train[\"new_comment_text\"] = new_train_data\n",
    "test[\"new_comment_text\"] = new_test_data\n",
    "\n",
    "trate = train[\"new_comment_text\"].tolist()\n",
    "tete = test[\"new_comment_text\"].tolist()\n",
    "for i, c in enumerate(trate):\n",
    "    trate[i] = re.sub('[^a-zA-Z ?!]+', '', str(trate[i]).lower())\n",
    "for i, c in enumerate(tete):\n",
    "    tete[i] = re.sub('[^a-zA-Z ?!]+', '', tete[i])\n",
    "train[\"comment_text\"] = trate\n",
    "test[\"comment_text\"] = tete\n",
    "del trate, tete\n",
    "train.drop([\"new_comment_text\"], axis=1, inplace=True)\n",
    "test.drop([\"new_comment_text\"], axis=1, inplace=True)\n",
    "\n",
    "train_text = train['comment_text']\n",
    "test_text = test['comment_text']\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "train.head()\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "test.head()\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "print(' Part 1/2 of vectorizing ...')\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    token_pattern=r'\\w{1,}',\n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=50000)\n",
    "word_vectorizer.fit(all_text)\n",
    "train_word_features = word_vectorizer.transform(train_text)\n",
    "test_word_features = word_vectorizer.transform(test_text)\n",
    "\n",
    "print(' Part 2/2 of vectorizing ...')\n",
    "char_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='char',\n",
    "    stop_words='english',\n",
    "    ngram_range=(4, 6),\n",
    "    max_features=50000)\n",
    "char_vectorizer.fit(all_text)\n",
    "train_char_features = char_vectorizer.transform(train_text)\n",
    "test_char_features = char_vectorizer.transform(test_text)\n",
    "\n",
    "train_features = hstack([train_char_features, train_word_features]).tocsr()\n",
    "test_features = hstack([test_char_features, test_word_features]).tocsr()\n",
    "print(train_features.shape, test_features.shape)\n",
    "\n",
    "del all_text\n",
    "del word_vectorizer\n",
    "del char_vectorizer\n",
    "del train_word_features\n",
    "del train_char_features\n",
    "del test_word_features\n",
    "del test_char_features\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "target = target.values\n",
    "print(target[:5])\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 24291)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.15312754867745565 0.14360858348565422\n",
      "0.9563407386911488 0.9709035305151267\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.15004052319536879 0.14523542916440946\n",
      "0.9589828896864575 0.9702390123933107\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.15374012637261034 0.14347498584438698\n",
      "0.9553292326519705 0.9709881006863271\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.15111452548780982 0.14498623987595957\n",
      "0.9588783894220568 0.970472740895429\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.15318092471758915 0.1444482876872729\n",
      "0.9565960199500014 0.970434930395617\n",
      "========= toxic\n",
      "(159571, 6358)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.026200849562584182 0.024393950920948357\n",
      "0.9834905295341142 0.9892345248271001\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.02485302796306221 0.024739270167841474\n",
      "0.9874761442452752 0.988636087688174\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.025724744345377654 0.02449048109690013\n",
      "0.9854929230201397 0.9890390278547435\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.025675778961342172 0.024452717823657967\n",
      "0.9856407083974738 0.9890739957612871\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.026239455402762443 0.024406165148494394\n",
      "0.9835741439585348 0.9890905154461658\n",
      "========= severe_toxic\n",
      "(159571, 13712)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.0654685076065036 0.06130104268213129\n",
      "0.9866266512008066 0.9911753780403698\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.06281796844572211 0.06141149207211225\n",
      "0.9882183329173213 0.9911021007645807\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.06382809426676027 0.06085577103505402\n",
      "0.9872662815866383 0.9910603747943911\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.0643834976476698 0.060106237361757554\n",
      "0.9861868169345417 0.9913520466073004\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.0639841199548176 0.061483905994617144\n",
      "0.987981292867215 0.9910035310464169\n",
      "========= obscene\n",
      "(159571, 3143)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.011243375881953049 0.009845917448007986\n",
      "0.9780125147972385 0.9941416227198985\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.011349363323167272 0.009995744189652673\n",
      "0.9756089456509214 0.9938538217460009\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.010721571025920824 0.010065130566948225\n",
      "0.9862478982683303 0.9938651239889994\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.01159756207158077 0.009951346749790526\n",
      "0.97525697139322 0.9938366738415665\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.011214676450116047 0.009969740249420872\n",
      "0.9752025500296092 0.994302216628638\n",
      "========= threat\n",
      "(159571, 16697)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.08232968179006281 0.07880854521546038\n",
      "0.9766851686768617 0.9819681365857412\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.08354598777502088 0.07807678235763334\n",
      "0.9753016693930497 0.982434427486526\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.08197103409436096 0.07859005258074968\n",
      "0.9756355092219822 0.982160542779399\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.08418600899759678 0.07893218536349927\n",
      "0.9726550822269204 0.9819625176526439\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.08167317358566811 0.07880112554838088\n",
      "0.9760656183716374 0.9821837263046896\n",
      "========= insult\n",
      "(159571, 6771)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.027363709384449188 0.02414495363599887\n",
      "0.9701457529029198 0.9866431991715936\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.02641484684325948 0.024388110389561464\n",
      "0.9773074156870055 0.9864593296846619\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.025740396850379207 0.024388155794732778\n",
      "0.9778550104158311 0.9862559414753829\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.025649729892117574 0.024265676442368718\n",
      "0.979659851141984 0.9863802848555704\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.02669046365512917 0.024537093892937747\n",
      "0.9731061519272466 0.9860790363072686\n",
      "========= identity_hate\n",
      "[[7.99756874e-01 1.75210004e-01 7.64584959e-01 2.81221692e-02\n",
      "  6.43627205e-01 1.12827662e-01]\n",
      " [3.17268732e-02 1.74658758e-04 4.79067993e-03 7.14903742e-06\n",
      "  7.08645492e-03 1.40205534e-03]\n",
      " [2.21007857e-02 5.42541690e-05 7.06161809e-03 2.71858175e-04\n",
      "  6.20110400e-03 1.78259840e-04]\n",
      " [2.21228921e-02 3.88829975e-05 3.79160805e-03 4.22235670e-04\n",
      "  4.96162184e-03 7.67239567e-05]\n",
      " [4.68747775e-02 5.00170501e-04 1.21403033e-02 3.57283810e-05\n",
      "  4.15381320e-03 3.02388785e-04]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = LogisticRegression(solver='sag')\n",
    "sfm = SelectFromModel(model, threshold=0.2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def kf_train(k=5):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=1001)\n",
    "    train_pred, test_pred = np.zeros((159571, 6)), np.zeros((153164, 6))\n",
    "    for j in range(6):\n",
    "        train_x = sfm.fit_transform(train_features, target[:, j])\n",
    "        print(train_x.shape)\n",
    "        test_x = sfm.transform(test_features)\n",
    "        fold_idx = 0\n",
    "        for train_index, test_index in skf.split(train_x, target[:, j]):\n",
    "            # data\n",
    "            curr_x, curr_y = train_x[train_index], target[train_index][:, j]\n",
    "            hold_out_x, hold_out_y = train_x[test_index], target[test_index][:, j]\n",
    "            \n",
    "            # fit\n",
    "            rf = RandomForestClassifier(n_estimators=100,max_features='sqrt',min_samples_leaf=50,oob_score=True)\n",
    "            rf.fit(curr_x,curr_y)\n",
    "\n",
    "            hold_out_pred = rf.predict_proba(hold_out_x)\n",
    "            print('hold out pred done')\n",
    "            curr_train_pred = rf.predict_proba(curr_x)\n",
    "            print('curr train pred done')\n",
    "            # floating point exception (core dumped), seems updated lightgbm fix this\n",
    "            print(fold_idx,log_loss(hold_out_y,hold_out_pred),log_loss(curr_y,curr_train_pred))\n",
    "            print(roc_auc_score(hold_out_y,hold_out_pred[:,1]),roc_auc_score(curr_y,curr_train_pred[:,1]))\n",
    "            fold_idx += 1\n",
    "\n",
    "            train_pred[test_index, j] = list(hold_out_pred[:,1].flatten())\n",
    "            y_test = rf.predict_proba(test_x)[:,1]\n",
    "            test_pred[:, j] += y_test\n",
    "        print('=========', class_names[j])\n",
    "    test_pred = test_pred / k\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred, test_pred = kf_train(5)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../features/rf1_feat.pkl', 'wb') as fout:\n",
    "    pickle.dump([train_pred, test_pred], fout)\n",
    "print(test_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 24295)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.17324130565629228 0.1658377861998216\n",
      "0.931159014340736 0.9400877161244077\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.1677725737488159 0.16789619540300962\n",
      "0.9403687272477792 0.9414593207184202\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.17126010412453882 0.1673507752472627\n",
      "0.9336669822039145 0.9399781227454979\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.16690990296452432 0.16550322173158946\n",
      "0.9379952087658139 0.9401682312634017\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.16993342426688393 0.16630487836107238\n",
      "0.9332119540894748 0.9391352110927718\n",
      "========= toxic\n",
      "(159571, 6358)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.03168559597384564 0.026145637132560275\n",
      "0.9731305022142797 0.9834744979687572\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.028033578671337416 0.026940780404558093\n",
      "0.9823432936742005 0.9847302146946367\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.031085402579847585 0.02657438834208363\n",
      "0.9778550135655963 0.9846745351711658\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.02977854339749681 0.02670898047965565\n",
      "0.9788668398684169 0.984661305160772\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.031033899140061025 0.026377222714445123\n",
      "0.9797516173792429 0.9864174053334621\n",
      "========= severe_toxic\n",
      "(159571, 13713)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.07816212857544337 0.07243551927021298\n",
      "0.9765770234092433 0.9817176586766645\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.07320297510285664 0.07272670166448605\n",
      "0.978832278620406 0.9815997453850243\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.07777014701175476 0.07513175882744952\n",
      "0.9779451789557105 0.9803494686239711\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.0764895324378003 0.07277008788579849\n",
      "0.9730172992347472 0.9802252887892929\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.07578350784298012 0.07302910695132488\n",
      "0.9802713941547478 0.9817529571115796\n",
      "========= obscene\n",
      "(159571, 3143)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.01222959198614235 0.007943559384472563\n",
      "0.9504719729825996 0.9824326840875197\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.042617313558378854 0.02991569914656937\n",
      "0.8507901463486177 0.9221509421031445\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.011238292365374488 0.008092411171142774\n",
      "0.9761918324481179 0.9793885241775855\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.013925760285455772 0.008728495357543355\n",
      "0.9626707490960098 0.9770073949164372\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.011700546936840228 0.009124079347310216\n",
      "0.9656989919641645 0.9773648885326226\n",
      "========= threat\n",
      "(159571, 16697)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.09204196652154881 0.09023603354785073\n",
      "0.9666191661969322 0.9678174475824876\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.09601283760759473 0.09052844325687338\n",
      "0.9653100380254729 0.9684012001681457\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.09331599313111792 0.09026041738691719\n",
      "0.9650665260335982 0.967113900755749\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.09335536684879733 0.08988617433546181\n",
      "0.9570502318509834 0.96842987402861\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.09197614810324607 0.09027914411319654\n",
      "0.9642004736058397 0.9674369034072393\n",
      "========= insult\n",
      "(159571, 6771)\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "0 0.029757845321901462 0.02436549275585719\n",
      "0.942066252874008 0.9756279880756538\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "1 0.029072598608158993 0.024717185549371214\n",
      "0.9621159510322626 0.9735682374279981\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "2 0.028205180017090915 0.025108762634648174\n",
      "0.9632241342631401 0.969543265168219\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "3 0.027502540639087122 0.024665049584057235\n",
      "0.9628448398351512 0.9702199766547055\n",
      "hold out pred done\n",
      "curr train pred done\n",
      "4 0.028609808644122728 0.025127977179577193\n",
      "0.952770503077274 0.9692022903003683\n",
      "========= identity_hate\n",
      "[[9.77287254e-01 8.27117217e-02 9.85817462e-01 2.80014297e-03\n",
      "  8.99556342e-01 1.07323246e-01]\n",
      " [5.36136099e-02 2.53320310e-03 1.78717349e-02 6.03283853e-04\n",
      "  1.97891423e-02 2.67787887e-03]\n",
      " [6.93464635e-02 2.74600162e-03 2.27101167e-02 6.03283853e-04\n",
      "  1.99600043e-02 2.71203618e-03]\n",
      " [3.59838357e-02 2.53320310e-03 1.41445390e-02 6.03283853e-04\n",
      "  1.69006741e-02 2.77538144e-03]\n",
      " [5.85822476e-02 2.74600162e-03 2.11708505e-02 5.87902889e-04\n",
      "  1.87430499e-02 2.71203618e-03]]\n",
      "CPU times: user 10min 24s, sys: 2.73 s, total: 10min 27s\n",
      "Wall time: 10min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "model = LogisticRegression(solver='sag')\n",
    "sfm = SelectFromModel(model, threshold=0.2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def kf_train(k=5):\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=1001)\n",
    "    train_pred, test_pred = np.zeros((159571, 6)), np.zeros((153164, 6))\n",
    "    for j in range(6):\n",
    "        train_x = sfm.fit_transform(train_features, target[:, j])\n",
    "        print(train_x.shape)\n",
    "        test_x = sfm.transform(test_features)\n",
    "        fold_idx = 0\n",
    "        for train_index, test_index in skf.split(train_x, target[:, j]):\n",
    "            # data\n",
    "            curr_x, curr_y = train_x[train_index], target[train_index][:, j]\n",
    "            hold_out_x, hold_out_y = train_x[test_index], target[test_index][:, j]\n",
    "            \n",
    "            # fit\n",
    "            rf = GradientBoostingClassifier(n_estimators=100,max_depth=3,\n",
    "                                            max_features='sqrt',min_samples_leaf=50)\n",
    "            rf.fit(curr_x,curr_y)\n",
    "\n",
    "            hold_out_pred = rf.predict_proba(hold_out_x)\n",
    "            print('hold out pred done')\n",
    "            curr_train_pred = rf.predict_proba(curr_x)\n",
    "            print('curr train pred done')\n",
    "            # floating point exception (core dumped), seems updated lightgbm fix this\n",
    "            print(fold_idx,log_loss(hold_out_y,hold_out_pred),log_loss(curr_y,curr_train_pred))\n",
    "            print(roc_auc_score(hold_out_y,hold_out_pred[:,1]),roc_auc_score(curr_y,curr_train_pred[:,1]))\n",
    "            fold_idx += 1\n",
    "\n",
    "            train_pred[test_index, j] = list(hold_out_pred[:,1].flatten())\n",
    "            y_test = rf.predict_proba(test_x)[:,1]\n",
    "            test_pred[:, j] += y_test\n",
    "        print('=========', class_names[j])\n",
    "    test_pred = test_pred / k\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred, test_pred = kf_train(5)\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('../features/gbrt1_feat.pkl', 'wb') as fout:\n",
    "    pickle.dump([train_pred, test_pred], fout)\n",
    "print(test_pred[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
