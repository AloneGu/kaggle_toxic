{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, MaxPool1D\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^A-za-z0-9^,?!.\\/'+-=]\",\" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    #\n",
    "    return text.lower()\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense?  kiss off, geek. what i said is true.  i will  have your account terminated.',\n",
       "       '    please do not vandalize pages, as you did with this edit to w. s. merwin. if you continue to do so, you will be blocked from editing.     ',\n",
       "       '      points of interest     i removed the   points of interest   section you added because it seemed kind of spammy. i know you probably did not  mean to disobey the rules, but generally, a point of interest tends to be rather touristy, and quite irrelevant to an area culture. that  just my opinion, though.  if you want to reply, just put your reply here and add   talkback jamiegraham08   on my talkpage.    ',\n",
       "       'asking some his nationality is a racial offence. wow was not  aware of it.  blocking me has shown your support towards your community. thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content. the cited sources in the external links are saying those things. if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it.'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s9 138179\n",
      "vse2kwkwh0yc 134170\n",
      "'pox 78037\n",
      "asing 135072\n",
      "atropatne 120328\n",
      "fractured 27636\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "[ 0.0137334   0.0554924   0.0455881   0.0301357  -0.0182521   0.0385928\n",
      "  0.0347148  -0.0847695  -0.036347   -0.00864285  0.00292431  0.0167314\n",
      "  0.0195147  -0.0372235  -0.014314    0.0173197   0.00033499 -0.0477708\n",
      "  0.0113374   0.0266912  -0.0615091   0.0665893  -0.125759   -0.069915\n",
      " -0.0024989   0.022528    0.00747391  0.0752322  -0.0552592   0.0327767\n",
      " -0.0275065   0.144234   -0.130117    0.0105687   0.00473044 -0.0610046\n",
      " -0.0559855   0.0619029   0.0353677  -0.0334999  -0.0226966  -0.00395828\n",
      " -0.0283532   0.0217597  -0.0418534   0.109104    0.0736382   0.00400721\n",
      " -0.0209592  -0.0116593   0.0260413  -0.0188025   0.0445063  -0.0389139\n",
      "  0.0402938   0.0368409   0.116023    0.0378068   0.0615779   0.0601903\n",
      "  0.0328234  -0.0483939   0.0331058  -0.00478472 -0.0229684   0.0221889\n",
      " -0.0747123   0.0113791   0.0517195  -0.0209997  -0.0373122   0.0159027\n",
      "  0.0738867  -0.0272447  -0.15535    -0.0287317   0.0143244   0.093508\n",
      "  0.0261212  -0.0315336  -0.0178931   0.049338    0.0325233  -0.101776\n",
      " -0.0343944  -0.0715855  -0.0756756   0.0464864  -0.0495292  -0.028531\n",
      "  0.101704    0.0813674   0.0523961  -0.084437    0.0233384  -0.0965087\n",
      "  0.0465284   0.0650641   0.0544679  -0.0228411  -0.0875562   0.0265608\n",
      "  0.0288188   0.0208174   0.00985346 -0.0303444  -0.0890547  -0.0215269\n",
      "  0.0327438  -0.0529924   0.00414673  0.0457334  -0.110981    0.0217876\n",
      "  0.0403129  -0.0348963   0.0596399   0.0637683   0.0753927   0.0261392\n",
      "  0.0963978   0.0493342  -0.0457792   0.139566    0.0176336   0.0570405\n",
      "  0.0442731   0.0580038   0.025341    0.0158113  -0.0421707   0.0131787\n",
      "  0.0332905   0.0258399   0.00777283 -0.0308482   0.0425262   0.0304446\n",
      "  0.0451829   0.0545712  -0.0620749   0.0183671   0.0777092  -0.0712911\n",
      "  0.0261243   0.0453817  -0.0835004   0.030951    0.0442502  -0.0329491\n",
      "  0.00636268 -0.0280386  -0.00686612 -0.0137984  -0.00504542 -0.03038\n",
      "  0.0423121  -0.0174176   0.204825   -0.00423121  0.0312328   0.0658707\n",
      " -0.0933092  -0.0154333   0.124245   -0.00172716 -0.00903085 -0.0510735\n",
      "  0.043738    0.0508174  -0.0500682  -0.0769485  -0.0323516   0.0213243\n",
      "  0.00939705 -0.0118726   0.0653929   0.00626215 -0.002706   -0.0148779\n",
      " -0.0157559   0.051838    0.0172559   0.151172    0.0323375   0.0495406\n",
      "  0.0283223   0.0191622   0.0516239  -0.0264136   0.00160694 -0.0751252\n",
      "  0.0199661   0.08394     0.0292538  -0.0523349  -0.0344334   0.0151347\n",
      " -0.0753545   0.0338726   0.0202455  -0.0582905  -0.132062    0.0284675\n",
      " -0.106914   -0.0284862  -0.0819485   0.0557218   0.0720403  -0.0460812\n",
      "  0.0737643   0.0343738   0.0423733   0.0914132  -0.0222386  -0.0583173\n",
      " -0.0191118  -0.113936   -0.0627973  -0.0202704   0.021929   -0.0464596\n",
      " -0.104777   -0.0475453   0.116076    0.00148308 -0.0448274   0.114979\n",
      " -0.00536576  0.00198231  0.00550757  0.0214129  -0.0920783   0.0133217\n",
      " -0.0657102  -0.0161932  -0.0667728  -0.0910844  -0.00707751 -0.00573922\n",
      " -0.0809584   0.0643034   0.0480154  -0.0452708   0.112357    0.116573\n",
      "  0.140782   -0.00062847  0.0514061  -0.0682063  -0.0157249   0.111057\n",
      " -0.0129283  -0.0190529  -0.0416547   0.0220406  -0.00198602 -0.0466087\n",
      "  0.0705343  -0.0166271   0.0500376   0.123882    0.0188373  -0.0474459\n",
      " -0.0897274  -0.0248861  -0.0506492   0.139134    0.00869064 -0.0600451\n",
      " -0.0222352   0.0452746  -0.00452899  0.0167024   0.01515    -0.0995897\n",
      "  0.124891    0.0685312   0.0421631  -0.0172826   0.0846549   0.0579083\n",
      "  0.0233793  -0.0212448  -0.0287898  -0.038436    0.0443993   0.0742001\n",
      " -0.0387648   0.0815203   0.0154142  -0.104777   -0.0444681  -0.0341807\n",
      " -0.0278108  -0.0995515   0.0322037  -0.00061475  0.0652897  -0.0135747 ]\n",
      "[ 0.0232281   0.0406795   0.00814886  0.126484    0.0048425   0.0333417\n",
      "  0.0743128  -0.0851157   0.00038209  0.0549285   0.00221096  0.115641\n",
      " -0.0400891  -0.0128689   0.0633394  -0.0238348   0.059047   -0.0107363\n",
      "  0.013712   -0.0946492  -0.0692117   0.0102299  -0.093025   -0.00856172\n",
      "  0.0313138   0.0376017   0.132739   -0.0344182  -0.0586819  -0.0228971\n",
      " -0.013827    0.0270869  -0.0504109   0.0425766  -0.0566516  -0.0339159\n",
      " -0.0263659   0.13468     0.00186968 -0.0287336   0.00435798 -0.0616811\n",
      " -0.0103132   0.0311043   0.0121752   0.023832    0.0553448  -0.0332147\n",
      "  0.00575388 -0.0254289   0.018202   -0.0731971   0.0113894   0.0140133\n",
      " -0.0942534   0.0263604  -0.0050793   0.0877805  -0.14708     0.077684\n",
      "  0.00313285 -0.0254989   0.076268    0.0315479   0.0129279   0.0967135\n",
      " -0.0923767   0.0111294  -0.0234328  -0.0198463  -0.0592107   0.0411708\n",
      "  0.00173893 -0.0584635  -0.115357   -0.028853    0.0748656   0.0656938\n",
      "  0.0324204  -0.050578    0.00052144  0.0363153  -0.0195645   0.0215961\n",
      " -0.0328725  -0.0382125  -0.0223833   0.0707165  -0.00495237 -0.0170337\n",
      " -0.0189592  -0.030699    0.00475685 -0.11388     0.00703411 -0.0335945\n",
      "  0.0542836   0.00303202  0.119817    0.00362914  0.0893979   0.0317403\n",
      "  0.0637864  -0.0625717  -0.0565936  -0.0447092  -0.0246564   0.0164475\n",
      " -0.0256783  -0.0460263  -0.0280003  -0.0257592  -0.0750533   0.00175455\n",
      "  0.0693277   0.00044122  0.0230715   0.0515744   0.0373765   0.0747018\n",
      "  0.0590913   0.0371957  -0.0700579   0.100119    0.00769607  0.0850133\n",
      "  0.110065    0.044774    0.0295887   0.0720199  -0.0329998   0.0078851\n",
      " -0.047183    0.103046    0.0535568  -0.0169818  -0.0143275   0.0375983\n",
      " -0.00749953 -0.027453   -0.0172941   0.0893228   0.0819936  -0.0333331\n",
      "  0.0478825   0.0383251  -0.111027   -0.0116323  -0.0120414   0.0167171\n",
      " -0.0258943  -0.118493    0.00547238 -0.016471   -0.00727979 -0.0450163\n",
      "  0.0393726   0.00821027  0.150397    0.0129688   0.0491381   0.0994944\n",
      " -0.0824815  -0.00167348  0.0531713   0.0347765  -0.0354589  -0.0981227\n",
      "  0.09714     0.142637   -0.014434   -0.0175022  -0.033631    0.116323\n",
      " -0.0381954   0.06726    -0.008617   -0.0237689   0.013221    0.0206963\n",
      " -0.0307017   0.00295897  0.0513253   0.0870196   0.0951917   0.0466404\n",
      "  0.018923    0.0204455   0.113215   -0.0515061  -0.0103162  -0.0607598\n",
      " -0.0592278   0.00244356 -0.00028886 -0.0934447  -0.0173852   0.0599409\n",
      " -0.0258571  -0.0285261   0.050967   -0.0596646  -0.0388744  -0.0426005\n",
      " -0.017882   -0.0561637  -0.0390416   0.0643562   0.0848427   0.0187599\n",
      "  0.0498308   0.0117896   0.0230493   0.0741695   0.0349198  -0.0116354\n",
      " -0.0478142  -0.0712487  -0.0211791  -0.0189919  -0.00788066 -0.0128614\n",
      " -0.082461   -0.0309238  -0.0604459   0.070464   -0.0278242   0.00524581\n",
      " -0.00461935 -0.0202063  -0.0667345  -0.0334846  -0.0567267  -0.0687306\n",
      " -0.113235    0.0557713  -0.0340623  -0.100624    0.0291703   0.00734257\n",
      " -0.00059941 -0.00881149  0.0671474  -0.0225951  -0.0108086   0.171859\n",
      "  0.096072   -0.0192335   0.0573614   0.00518269  0.013411    0.0656494\n",
      " -0.0833652  -0.0489539   0.00559965  0.0251996  -0.0277905  -0.0732926\n",
      "  0.0428734  -0.0544269   0.0114016   0.0998527  -0.0774145   0.00432488\n",
      " -0.117145   -0.0323712  -0.0557099   0.129402   -0.0403655   0.0411264\n",
      "  0.0601764   0.105776    0.0527379  -0.0144333   0.0037414  -0.0233015\n",
      "  0.0728081   0.0440506  -0.0686146   0.0457431   0.0691469   0.00605824\n",
      "  0.0349505   0.0434057  -0.0166202   0.0426858   0.0307549   0.0522636\n",
      " -0.172674    0.135554    0.0268552   0.029893   -0.0305673   0.0626126\n",
      "  0.0385742   0.017581   -0.0933594  -0.0207052   0.128395   -0.0459614 ]\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print(word_vec_dict['is'])\n",
    "print(word_vec_dict['are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 5293\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Conv1D(384,\n",
    "             5,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 146, 384)          576384    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 384)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 12,676,486\n",
      "Trainable params: 676,486\n",
      "Non-trainable params: 12,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_m=get_cnn_model()\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9756Epoch 00001: val_loss improved from inf to 0.05349, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 25s 395us/step - loss: 0.0733 - acc: 0.9756 - val_loss: 0.0535 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9806Epoch 00002: val_loss improved from 0.05349 to 0.04956, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 24s 376us/step - loss: 0.0530 - acc: 0.9806 - val_loss: 0.0496 - val_acc: 0.9819\n",
      "Epoch 3/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9822Epoch 00003: val_loss improved from 0.04956 to 0.04812, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 24s 372us/step - loss: 0.0477 - acc: 0.9822 - val_loss: 0.0481 - val_acc: 0.9821\n",
      "Epoch 4/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9834Epoch 00004: val_loss improved from 0.04812 to 0.04783, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 24s 374us/step - loss: 0.0436 - acc: 0.9834 - val_loss: 0.0478 - val_acc: 0.9823\n",
      "Epoch 5/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9845Epoch 00005: val_loss did not improve\n",
      "63900/63900 [==============================] - 24s 371us/step - loss: 0.0399 - acc: 0.9845 - val_loss: 0.0479 - val_acc: 0.9826\n",
      "Epoch 6/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9857Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 24s 372us/step - loss: 0.0364 - acc: 0.9857 - val_loss: 0.0491 - val_acc: 0.9822\n",
      "Epoch 7/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9870Epoch 00007: val_loss did not improve\n",
      "63900/63900 [==============================] - 24s 372us/step - loss: 0.0331 - acc: 0.9871 - val_loss: 0.0499 - val_acc: 0.9816\n",
      "Epoch 8/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0305 - acc: 0.9878Epoch 00008: val_loss did not improve\n",
      "63900/63900 [==============================] - 24s 372us/step - loss: 0.0305 - acc: 0.9878 - val_loss: 0.0499 - val_acc: 0.9819\n",
      "Epoch 9/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0280 - acc: 0.9890Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 24s 372us/step - loss: 0.0280 - acc: 0.9890 - val_loss: 0.0530 - val_acc: 0.9818\n",
      "Epoch 10/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9898Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 24s 372us/step - loss: 0.0254 - acc: 0.9898 - val_loss: 0.0569 - val_acc: 0.9824\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9749Epoch 00001: val_loss improved from inf to 0.05241, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 385us/step - loss: 0.0768 - acc: 0.9749 - val_loss: 0.0524 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0534 - acc: 0.9804Epoch 00002: val_loss improved from 0.05241 to 0.05097, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 24s 374us/step - loss: 0.0534 - acc: 0.9804 - val_loss: 0.0510 - val_acc: 0.9817\n",
      "Epoch 3/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9818Epoch 00003: val_loss improved from 0.05097 to 0.04779, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 24s 373us/step - loss: 0.0488 - acc: 0.9818 - val_loss: 0.0478 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9833Epoch 00004: val_loss improved from 0.04779 to 0.04763, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 24s 373us/step - loss: 0.0443 - acc: 0.9833 - val_loss: 0.0476 - val_acc: 0.9827\n",
      "Epoch 5/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9846- - ETA: 0s - loss: 0.0399 - acc: 0.9846Epoch 00005: val_loss improved from 0.04763 to 0.04714, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 391us/step - loss: 0.0398 - acc: 0.9846 - val_loss: 0.0471 - val_acc: 0.9826\n",
      "Epoch 6/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0516 - val_acc: 0.9827\n",
      "Epoch 7/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9868Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 365us/step - loss: 0.0334 - acc: 0.9868 - val_loss: 0.0502 - val_acc: 0.9826\n",
      "Epoch 8/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9880Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0301 - acc: 0.9880 - val_loss: 0.0554 - val_acc: 0.9823\n",
      "Epoch 9/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9892Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0272 - acc: 0.9892 - val_loss: 0.0543 - val_acc: 0.9817\n",
      "Epoch 10/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0254 - acc: 0.9899Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0254 - acc: 0.9899 - val_loss: 0.0588 - val_acc: 0.9817\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0774 - acc: 0.9751Epoch 00001: val_loss improved from inf to 0.05527, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 24s 372us/step - loss: 0.0774 - acc: 0.9751 - val_loss: 0.0553 - val_acc: 0.9801\n",
      "Epoch 2/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9808Epoch 00002: val_loss improved from 0.05527 to 0.05247, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0527 - acc: 0.9808 - val_loss: 0.0525 - val_acc: 0.9809\n",
      "Epoch 3/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9823Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 367us/step - loss: 0.0476 - acc: 0.9823 - val_loss: 0.0527 - val_acc: 0.9810\n",
      "Epoch 4/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9835Epoch 00004: val_loss improved from 0.05247 to 0.04949, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 23s 367us/step - loss: 0.0437 - acc: 0.9835 - val_loss: 0.0495 - val_acc: 0.9817\n",
      "Epoch 5/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9846Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0400 - acc: 0.9845 - val_loss: 0.0507 - val_acc: 0.9808\n",
      "Epoch 6/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9857Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 368us/step - loss: 0.0363 - acc: 0.9857 - val_loss: 0.0532 - val_acc: 0.9817\n",
      "Epoch 7/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9867Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 366us/step - loss: 0.0332 - acc: 0.9867 - val_loss: 0.0619 - val_acc: 0.9815\n",
      "Epoch 8/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0304 - acc: 0.9879Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 23s 367us/step - loss: 0.0304 - acc: 0.9879 - val_loss: 0.0546 - val_acc: 0.9812\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0276 - acc: 0.9890Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 386us/step - loss: 0.0276 - acc: 0.9890 - val_loss: 0.0584 - val_acc: 0.9814\n",
      "Epoch 10/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0251 - acc: 0.9899- ETA: 0s - loss: 0.0251 - acc: 0.989Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 386us/step - loss: 0.0251 - acc: 0.9899 - val_loss: 0.0586 - val_acc: 0.9809\n",
      "-------------------------------\n",
      "0 0.104356182777 0.961753137682\n",
      "1 0.0236904011323 0.990568695162\n",
      "2 0.0557392148131 0.979520297128\n",
      "3 0.0126944686347 0.996828410763\n",
      "4 0.0682361828897 0.97227989275\n",
      "5 0.0241961483835 0.992342281249\n",
      "final 0.0481520997718 0.982215452456\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_muse_adj_2_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_muse_adj_2_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.36 ,  0.   ,  0.018,  0.   ,  0.02 ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.019,  0.   ,  0.002,  0.   ,  0.002,  0.001],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.314,  0.005,  0.084,  0.006,  0.07 ,  0.007],\n",
       "       [ 0.005,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.028,  0.   ,  0.004,  0.   ,  0.005,  0.   ],\n",
       "       [ 0.057,  0.   ,  0.007,  0.   ,  0.01 ,  0.001],\n",
       "       [ 0.007,  0.   ,  0.002,  0.   ,  0.002,  0.   ]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
