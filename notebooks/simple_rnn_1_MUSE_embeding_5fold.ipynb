{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, SimpleRNN\n",
    "from keras.layers import ConvLSTM2D, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^A-za-z0-9^,?!.\\/'+-=]\",\" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'Nonsense?  kiss off, geek. what I said is true.  I will  have your account terminated.',\n",
       "       '    Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.     ',\n",
       "       '      Points of interest     I removed the   points of interest   section you added because it seemed kind of spammy. I know you probably did not  mean to disobey the rules, but generally, a point of interest tends to be rather touristy, and quite irrelevant to an area culture. That  just my opinion, though.  If you want to reply, just put your reply here and add   talkback Jamiegraham08   on my talkpage.    ',\n",
       "       'Asking some his nationality is a Racial offence. Wow was not  aware of it.  Blocking me has shown your support towards your community. Thanku for that',\n",
       "       'The reader here is not going by my say so for ethereal vocal style and dark lyrical content. The cited sources in the External Links are saying those things. If you feel the sources are unreliable or I did not represent what they said correctly rewrite or delete it.'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indium 59229\n",
      "inmthis 76439\n",
      "multimedia 21127\n",
      "koresh 82385\n",
      "wolfian 95621\n",
      "talkstalk 24443\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "[ 0.0137334   0.0554924   0.0455881   0.0301357  -0.0182521   0.0385928\n",
      "  0.0347148  -0.0847695  -0.036347   -0.00864285  0.00292431  0.0167314\n",
      "  0.0195147  -0.0372235  -0.014314    0.0173197   0.00033499 -0.0477708\n",
      "  0.0113374   0.0266912  -0.0615091   0.0665893  -0.125759   -0.069915\n",
      " -0.0024989   0.022528    0.00747391  0.0752322  -0.0552592   0.0327767\n",
      " -0.0275065   0.144234   -0.130117    0.0105687   0.00473044 -0.0610046\n",
      " -0.0559855   0.0619029   0.0353677  -0.0334999  -0.0226966  -0.00395828\n",
      " -0.0283532   0.0217597  -0.0418534   0.109104    0.0736382   0.00400721\n",
      " -0.0209592  -0.0116593   0.0260413  -0.0188025   0.0445063  -0.0389139\n",
      "  0.0402938   0.0368409   0.116023    0.0378068   0.0615779   0.0601903\n",
      "  0.0328234  -0.0483939   0.0331058  -0.00478472 -0.0229684   0.0221889\n",
      " -0.0747123   0.0113791   0.0517195  -0.0209997  -0.0373122   0.0159027\n",
      "  0.0738867  -0.0272447  -0.15535    -0.0287317   0.0143244   0.093508\n",
      "  0.0261212  -0.0315336  -0.0178931   0.049338    0.0325233  -0.101776\n",
      " -0.0343944  -0.0715855  -0.0756756   0.0464864  -0.0495292  -0.028531\n",
      "  0.101704    0.0813674   0.0523961  -0.084437    0.0233384  -0.0965087\n",
      "  0.0465284   0.0650641   0.0544679  -0.0228411  -0.0875562   0.0265608\n",
      "  0.0288188   0.0208174   0.00985346 -0.0303444  -0.0890547  -0.0215269\n",
      "  0.0327438  -0.0529924   0.00414673  0.0457334  -0.110981    0.0217876\n",
      "  0.0403129  -0.0348963   0.0596399   0.0637683   0.0753927   0.0261392\n",
      "  0.0963978   0.0493342  -0.0457792   0.139566    0.0176336   0.0570405\n",
      "  0.0442731   0.0580038   0.025341    0.0158113  -0.0421707   0.0131787\n",
      "  0.0332905   0.0258399   0.00777283 -0.0308482   0.0425262   0.0304446\n",
      "  0.0451829   0.0545712  -0.0620749   0.0183671   0.0777092  -0.0712911\n",
      "  0.0261243   0.0453817  -0.0835004   0.030951    0.0442502  -0.0329491\n",
      "  0.00636268 -0.0280386  -0.00686612 -0.0137984  -0.00504542 -0.03038\n",
      "  0.0423121  -0.0174176   0.204825   -0.00423121  0.0312328   0.0658707\n",
      " -0.0933092  -0.0154333   0.124245   -0.00172716 -0.00903085 -0.0510735\n",
      "  0.043738    0.0508174  -0.0500682  -0.0769485  -0.0323516   0.0213243\n",
      "  0.00939705 -0.0118726   0.0653929   0.00626215 -0.002706   -0.0148779\n",
      " -0.0157559   0.051838    0.0172559   0.151172    0.0323375   0.0495406\n",
      "  0.0283223   0.0191622   0.0516239  -0.0264136   0.00160694 -0.0751252\n",
      "  0.0199661   0.08394     0.0292538  -0.0523349  -0.0344334   0.0151347\n",
      " -0.0753545   0.0338726   0.0202455  -0.0582905  -0.132062    0.0284675\n",
      " -0.106914   -0.0284862  -0.0819485   0.0557218   0.0720403  -0.0460812\n",
      "  0.0737643   0.0343738   0.0423733   0.0914132  -0.0222386  -0.0583173\n",
      " -0.0191118  -0.113936   -0.0627973  -0.0202704   0.021929   -0.0464596\n",
      " -0.104777   -0.0475453   0.116076    0.00148308 -0.0448274   0.114979\n",
      " -0.00536576  0.00198231  0.00550757  0.0214129  -0.0920783   0.0133217\n",
      " -0.0657102  -0.0161932  -0.0667728  -0.0910844  -0.00707751 -0.00573922\n",
      " -0.0809584   0.0643034   0.0480154  -0.0452708   0.112357    0.116573\n",
      "  0.140782   -0.00062847  0.0514061  -0.0682063  -0.0157249   0.111057\n",
      " -0.0129283  -0.0190529  -0.0416547   0.0220406  -0.00198602 -0.0466087\n",
      "  0.0705343  -0.0166271   0.0500376   0.123882    0.0188373  -0.0474459\n",
      " -0.0897274  -0.0248861  -0.0506492   0.139134    0.00869064 -0.0600451\n",
      " -0.0222352   0.0452746  -0.00452899  0.0167024   0.01515    -0.0995897\n",
      "  0.124891    0.0685312   0.0421631  -0.0172826   0.0846549   0.0579083\n",
      "  0.0233793  -0.0212448  -0.0287898  -0.038436    0.0443993   0.0742001\n",
      " -0.0387648   0.0815203   0.0154142  -0.104777   -0.0444681  -0.0341807\n",
      " -0.0278108  -0.0995515   0.0322037  -0.00061475  0.0652897  -0.0135747 ]\n",
      "[ 0.0232281   0.0406795   0.00814886  0.126484    0.0048425   0.0333417\n",
      "  0.0743128  -0.0851157   0.00038209  0.0549285   0.00221096  0.115641\n",
      " -0.0400891  -0.0128689   0.0633394  -0.0238348   0.059047   -0.0107363\n",
      "  0.013712   -0.0946492  -0.0692117   0.0102299  -0.093025   -0.00856172\n",
      "  0.0313138   0.0376017   0.132739   -0.0344182  -0.0586819  -0.0228971\n",
      " -0.013827    0.0270869  -0.0504109   0.0425766  -0.0566516  -0.0339159\n",
      " -0.0263659   0.13468     0.00186968 -0.0287336   0.00435798 -0.0616811\n",
      " -0.0103132   0.0311043   0.0121752   0.023832    0.0553448  -0.0332147\n",
      "  0.00575388 -0.0254289   0.018202   -0.0731971   0.0113894   0.0140133\n",
      " -0.0942534   0.0263604  -0.0050793   0.0877805  -0.14708     0.077684\n",
      "  0.00313285 -0.0254989   0.076268    0.0315479   0.0129279   0.0967135\n",
      " -0.0923767   0.0111294  -0.0234328  -0.0198463  -0.0592107   0.0411708\n",
      "  0.00173893 -0.0584635  -0.115357   -0.028853    0.0748656   0.0656938\n",
      "  0.0324204  -0.050578    0.00052144  0.0363153  -0.0195645   0.0215961\n",
      " -0.0328725  -0.0382125  -0.0223833   0.0707165  -0.00495237 -0.0170337\n",
      " -0.0189592  -0.030699    0.00475685 -0.11388     0.00703411 -0.0335945\n",
      "  0.0542836   0.00303202  0.119817    0.00362914  0.0893979   0.0317403\n",
      "  0.0637864  -0.0625717  -0.0565936  -0.0447092  -0.0246564   0.0164475\n",
      " -0.0256783  -0.0460263  -0.0280003  -0.0257592  -0.0750533   0.00175455\n",
      "  0.0693277   0.00044122  0.0230715   0.0515744   0.0373765   0.0747018\n",
      "  0.0590913   0.0371957  -0.0700579   0.100119    0.00769607  0.0850133\n",
      "  0.110065    0.044774    0.0295887   0.0720199  -0.0329998   0.0078851\n",
      " -0.047183    0.103046    0.0535568  -0.0169818  -0.0143275   0.0375983\n",
      " -0.00749953 -0.027453   -0.0172941   0.0893228   0.0819936  -0.0333331\n",
      "  0.0478825   0.0383251  -0.111027   -0.0116323  -0.0120414   0.0167171\n",
      " -0.0258943  -0.118493    0.00547238 -0.016471   -0.00727979 -0.0450163\n",
      "  0.0393726   0.00821027  0.150397    0.0129688   0.0491381   0.0994944\n",
      " -0.0824815  -0.00167348  0.0531713   0.0347765  -0.0354589  -0.0981227\n",
      "  0.09714     0.142637   -0.014434   -0.0175022  -0.033631    0.116323\n",
      " -0.0381954   0.06726    -0.008617   -0.0237689   0.013221    0.0206963\n",
      " -0.0307017   0.00295897  0.0513253   0.0870196   0.0951917   0.0466404\n",
      "  0.018923    0.0204455   0.113215   -0.0515061  -0.0103162  -0.0607598\n",
      " -0.0592278   0.00244356 -0.00028886 -0.0934447  -0.0173852   0.0599409\n",
      " -0.0258571  -0.0285261   0.050967   -0.0596646  -0.0388744  -0.0426005\n",
      " -0.017882   -0.0561637  -0.0390416   0.0643562   0.0848427   0.0187599\n",
      "  0.0498308   0.0117896   0.0230493   0.0741695   0.0349198  -0.0116354\n",
      " -0.0478142  -0.0712487  -0.0211791  -0.0189919  -0.00788066 -0.0128614\n",
      " -0.082461   -0.0309238  -0.0604459   0.070464   -0.0278242   0.00524581\n",
      " -0.00461935 -0.0202063  -0.0667345  -0.0334846  -0.0567267  -0.0687306\n",
      " -0.113235    0.0557713  -0.0340623  -0.100624    0.0291703   0.00734257\n",
      " -0.00059941 -0.00881149  0.0671474  -0.0225951  -0.0108086   0.171859\n",
      "  0.096072   -0.0192335   0.0573614   0.00518269  0.013411    0.0656494\n",
      " -0.0833652  -0.0489539   0.00559965  0.0251996  -0.0277905  -0.0732926\n",
      "  0.0428734  -0.0544269   0.0114016   0.0998527  -0.0774145   0.00432488\n",
      " -0.117145   -0.0323712  -0.0557099   0.129402   -0.0403655   0.0411264\n",
      "  0.0601764   0.105776    0.0527379  -0.0144333   0.0037414  -0.0233015\n",
      "  0.0728081   0.0440506  -0.0686146   0.0457431   0.0691469   0.00605824\n",
      "  0.0349505   0.0434057  -0.0166202   0.0426858   0.0307549   0.0522636\n",
      " -0.172674    0.135554    0.0268552   0.029893   -0.0305673   0.0626126\n",
      "  0.0385742   0.017581   -0.0933594  -0.0207052   0.128395   -0.0459614 ]\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print(word_vec_dict['is'])\n",
    "print(word_vec_dict['are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 5293\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(SimpleRNN(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76680 samples, validate on 19171 samples\n",
      "Epoch 1/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.9684Epoch 00001: val_loss improved from inf to 0.06171, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 99s 1ms/step - loss: 0.1056 - acc: 0.9684 - val_loss: 0.0617 - val_acc: 0.9786\n",
      "Epoch 2/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0662 - acc: 0.9771Epoch 00002: val_loss improved from 0.06171 to 0.05770, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 93s 1ms/step - loss: 0.0662 - acc: 0.9771 - val_loss: 0.0577 - val_acc: 0.9801\n",
      "Epoch 3/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9789Epoch 00003: val_loss improved from 0.05770 to 0.05405, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 89s 1ms/step - loss: 0.0601 - acc: 0.9789 - val_loss: 0.0540 - val_acc: 0.9807\n",
      "Epoch 4/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0571 - acc: 0.9795Epoch 00004: val_loss improved from 0.05405 to 0.05240, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 90s 1ms/step - loss: 0.0571 - acc: 0.9795 - val_loss: 0.0524 - val_acc: 0.9813\n",
      "Epoch 5/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9801Epoch 00005: val_loss improved from 0.05240 to 0.05163, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 91s 1ms/step - loss: 0.0552 - acc: 0.9801 - val_loss: 0.0516 - val_acc: 0.9812\n",
      "Epoch 6/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9799Epoch 00006: val_loss did not improve\n",
      "76680/76680 [==============================] - 89s 1ms/step - loss: 0.0553 - acc: 0.9799 - val_loss: 0.0604 - val_acc: 0.9797\n",
      "Epoch 7/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9803Epoch 00007: val_loss improved from 0.05163 to 0.04976, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0542 - acc: 0.9803 - val_loss: 0.0498 - val_acc: 0.9818\n",
      "Epoch 8/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9810Epoch 00008: val_loss did not improve\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0516 - acc: 0.9810 - val_loss: 0.0501 - val_acc: 0.9815\n",
      "Epoch 9/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9812Epoch 00009: val_loss improved from 0.04976 to 0.04916, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 90s 1ms/step - loss: 0.0501 - acc: 0.9812 - val_loss: 0.0492 - val_acc: 0.9824\n",
      "Epoch 10/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9816Epoch 00010: val_loss improved from 0.04916 to 0.04758, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0493 - acc: 0.9816 - val_loss: 0.0476 - val_acc: 0.9821\n",
      "Epoch 11/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9822Epoch 00011: val_loss did not improve\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0479 - acc: 0.9822 - val_loss: 0.0479 - val_acc: 0.9829\n",
      "Epoch 12/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9820Epoch 00012: val_loss improved from 0.04758 to 0.04555, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0475 - acc: 0.9820 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "Epoch 13/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9824Epoch 00013: val_loss did not improve\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0460 - acc: 0.9824 - val_loss: 0.0457 - val_acc: 0.9830\n",
      "Epoch 14/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9826Epoch 00014: val_loss did not improve\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0456 - acc: 0.9826 - val_loss: 0.0463 - val_acc: 0.9827\n",
      "Epoch 15/15\n",
      "76672/76680 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9828Epoch 00015: val_loss improved from 0.04555 to 0.04507, saving model to weights_base.best.h5\n",
      "76680/76680 [==============================] - 88s 1ms/step - loss: 0.0448 - acc: 0.9828 - val_loss: 0.0451 - val_acc: 0.9829\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.1070 - acc: 0.9673Epoch 00001: val_loss improved from inf to 0.06642, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 90s 1ms/step - loss: 0.1069 - acc: 0.9673 - val_loss: 0.0664 - val_acc: 0.9775\n",
      "Epoch 2/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0645 - acc: 0.9777- ETA: Epoch 00002: val_loss improved from 0.06642 to 0.05346, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0645 - acc: 0.9777 - val_loss: 0.0535 - val_acc: 0.9807\n",
      "Epoch 3/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9788Epoch 00003: val_loss did not improve\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0600 - acc: 0.9788 - val_loss: 0.0568 - val_acc: 0.9802\n",
      "Epoch 4/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0583 - acc: 0.9794Epoch 00004: val_loss improved from 0.05346 to 0.05127, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0583 - acc: 0.9794 - val_loss: 0.0513 - val_acc: 0.9812\n",
      "Epoch 5/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9797- ETA: 1s - Epoch 00005: val_loss did not improve\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0565 - acc: 0.9797 - val_loss: 0.0515 - val_acc: 0.9817\n",
      "Epoch 6/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9803Epoch 00006: val_loss did not improve\n",
      "76681/76681 [==============================] - 87s 1ms/step - loss: 0.0549 - acc: 0.9803 - val_loss: 0.0531 - val_acc: 0.9814\n",
      "Epoch 7/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9806Epoch 00007: val_loss improved from 0.05127 to 0.04825, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0536 - acc: 0.9806 - val_loss: 0.0482 - val_acc: 0.9824\n",
      "Epoch 8/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9808Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 87s 1ms/step - loss: 0.0523 - acc: 0.9808 - val_loss: 0.0485 - val_acc: 0.9825\n",
      "Epoch 9/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9813Epoch 00009: val_loss improved from 0.04825 to 0.04665, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0507 - acc: 0.9813 - val_loss: 0.0466 - val_acc: 0.9830\n",
      "Epoch 10/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9815Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 87s 1ms/step - loss: 0.0495 - acc: 0.9815 - val_loss: 0.0470 - val_acc: 0.9830\n",
      "Epoch 11/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0487 - acc: 0.9818Epoch 00011: val_loss improved from 0.04665 to 0.04612, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 87s 1ms/step - loss: 0.0487 - acc: 0.9818 - val_loss: 0.0461 - val_acc: 0.9828\n",
      "Epoch 12/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.981 - ETA: 0s - loss: 0.0481 - acc: 0.9819Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 87s 1ms/step - loss: 0.0480 - acc: 0.9819 - val_loss: 0.0469 - val_acc: 0.9827\n",
      "Epoch 13/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9823Epoch 00013: val_loss did not improve\n",
      "76681/76681 [==============================] - 88s 1ms/step - loss: 0.0471 - acc: 0.9822 - val_loss: 0.0468 - val_acc: 0.9827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9823Epoch 00014: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0466 - acc: 0.9823 - val_loss: 0.0477 - val_acc: 0.9828\n",
      "Epoch 15/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9825- ETA: 0s - loss: 0.0460 - acc: 0Epoch 00015: val_loss improved from 0.04612 to 0.04565, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0457 - val_acc: 0.9825\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9688- EEpoch 00001: val_loss improved from inf to 0.06180, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.1022 - acc: 0.9688 - val_loss: 0.0618 - val_acc: 0.9784\n",
      "Epoch 2/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9770- ETA: 8s - loss: 0.0667 -Epoch 00002: val_loss improved from 0.06180 to 0.05994, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0664 - acc: 0.9770 - val_loss: 0.0599 - val_acc: 0.9793\n",
      "Epoch 3/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9784Epoch 00003: val_loss improved from 0.05994 to 0.05609, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0617 - acc: 0.9784 - val_loss: 0.0561 - val_acc: 0.9799\n",
      "Epoch 4/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0579 - acc: 0.9794Epoch 00004: val_loss improved from 0.05609 to 0.05295, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0579 - acc: 0.9793 - val_loss: 0.0529 - val_acc: 0.9808\n",
      "Epoch 5/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0561 - acc: 0.9799- ETA: 0s - loss: 0.0561 - acc:Epoch 00005: val_loss improved from 0.05295 to 0.05138, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0561 - acc: 0.9799 - val_loss: 0.0514 - val_acc: 0.9812\n",
      "Epoch 6/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0543 - acc: 0.9803- ET - ETA: 7s - loss: 0.0545 - acc: - ETA: 6s - loss:  - ETA: 5s - loss: 0.0544 - acc: 0Epoch 00006: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0543 - acc: 0.9803 - val_loss: 0.0545 - val_acc: 0.9788\n",
      "Epoch 7/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0525 - acc: 0.9808- ETA: 4s - lEpoch 00007: val_loss improved from 0.05138 to 0.04922, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0525 - acc: 0.9808 - val_loss: 0.0492 - val_acc: 0.9816\n",
      "Epoch 8/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9810Epoch 00008: val_loss improved from 0.04922 to 0.04903, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0512 - acc: 0.9810 - val_loss: 0.0490 - val_acc: 0.9819\n",
      "Epoch 9/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9815Epoch 00009: val_loss improved from 0.04903 to 0.04689, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0495 - acc: 0.9815 - val_loss: 0.0469 - val_acc: 0.9825\n",
      "Epoch 10/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9817Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0491 - acc: 0.9817 - val_loss: 0.0472 - val_acc: 0.9827\n",
      "Epoch 11/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9820Epoch 00011: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0483 - acc: 0.9820 - val_loss: 0.0471 - val_acc: 0.9824\n",
      "Epoch 12/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9824Epoch 00012: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0470 - acc: 0.9824 - val_loss: 0.0477 - val_acc: 0.9824\n",
      "Epoch 13/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9824Epoch 00013: val_loss improved from 0.04689 to 0.04590, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0467 - acc: 0.9824 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "Epoch 14/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9825Epoch 00014: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0463 - acc: 0.9825 - val_loss: 0.0460 - val_acc: 0.9826\n",
      "Epoch 15/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9826Epoch 00015: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0456 - acc: 0.9826 - val_loss: 0.0487 - val_acc: 0.9826\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.1048 - acc: 0.9686- ETA: 0s - loss: 0.1051 - aEpoch 00001: val_loss improved from inf to 0.05966, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.1048 - acc: 0.9686 - val_loss: 0.0597 - val_acc: 0.9790\n",
      "Epoch 2/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0651 - acc: 0.9777- ETA: 4sEpoch 00002: val_loss improved from 0.05966 to 0.05615, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0651 - acc: 0.9777 - val_loss: 0.0561 - val_acc: 0.9798\n",
      "Epoch 3/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9786- ETA: 1s - loss: 0.06Epoch 00003: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0617 - acc: 0.9786 - val_loss: 0.0596 - val_acc: 0.9793\n",
      "Epoch 4/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9787Epoch 00004: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0607 - acc: 0.9787 - val_loss: 0.0566 - val_acc: 0.9790\n",
      "Epoch 5/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0576 - acc: 0.9795- ETA: 0s - loss: 0.0576 - Epoch 00005: val_loss improved from 0.05615 to 0.05309, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0576 - acc: 0.9795 - val_loss: 0.0531 - val_acc: 0.9805\n",
      "Epoch 6/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9801Epoch 00006: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0553 - acc: 0.9801 - val_loss: 0.0549 - val_acc: 0.9801\n",
      "Epoch 7/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9806Epoch 00007: val_loss improved from 0.05309 to 0.04973, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0531 - acc: 0.9806 - val_loss: 0.0497 - val_acc: 0.9813\n",
      "Epoch 8/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9809Epoch 00008: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0518 - acc: 0.9809 - val_loss: 0.0504 - val_acc: 0.9810\n",
      "Epoch 9/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9812Epoch 00009: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0503 - acc: 0.9812 - val_loss: 0.0510 - val_acc: 0.9812\n",
      "Epoch 10/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9820- ETA: 1s - loss: 0.0489 - - ETA: 1s - loss: 0.0Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0488 - acc: 0.9820 - val_loss: 0.0502 - val_acc: 0.9818\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9819Epoch 00011: val_loss improved from 0.04973 to 0.04756, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0478 - acc: 0.9819 - val_loss: 0.0476 - val_acc: 0.9822\n",
      "Epoch 12/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9823Epoch 00012: val_loss improved from 0.04756 to 0.04703, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0467 - acc: 0.9823 - val_loss: 0.0470 - val_acc: 0.9820\n",
      "Epoch 13/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9825- ETA: 0s - loss: 0.0461 - acc: 0.Epoch 00013: val_loss improved from 0.04703 to 0.04671, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0462 - acc: 0.9825 - val_loss: 0.0467 - val_acc: 0.9825\n",
      "Epoch 14/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9823- ETEpoch 00014: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0475 - acc: 0.9823 - val_loss: 0.0469 - val_acc: 0.9825\n",
      "Epoch 15/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9825Epoch 00015: val_loss improved from 0.04671 to 0.04573, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0458 - acc: 0.9825 - val_loss: 0.0457 - val_acc: 0.9828\n",
      "Train on 76681 samples, validate on 19170 samples\n",
      "Epoch 1/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9685- ETA: 2s - los - ETA: 0s - loss: 0.1056 -Epoch 00001: val_loss improved from inf to 0.06057, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 87s 1ms/step - loss: 0.1052 - acc: 0.9685 - val_loss: 0.0606 - val_acc: 0.9793\n",
      "Epoch 2/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9778- ETA: 0s - loss: 0.0644 - acc: 0.Epoch 00002: val_loss improved from 0.06057 to 0.05755, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0643 - acc: 0.9778 - val_loss: 0.0576 - val_acc: 0.9794\n",
      "Epoch 3/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9781- ETA: 3s - loss: 0Epoch 00003: val_loss improved from 0.05755 to 0.05575, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0631 - acc: 0.9781 - val_loss: 0.0557 - val_acc: 0.9802\n",
      "Epoch 4/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0604 - acc: 0.9788Epoch 00004: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0604 - acc: 0.9788 - val_loss: 0.0565 - val_acc: 0.9793\n",
      "Epoch 5/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9797Epoch 00005: val_loss improved from 0.05575 to 0.05231, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0566 - acc: 0.9797 - val_loss: 0.0523 - val_acc: 0.9808\n",
      "Epoch 6/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9804- ETA: 7s - loss: 0.0546  - ETA: 4s -Epoch 00006: val_loss improved from 0.05231 to 0.05146, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0546 - acc: 0.9804 - val_loss: 0.0515 - val_acc: 0.9810\n",
      "Epoch 7/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9803- ETA: 1s - loss: 0.0Epoch 00007: val_loss did not improve\n",
      "76681/76681 [==============================] - 85s 1ms/step - loss: 0.0539 - acc: 0.9803 - val_loss: 0.0525 - val_acc: 0.9806\n",
      "Epoch 8/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0523 - acc: 0.9809- ETA: 0s - loss: 0.0523 - acc: 0.9Epoch 00008: val_loss improved from 0.05146 to 0.05015, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0523 - acc: 0.9809 - val_loss: 0.0502 - val_acc: 0.9814\n",
      "Epoch 9/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9811- ETA: 0s - loss: 0.0516 - acc: 0Epoch 00009: val_loss improved from 0.05015 to 0.04988, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0516 - acc: 0.9811 - val_loss: 0.0499 - val_acc: 0.9816\n",
      "Epoch 10/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0503 - acc: 0.9814Epoch 00010: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0503 - acc: 0.9814 - val_loss: 0.0524 - val_acc: 0.9799\n",
      "Epoch 11/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0493 - acc: 0.9815Epoch 00011: val_loss improved from 0.04988 to 0.04789, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0493 - acc: 0.9815 - val_loss: 0.0479 - val_acc: 0.9820\n",
      "Epoch 12/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9818- ETA: 0s - loss: 0.0482 - acc: Epoch 00012: val_loss improved from 0.04789 to 0.04648, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0482 - acc: 0.9818 - val_loss: 0.0465 - val_acc: 0.9825\n",
      "Epoch 13/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9822- ETA: 2s - loss - ETA: 0s - loss: 0.0472 -Epoch 00013: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0473 - acc: 0.9822 - val_loss: 0.0469 - val_acc: 0.9823\n",
      "Epoch 14/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9824Epoch 00014: val_loss did not improve\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0463 - acc: 0.9824 - val_loss: 0.0476 - val_acc: 0.9821\n",
      "Epoch 15/15\n",
      "76672/76681 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9826Epoch 00015: val_loss improved from 0.04648 to 0.04626, saving model to weights_base.best.h5\n",
      "76681/76681 [==============================] - 86s 1ms/step - loss: 0.0457 - acc: 0.9826 - val_loss: 0.0463 - val_acc: 0.9825\n",
      "-------------------------------\n",
      "0 0.100560586265 0.962180884915\n",
      "1 0.0243937633578 0.989921857884\n",
      "2 0.0518553296575 0.980720075951\n",
      "3 0.0102360180558 0.9968701422\n",
      "4 0.0662744664814 0.972989327185\n",
      "5 0.0210332820899 0.993051715684\n",
      "final 0.0457255743179 0.98262233397\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 15\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(5,2333)\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/simple_rnn_muse_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/simple_rnn_muse_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.376,  0.005,  0.097,  0.023,  0.1  ,  0.004],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.01 ,  0.   ,  0.001,  0.   ,  0.002,  0.002],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.404,  0.013,  0.187,  0.005,  0.189,  0.009],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.116,  0.001,  0.02 ,  0.027,  0.025,  0.009],\n",
       "       [ 0.061,  0.   ,  0.008,  0.001,  0.014,  0.008],\n",
       "       [ 0.007,  0.   ,  0.002,  0.   ,  0.002,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
