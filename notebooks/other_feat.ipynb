{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "train_df['comment_text'] = train_df['comment_text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "test_df['comment_text'] =test_df['comment_text'].str.replace('[^a-zA-Z0-9]', ' ')\n",
    "\n",
    "## Number of words in the comment_text ##\n",
    "train_df[\"num_words\"] = train_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "test_df[\"num_words\"] = test_df[\"comment_text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "## Number of unique words in the comment_text ##\n",
    "train_df[\"num_unique_words\"] = train_df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "test_df[\"num_unique_words\"] = test_df[\"comment_text\"].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "## Number of characters in the comment_text ##\n",
    "train_df[\"num_chars\"] = train_df[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "test_df[\"num_chars\"] = test_df[\"comment_text\"].apply(lambda x: len(str(x)))\n",
    "\n",
    "## asterix_freq\n",
    "train_df[\"asterix_freq\"] = train_df[\"comment_text\"].apply(lambda x: x.count('!')/len(x))\n",
    "test_df[\"asterix_freq\"] = test_df[\"comment_text\"].apply(lambda x: x.count('!')/len(x))\n",
    "\n",
    "## Number of stopwords in the comment_text ##\n",
    "eng_stopwords = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"]\n",
    "train_df[\"num_stopwords\"] = train_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "test_df[\"num_stopwords\"] = test_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n",
    "\n",
    "## Number of punctuations in the comment_text ##\n",
    "import string\n",
    "train_df[\"num_punctuations\"] =train_df['comment_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "test_df[\"num_punctuations\"] =test_df['comment_text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n",
    "\n",
    "## Number of title case words in the comment_text ##\n",
    "train_df[\"num_words_upper\"] = train_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "test_df[\"num_words_upper\"] = test_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n",
    "\n",
    "## Number of title case words in the comment_text ##\n",
    "train_df[\"num_words_title\"] = train_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "test_df[\"num_words_title\"] = test_df[\"comment_text\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n",
    "\n",
    "## Average length of the words in the comment_text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "## Average length of the words in the comment_text ##\n",
    "train_df[\"mean_word_len\"] = train_df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "test_df[\"mean_word_len\"] = test_df[\"comment_text\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train feat done\n",
      "test feat done\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/emotionevil/general-oof-class-for-stacking\n",
    "# Thanks olivier for his text clean  \n",
    "# https://www.kaggle.com/ogrellier/lgbm-with-words-and-chars-n-gram?scriptVersionId=2694282\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "    (b'&lt;3', b' heart '),\n",
    "    (b':d', b' smile '),\n",
    "    (b':dd', b' smile '),\n",
    "    (b':p', b' smile '),\n",
    "    (b'8\\)', b' smile '),\n",
    "    (b':-\\)', b' smile '),\n",
    "    (b':\\)', b' smile '),\n",
    "    (b';\\)', b' smile '),\n",
    "    (b'\\(-:', b' smile '),\n",
    "    (b'\\(:', b' smile '),\n",
    "    (b'yay!', b' good '),\n",
    "    (b'yay', b' good '),\n",
    "    (b'yaay', b' good '),\n",
    "    (b':/', b' worry '),\n",
    "    (b':&gt;', b' angry '),\n",
    "    (b\":'\\)\", b' sad '),\n",
    "    (b':-\\(', b' sad '),\n",
    "    (b':\\(', b' sad '),\n",
    "    (b':s', b' sad '),\n",
    "    (b':-s', b' sad '),\n",
    "    (b'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', b' '),\n",
    "    (b'(\\[[\\s\\S]*\\])', b' '),\n",
    "    (b'[\\s]*?(www.[\\S]*)', b' ')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def prepare_for_char_n_gram(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    \n",
    "    # replace words like hhhhhhhhhhhhhhi with hi\n",
    "    for ch in string.ascii_lowercase:\n",
    "        pattern = bytes(ch+'{3,}', encoding=\"utf-8\")\n",
    "        clean = re.sub(pattern, bytes(ch, encoding=\"utf-8\"), clean)\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "\n",
    "def count_regexp_occ(regexp=\"\", text=None):\n",
    "    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(re.findall(regexp, text)) / len(text)\n",
    "\n",
    "\n",
    "def get_indicators_and_clean_comments(df):\n",
    "    \"\"\"\n",
    "    Check all sorts of content as it may help find toxic comment\n",
    "    Though I'm not sure all of them improve scores\n",
    "    \"\"\"\n",
    "    # Count number of \\n\n",
    "    df[\"ant_slash_n\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\n\", x))\n",
    "    # Get length in words and characters\n",
    "    # df[\"raw_word_len\"] = df[\"comment_text\"].apply(lambda x: len(x.split()))\n",
    "    # df[\"raw_char_len\"] = df[\"comment_text\"].apply(lambda x: len(x))\n",
    "    # Check number of upper case, if you're angry you may write in upper case\n",
    "    df[\"nb_upper\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\n",
    "    # Number of F words - f..k contains folk, fork,\n",
    "    df[\"nb_fk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ff]\\S{2}[Kk]\", x))\n",
    "    # Number of S word\n",
    "    df[\"nb_sk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[Ss]\\S{2}[Kk]\", x))\n",
    "    # Number of D words\n",
    "    df[\"nb_dk\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"[dD]ick\", x))\n",
    "    # Number of occurence of You, insulting someone usually needs someone called : you\n",
    "    df[\"nb_you\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\W[Yy]ou\\W\", x))\n",
    "    # Just to check you really refered to my mother ;-)\n",
    "    df[\"nb_mother\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wmother\\W\", x))\n",
    "    # Just checking for toxic 19th century vocabulary\n",
    "    df[\"nb_ng\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\Wnigger\\W\", x))\n",
    "    # Some Sentences start with a <:> so it may help\n",
    "    df[\"start_with_columns\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"^\\:+\", x))\n",
    "    # Check for time stamp\n",
    "    df[\"has_timestamp\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\d{2}|:\\d{2}\", x))\n",
    "    # Check for dates 18:44, 8 December 2010\n",
    "    df[\"has_date_long\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{2}:\\d{2}, \\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for date short 8 December 2010\n",
    "    df[\"has_date_short\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\D\\d{1,2} \\w+ \\d{4}\", x))\n",
    "    # Check for http links\n",
    "    df[\"has_http\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\n",
    "    # check for mail\n",
    "    df[\"has_mail\"] = df[\"comment_text\"].apply(\n",
    "        lambda x: count_regexp_occ(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', x)\n",
    "    )\n",
    "    # Looking for words surrounded by == word == or \"\"\"\" word \"\"\"\"\n",
    "    df[\"has_emphasize_equal\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\={2}.+\\={2}\", x))\n",
    "    df[\"has_emphasize_quotes\"] = df[\"comment_text\"].apply(lambda x: count_regexp_occ(r\"\\\"{4}\\S+\\\"{4}\", x))\n",
    "\n",
    "    # Now clean comments\n",
    "    df[\"clean_comment\"] = df[\"comment_text\"].apply(lambda x: prepare_for_char_n_gram(x))\n",
    "\n",
    "    # Get the new length in words and characters\n",
    "    df[\"clean_word_len\"] = df[\"clean_comment\"].apply(lambda x: len(x.split()))\n",
    "    df[\"clean_char_len\"] = df[\"clean_comment\"].apply(lambda x: len(x))\n",
    "    # Number of different characters used in a comment\n",
    "    # Using the f word only will reduce the number of letters required in the comment\n",
    "    df[\"clean_chars\"] = df[\"clean_comment\"].apply(lambda x: len(set(x)))\n",
    "    df[\"clean_chars_ratio\"] = df[\"clean_comment\"].apply(lambda x: len(set(x))) / df[\"clean_comment\"].apply(\n",
    "        lambda x: 1 + min(99, len(x)))\n",
    "\n",
    "# add features\n",
    "get_indicators_and_clean_comments(train_df)\n",
    "print('train feat done')\n",
    "get_indicators_and_clean_comments(test_df)\n",
    "print('test feat done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
      "       'insult', 'identity_hate', 'num_words', 'num_unique_words', 'num_chars',\n",
      "       'asterix_freq', 'num_stopwords', 'num_punctuations', 'num_words_upper',\n",
      "       'num_words_title', 'mean_word_len', 'ant_slash_n', 'nb_upper', 'nb_fk',\n",
      "       'nb_sk', 'nb_dk', 'nb_you', 'nb_mother', 'nb_ng', 'start_with_columns',\n",
      "       'has_timestamp', 'has_date_long', 'has_date_short', 'has_http',\n",
      "       'has_mail', 'has_emphasize_equal', 'has_emphasize_quotes',\n",
      "       'clean_comment', 'clean_word_len', 'clean_char_len', 'clean_chars',\n",
      "       'clean_chars_ratio', 'unique_r', 'w_p', 'w_p_r', 'stop_r', 'w_p_stop',\n",
      "       'w_p_stop_r', 'num_words_upper_r', 'num_words_title_r'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# add features\n",
    "def add_feat(df):\n",
    "    df['unique_r'] = df['num_unique_words'] / df['num_words']\n",
    "    df['w_p'] = df['num_words'] - df['num_punctuations']\n",
    "    df['w_p_r'] = df['w_p'] / df['num_words']\n",
    "    df['stop_r'] = df['num_stopwords'] / df['num_words']\n",
    "    df['w_p_stop'] = df['w_p'] - df['num_stopwords']\n",
    "    df['w_p_stop_r'] = df['w_p_stop'] / df['num_words']\n",
    "    df['num_words_upper_r'] = df['num_words_upper'] / df['num_words']\n",
    "    df['num_words_title_r'] = df['num_words_title'] / df['num_words']\n",
    "\n",
    "add_feat(train_df)\n",
    "add_feat(test_df)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_words  num_unique_words  num_chars  asterix_freq  num_stopwords  \\\n",
      "0         50                46        264           0.0             20   \n",
      "1         20                20        112           0.0              4   \n",
      "2         44                41        233           0.0             21   \n",
      "3        116                84        622           0.0             70   \n",
      "4         14                14         67           0.0              8   \n",
      "\n",
      "   num_punctuations  num_words_upper  num_words_title  mean_word_len  \\\n",
      "0                 0                3               12       4.240000   \n",
      "1                 0                3                5       4.150000   \n",
      "2                 0                1                4       4.227273   \n",
      "3                 0                5               11       4.189655   \n",
      "4                 0                0                2       3.571429   \n",
      "\n",
      "   ant_slash_n        ...          clean_chars  clean_chars_ratio  unique_r  \\\n",
      "0          0.0        ...                   25           0.250000  0.920000   \n",
      "1          0.0        ...                   22           0.220000  1.000000   \n",
      "2          0.0        ...                   25           0.250000  0.931818   \n",
      "3          0.0        ...                   25           0.250000  0.724138   \n",
      "4          0.0        ...                   19           0.206522  1.000000   \n",
      "\n",
      "   w_p  w_p_r    stop_r  w_p_stop  w_p_stop_r  num_words_upper_r  \\\n",
      "0   50    1.0  0.400000        30    0.600000           0.060000   \n",
      "1   20    1.0  0.200000        16    0.800000           0.150000   \n",
      "2   44    1.0  0.477273        23    0.522727           0.022727   \n",
      "3  116    1.0  0.603448        46    0.396552           0.043103   \n",
      "4   14    1.0  0.571429         6    0.428571           0.000000   \n",
      "\n",
      "   num_words_title_r  \n",
      "0           0.240000  \n",
      "1           0.250000  \n",
      "2           0.090909  \n",
      "3           0.094828  \n",
      "4           0.142857  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "train_df=train_df.drop(['id','comment_text','toxic', 'severe_toxic', 'obscene', 'threat',\n",
    "       'insult', 'identity_hate','clean_comment'],axis=1)\n",
    "print(train_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_words  num_unique_words  num_chars  asterix_freq  num_stopwords  \\\n",
      "0         75                63        367           0.0             34   \n",
      "1         10                 9         50           0.0              6   \n",
      "2          5                 5         54           0.0              1   \n",
      "3         39                29        205           0.0             23   \n",
      "4          8                 8         41           0.0              3   \n",
      "\n",
      "   num_punctuations  num_words_upper  num_words_title  mean_word_len  \\\n",
      "0                 0                0                4       3.786667   \n",
      "1                 0                1                2       3.000000   \n",
      "2                 0                0                4       5.200000   \n",
      "3                 0                3                4       4.153846   \n",
      "4                 0                1                1       4.125000   \n",
      "\n",
      "   ant_slash_n        ...          clean_chars  clean_chars_ratio  unique_r  \\\n",
      "0          0.0        ...                   26           0.260000   0.84000   \n",
      "1          0.0        ...                   15           0.250000   0.90000   \n",
      "2          0.0        ...                   17           0.414634   1.00000   \n",
      "3          0.0        ...                   24           0.240000   0.74359   \n",
      "4          0.0        ...                   16           0.280702   1.00000   \n",
      "\n",
      "   w_p  w_p_r    stop_r  w_p_stop  w_p_stop_r  num_words_upper_r  \\\n",
      "0   75    1.0  0.453333        41    0.546667           0.000000   \n",
      "1   10    1.0  0.600000         4    0.400000           0.100000   \n",
      "2    5    1.0  0.200000         4    0.800000           0.000000   \n",
      "3   39    1.0  0.589744        16    0.410256           0.076923   \n",
      "4    8    1.0  0.375000         5    0.625000           0.125000   \n",
      "\n",
      "   num_words_title_r  \n",
      "0           0.053333  \n",
      "1           0.200000  \n",
      "2           0.800000  \n",
      "3           0.102564  \n",
      "4           0.125000  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "test_df = test_df.drop(['id','comment_text','clean_comment'],axis=1)\n",
    "print(test_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('../features/other_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_df.values,test_df.values],fout)\n",
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
