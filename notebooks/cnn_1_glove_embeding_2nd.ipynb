{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dakhla 176361\n",
      "coalfield 116521\n",
      "preeclampsia 37803\n",
      "mundy 97592\n",
      "rickbartolucci 76936\n",
      "inertness 147032\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(256,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106380 samples, validate on 53191 samples\n",
      "Epoch 1/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0589 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.04581, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 30s 278us/step - loss: 0.0589 - acc: 0.9791 - val_loss: 0.0458 - val_acc: 0.9822\n",
      "Epoch 2/10\n",
      "106368/106380 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9818Epoch 00002: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 256us/step - loss: 0.0476 - acc: 0.9818 - val_loss: 0.0458 - val_acc: 0.9817\n",
      "Epoch 3/10\n",
      "106240/106380 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9824Epoch 00003: val_loss improved from 0.04581 to 0.04432, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 27s 250us/step - loss: 0.0454 - acc: 0.9824 - val_loss: 0.0443 - val_acc: 0.9827\n",
      "Epoch 4/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9831Epoch 00004: val_loss improved from 0.04432 to 0.04395, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 27s 251us/step - loss: 0.0429 - acc: 0.9831 - val_loss: 0.0440 - val_acc: 0.9827\n",
      "Epoch 5/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9838Epoch 00005: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 251us/step - loss: 0.0411 - acc: 0.9838 - val_loss: 0.0444 - val_acc: 0.9825\n",
      "Epoch 6/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9842Epoch 00006: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 251us/step - loss: 0.0392 - acc: 0.9841 - val_loss: 0.0441 - val_acc: 0.9831\n",
      "Epoch 7/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9847Epoch 00007: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 251us/step - loss: 0.0376 - acc: 0.9847 - val_loss: 0.0447 - val_acc: 0.9830\n",
      "Epoch 8/10\n",
      "106240/106380 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9851Epoch 00008: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 254us/step - loss: 0.0366 - acc: 0.9851 - val_loss: 0.0446 - val_acc: 0.9826\n",
      "Epoch 9/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9855Epoch 00009: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 251us/step - loss: 0.0357 - acc: 0.9855 - val_loss: 0.0449 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9859Epoch 00010: val_loss did not improve\n",
      "106380/106380 [==============================] - 27s 251us/step - loss: 0.0345 - acc: 0.9859 - val_loss: 0.0468 - val_acc: 0.9822\n",
      "Train on 106381 samples, validate on 53190 samples\n",
      "Epoch 1/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9790Epoch 00001: val_loss improved from inf to 0.04827, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 256us/step - loss: 0.0586 - acc: 0.9790 - val_loss: 0.0483 - val_acc: 0.9810\n",
      "Epoch 2/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9816Epoch 00002: val_loss improved from 0.04827 to 0.04422, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0476 - acc: 0.9816 - val_loss: 0.0442 - val_acc: 0.9830\n",
      "Epoch 3/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9824Epoch 00003: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0451 - acc: 0.9824 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Epoch 4/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9832Epoch 00004: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0426 - acc: 0.9832 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Epoch 5/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9838Epoch 00005: val_loss improved from 0.04422 to 0.04344, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 254us/step - loss: 0.0409 - acc: 0.9838 - val_loss: 0.0434 - val_acc: 0.9832\n",
      "Epoch 6/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9843Epoch 00006: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0392 - acc: 0.9843 - val_loss: 0.0442 - val_acc: 0.9829\n",
      "Epoch 7/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9847Epoch 00007: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 253us/step - loss: 0.0379 - acc: 0.9847 - val_loss: 0.0451 - val_acc: 0.9830\n",
      "Epoch 8/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9852Epoch 00008: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 254us/step - loss: 0.0363 - acc: 0.9852 - val_loss: 0.0453 - val_acc: 0.9829\n",
      "Epoch 9/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9857Epoch 00009: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 255us/step - loss: 0.0353 - acc: 0.9857 - val_loss: 0.0452 - val_acc: 0.9829\n",
      "Epoch 10/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9859Epoch 00010: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 255us/step - loss: 0.0341 - acc: 0.9859 - val_loss: 0.0467 - val_acc: 0.9827\n",
      "Train on 106381 samples, validate on 53190 samples\n",
      "Epoch 1/10\n",
      "106240/106381 [============================>.] - ETA: 0s - loss: 0.0587 - acc: 0.9790Epoch 00001: val_loss improved from inf to 0.04673, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 256us/step - loss: 0.0587 - acc: 0.9790 - val_loss: 0.0467 - val_acc: 0.9821\n",
      "Epoch 2/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0474 - acc: 0.9817Epoch 00002: val_loss improved from 0.04673 to 0.04574, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 251us/step - loss: 0.0475 - acc: 0.9817 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "Epoch 3/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9826Epoch 00003: val_loss improved from 0.04574 to 0.04476, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 250us/step - loss: 0.0448 - acc: 0.9825 - val_loss: 0.0448 - val_acc: 0.9828\n",
      "Epoch 4/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9832Epoch 00004: val_loss improved from 0.04476 to 0.04371, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0424 - acc: 0.9832 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "Epoch 5/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9838Epoch 00005: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 253us/step - loss: 0.0406 - acc: 0.9838 - val_loss: 0.0446 - val_acc: 0.9828\n",
      "Epoch 6/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9844Epoch 00006: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 251us/step - loss: 0.0387 - acc: 0.9844 - val_loss: 0.0448 - val_acc: 0.9830\n",
      "Epoch 7/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9847Epoch 00007: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0377 - acc: 0.9847 - val_loss: 0.0448 - val_acc: 0.9829\n",
      "Epoch 8/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9851Epoch 00008: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 253us/step - loss: 0.0365 - acc: 0.9851 - val_loss: 0.0459 - val_acc: 0.9829\n",
      "Epoch 9/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9855Epoch 00009: val_loss did not improve\n",
      "106381/106381 [==============================] - 27s 252us/step - loss: 0.0354 - acc: 0.9855 - val_loss: 0.0460 - val_acc: 0.9826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9857Epoch 00010: val_loss did not improve\n",
      "106381/106381 [==============================] - 26s 248us/step - loss: 0.0345 - acc: 0.9857 - val_loss: 0.0466 - val_acc: 0.9826\n",
      "-------------------------------\n",
      "0 0.0976917454805 0.962618520909\n",
      "1 0.0230722786816 0.990016983036\n",
      "2 0.0489339577384 0.981525465153\n",
      "3 0.00894246588533 0.997085936668\n",
      "4 0.062383105758 0.974462778324\n",
      "5 0.0212238119098 0.992680374253\n",
      "final 0.0437078942423 0.983065009724\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_glove_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_glove_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "\n",
    "# muse best local cv 0.470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.129,  0.   ,  0.005,  0.   ,  0.032,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.99 ,  0.399,  0.91 ,  0.028,  0.804,  0.158],\n",
       "       [ 0.01 ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.012,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
