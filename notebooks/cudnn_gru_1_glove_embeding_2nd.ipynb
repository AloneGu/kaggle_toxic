{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wavegetarian 86008\n",
      "stiffle 105315\n",
      "louisphilippecharles 86672\n",
      "masm32 44364\n",
      "collected 6573\n",
      "interchangable 52607\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "# print(word_vec_dict['is'])\n",
    "# print(word_vec_dict['are'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.layers import Bidirectional, Dropout, CuDNNGRU\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "# https://github.com/PavelOstyakov/toxic/blob/master/toxic/model.py\n",
    "def get_model(comp=True):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Bidirectional(CuDNNGRU(128, return_sequences=False))(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 150, 300)          12000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 150, 256)          330240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 256)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               296448    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 198       \n",
      "=================================================================\n",
      "Total params: 12,635,110\n",
      "Trainable params: 635,110\n",
      "Non-trainable params: 12,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tmp_m = get_model(False)\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/6\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0688 - acc: 0.9765Epoch 00001: val_loss improved from inf to 0.04492, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 105s 823us/step - loss: 0.0688 - acc: 0.9765 - val_loss: 0.0449 - val_acc: 0.9825\n",
      "Epoch 2/6\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9832Epoch 00002: val_loss improved from 0.04492 to 0.04201, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 105s 824us/step - loss: 0.0442 - acc: 0.9832 - val_loss: 0.0420 - val_acc: 0.9835\n",
      "Epoch 3/6\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9842Epoch 00003: val_loss improved from 0.04201 to 0.04110, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 105s 825us/step - loss: 0.0409 - acc: 0.9842 - val_loss: 0.0411 - val_acc: 0.9837\n",
      "Epoch 4/6\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9849Epoch 00004: val_loss improved from 0.04110 to 0.04055, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 105s 819us/step - loss: 0.0385 - acc: 0.9849 - val_loss: 0.0406 - val_acc: 0.9840\n",
      "Epoch 5/6\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9855Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 104s 816us/step - loss: 0.0362 - acc: 0.9855 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 6/6\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 104s 818us/step - loss: 0.0339 - acc: 0.9864 - val_loss: 0.0413 - val_acc: 0.9841\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0753 - acc: 0.9745Epoch 00001: val_loss improved from inf to 0.04605, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 108s 850us/step - loss: 0.0753 - acc: 0.9745 - val_loss: 0.0460 - val_acc: 0.9827\n",
      "Epoch 2/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9829Epoch 00002: val_loss improved from 0.04605 to 0.04370, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 109s 854us/step - loss: 0.0455 - acc: 0.9829 - val_loss: 0.0437 - val_acc: 0.9836\n",
      "Epoch 3/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9840Epoch 00003: val_loss improved from 0.04370 to 0.04187, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 110s 861us/step - loss: 0.0417 - acc: 0.9840 - val_loss: 0.0419 - val_acc: 0.9840\n",
      "Epoch 4/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9845Epoch 00004: val_loss improved from 0.04187 to 0.04120, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 110s 864us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0412 - val_acc: 0.9842\n",
      "Epoch 5/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9854Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 872us/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0418 - val_acc: 0.9841\n",
      "Epoch 6/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 112s 876us/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0424 - val_acc: 0.9842\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0687 - acc: 0.9762Epoch 00001: val_loss improved from inf to 0.04360, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 106s 829us/step - loss: 0.0687 - acc: 0.9763 - val_loss: 0.0436 - val_acc: 0.9835\n",
      "Epoch 2/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9830Epoch 00002: val_loss improved from 0.04360 to 0.04308, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 104s 818us/step - loss: 0.0449 - acc: 0.9830 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "Epoch 3/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9839Epoch 00003: val_loss improved from 0.04308 to 0.04029, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 105s 819us/step - loss: 0.0414 - acc: 0.9839 - val_loss: 0.0403 - val_acc: 0.9845\n",
      "Epoch 4/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9846Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 105s 819us/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.0408 - val_acc: 0.9839\n",
      "Epoch 5/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 105s 821us/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0406 - val_acc: 0.9844\n",
      "Epoch 6/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 105s 821us/step - loss: 0.0341 - acc: 0.9863 - val_loss: 0.0409 - val_acc: 0.9843\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9755Epoch 00001: val_loss improved from inf to 0.04605, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 110s 862us/step - loss: 0.0719 - acc: 0.9755 - val_loss: 0.0461 - val_acc: 0.9826\n",
      "Epoch 2/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9829Epoch 00002: val_loss improved from 0.04605 to 0.04343, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 109s 857us/step - loss: 0.0452 - acc: 0.9829 - val_loss: 0.0434 - val_acc: 0.9831\n",
      "Epoch 3/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9841Epoch 00003: val_loss improved from 0.04343 to 0.04173, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 110s 862us/step - loss: 0.0415 - acc: 0.9841 - val_loss: 0.0417 - val_acc: 0.9836\n",
      "Epoch 4/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9848Epoch 00004: val_loss improved from 0.04173 to 0.04103, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 111s 869us/step - loss: 0.0390 - acc: 0.9848 - val_loss: 0.0410 - val_acc: 0.9837\n",
      "Epoch 5/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9855Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 869us/step - loss: 0.0367 - acc: 0.9855 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 6/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9863Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 109s 856us/step - loss: 0.0345 - acc: 0.9863 - val_loss: 0.0432 - val_acc: 0.9838\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9770Epoch 00001: val_loss improved from inf to 0.04521, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 111s 872us/step - loss: 0.0667 - acc: 0.9770 - val_loss: 0.0452 - val_acc: 0.9831\n",
      "Epoch 2/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9829Epoch 00002: val_loss improved from 0.04521 to 0.04266, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 111s 872us/step - loss: 0.0449 - acc: 0.9829 - val_loss: 0.0427 - val_acc: 0.9838\n",
      "Epoch 3/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9841Epoch 00003: val_loss improved from 0.04266 to 0.04147, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 111s 869us/step - loss: 0.0411 - acc: 0.9841 - val_loss: 0.0415 - val_acc: 0.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 105s 820us/step - loss: 0.0388 - acc: 0.9847 - val_loss: 0.0419 - val_acc: 0.9841\n",
      "Epoch 5/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9853Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 107s 841us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0419 - val_acc: 0.9842\n",
      "Epoch 6/6\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0343 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 107s 835us/step - loss: 0.0343 - acc: 0.9861 - val_loss: 0.0431 - val_acc: 0.9836\n",
      "-------------------------------\n",
      "0 0.0895173350258 0.966284600585\n",
      "1 0.0216475403429 0.990543394476\n",
      "2 0.0455771193036 0.982158412243\n",
      "3 0.00927643400858 0.997179938711\n",
      "4 0.0591609232952 0.975515601206\n",
      "5 0.0202713450041 0.992793176705\n",
      "final 0.0409084494967 0.984079187321\n",
      "all eval None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 256\n",
    "        epochs = 6\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6) (153164, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cudnn_gru_glove_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cudnn_gru_glove_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.005,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.998,  0.325,  0.972,  0.002,  0.792,  0.007],\n",
       "       [ 0.009,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.009,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
