{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    #https://www.kaggle.com/sreeram004/test-lr-with-convai-dataset\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citys 39501\n",
      "prostituted 34573\n",
      "foot 3998\n",
      "dicated 123524\n",
      "physica 73148\n",
      "sellers 14629\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "de\n",
      "435713\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "\n",
    "with open('../wiki.multi.fr.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print('fr')\n",
    "with open('../wiki.multi.de.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print('de')\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "        \n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4716\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Conv1D(256,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/10\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0835 - acc: 0.9731Epoch 00001: val_loss improved from inf to 0.05375, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 25s 384us/step - loss: 0.0834 - acc: 0.9732 - val_loss: 0.0537 - val_acc: 0.9807\n",
      "Epoch 2/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0556 - acc: 0.9799Epoch 00002: val_loss improved from 0.05375 to 0.05169, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 22s 350us/step - loss: 0.0555 - acc: 0.9799 - val_loss: 0.0517 - val_acc: 0.9816\n",
      "Epoch 3/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9812Epoch 00003: val_loss improved from 0.05169 to 0.04841, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 22s 347us/step - loss: 0.0505 - acc: 0.9812 - val_loss: 0.0484 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9823Epoch 00004: val_loss improved from 0.04841 to 0.04793, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 22s 347us/step - loss: 0.0468 - acc: 0.9823 - val_loss: 0.0479 - val_acc: 0.9819\n",
      "Epoch 5/10\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9831Epoch 00005: val_loss improved from 0.04793 to 0.04694, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 23s 358us/step - loss: 0.0440 - acc: 0.9831 - val_loss: 0.0469 - val_acc: 0.9828\n",
      "Epoch 6/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9840Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 22s 344us/step - loss: 0.0412 - acc: 0.9840 - val_loss: 0.0476 - val_acc: 0.9819\n",
      "Epoch 7/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9849Epoch 00007: val_loss improved from 0.04694 to 0.04660, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 22s 352us/step - loss: 0.0386 - acc: 0.9849 - val_loss: 0.0466 - val_acc: 0.9829\n",
      "Epoch 8/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9860Epoch 00008: val_loss did not improve\n",
      "63900/63900 [==============================] - 22s 339us/step - loss: 0.0358 - acc: 0.9860 - val_loss: 0.0509 - val_acc: 0.9826\n",
      "Epoch 9/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9869Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 22s 351us/step - loss: 0.0332 - acc: 0.9869 - val_loss: 0.0502 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9875Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 22s 342us/step - loss: 0.0316 - acc: 0.9875 - val_loss: 0.0501 - val_acc: 0.9817\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0768 - acc: 0.9752Epoch 00001: val_loss improved from inf to 0.05398, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 347us/step - loss: 0.0768 - acc: 0.9752 - val_loss: 0.0540 - val_acc: 0.9804\n",
      "Epoch 2/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9803Epoch 00002: val_loss improved from 0.05398 to 0.04859, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 340us/step - loss: 0.0538 - acc: 0.9803 - val_loss: 0.0486 - val_acc: 0.9820\n",
      "Epoch 3/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9818Epoch 00003: val_loss improved from 0.04859 to 0.04654, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 342us/step - loss: 0.0485 - acc: 0.9818 - val_loss: 0.0465 - val_acc: 0.9826\n",
      "Epoch 4/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9831Epoch 00004: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 347us/step - loss: 0.0448 - acc: 0.9831 - val_loss: 0.0472 - val_acc: 0.9828\n",
      "Epoch 5/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9839Epoch 00005: val_loss improved from 0.04654 to 0.04591, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 343us/step - loss: 0.0419 - acc: 0.9839 - val_loss: 0.0459 - val_acc: 0.9827\n",
      "Epoch 6/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9845Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 341us/step - loss: 0.0389 - acc: 0.9845 - val_loss: 0.0461 - val_acc: 0.9829\n",
      "Epoch 7/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9855Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 341us/step - loss: 0.0364 - acc: 0.9855 - val_loss: 0.0469 - val_acc: 0.9829\n",
      "Epoch 8/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 341us/step - loss: 0.0342 - acc: 0.9864 - val_loss: 0.0500 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 342us/step - loss: 0.0319 - acc: 0.9872 - val_loss: 0.0520 - val_acc: 0.9829\n",
      "Epoch 10/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0302 - acc: 0.9878Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 342us/step - loss: 0.0302 - acc: 0.9878 - val_loss: 0.0518 - val_acc: 0.9826\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0841 - acc: 0.9732Epoch 00001: val_loss improved from inf to 0.05513, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 349us/step - loss: 0.0840 - acc: 0.9732 - val_loss: 0.0551 - val_acc: 0.9801\n",
      "Epoch 2/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9800Epoch 00002: val_loss improved from 0.05513 to 0.05320, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 344us/step - loss: 0.0555 - acc: 0.9800 - val_loss: 0.0532 - val_acc: 0.9806\n",
      "Epoch 3/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9813Epoch 00003: val_loss improved from 0.05320 to 0.05117, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 345us/step - loss: 0.0507 - acc: 0.9813 - val_loss: 0.0512 - val_acc: 0.9812\n",
      "Epoch 4/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9823Epoch 00004: val_loss improved from 0.05117 to 0.04874, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 22s 344us/step - loss: 0.0464 - acc: 0.9823 - val_loss: 0.0487 - val_acc: 0.9816\n",
      "Epoch 5/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9836Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 342us/step - loss: 0.0429 - acc: 0.9836 - val_loss: 0.0496 - val_acc: 0.9815\n",
      "Epoch 6/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9845Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 343us/step - loss: 0.0401 - acc: 0.9845 - val_loss: 0.0491 - val_acc: 0.9811\n",
      "Epoch 7/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 343us/step - loss: 0.0370 - acc: 0.9854 - val_loss: 0.0493 - val_acc: 0.9818\n",
      "Epoch 8/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9863Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 343us/step - loss: 0.0347 - acc: 0.9863 - val_loss: 0.0516 - val_acc: 0.9819\n",
      "Epoch 9/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9872Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 342us/step - loss: 0.0323 - acc: 0.9872 - val_loss: 0.0553 - val_acc: 0.9817\n",
      "Epoch 10/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0303 - acc: 0.9878Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 22s 343us/step - loss: 0.0303 - acc: 0.9878 - val_loss: 0.0532 - val_acc: 0.9812\n",
      "-------------------------------\n",
      "0 0.10404212688 0.961586211933\n",
      "1 0.0234330263001 0.990422635132\n",
      "2 0.0538225089594 0.979968910079\n",
      "3 0.0111456385188 0.9968701422\n",
      "4 0.0669737884117 0.97291629717\n",
      "5 0.0230888875458 0.992655267029\n",
      "final 0.0470843294359 0.982403243924\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_muse_adj_1_csv_de_fr.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_muse_adj_1_feat_de_fr.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.517,  0.   ,  0.022,  0.   ,  0.025,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.013,  0.   ,  0.   ,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.161,  0.001,  0.023,  0.001,  0.033,  0.005],\n",
       "       [ 0.004,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.005,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.001,  0.   ,  0.001,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
