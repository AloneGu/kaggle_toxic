{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    #https://www.kaggle.com/sreeram004/test-lr-with-convai-dataset\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 6059\n",
      "describe 1185\n",
      "harangue 55948\n",
      "dilligence 39269\n",
      "27211 82991\n",
      "200620062006 124197\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "de\n",
      "435713\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "\n",
    "with open('../wiki.multi.fr.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print('fr')\n",
    "with open('../wiki.multi.de.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print('de')\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "        \n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4716\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(256,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0789 - acc: 0.9745Epoch 00001: val_loss improved from inf to 0.05291, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 28s 436us/step - loss: 0.0789 - acc: 0.9745 - val_loss: 0.0529 - val_acc: 0.9809\n",
      "Epoch 2/10\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0558 - acc: 0.9796Epoch 00002: val_loss did not improve\n",
      "63900/63900 [==============================] - 26s 406us/step - loss: 0.0558 - acc: 0.9796 - val_loss: 0.0562 - val_acc: 0.9798\n",
      "Epoch 3/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9809Epoch 00003: val_loss improved from 0.05291 to 0.04823, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 26s 410us/step - loss: 0.0512 - acc: 0.9809 - val_loss: 0.0482 - val_acc: 0.9821\n",
      "Epoch 4/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9821Epoch 00004: val_loss improved from 0.04823 to 0.04710, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 26s 403us/step - loss: 0.0479 - acc: 0.9821 - val_loss: 0.0471 - val_acc: 0.9824\n",
      "Epoch 5/10\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9826Epoch 00005: val_loss improved from 0.04710 to 0.04669, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 26s 399us/step - loss: 0.0460 - acc: 0.9826 - val_loss: 0.0467 - val_acc: 0.9824\n",
      "Epoch 6/10\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9831Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 25s 396us/step - loss: 0.0440 - acc: 0.9831 - val_loss: 0.0467 - val_acc: 0.9825\n",
      "Epoch 7/10\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9835Epoch 00007: val_loss did not improve\n",
      "63900/63900 [==============================] - 25s 398us/step - loss: 0.0423 - acc: 0.9835 - val_loss: 0.0469 - val_acc: 0.9823\n",
      "Epoch 8/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9841Epoch 00008: val_loss improved from 0.04669 to 0.04547, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 26s 400us/step - loss: 0.0408 - acc: 0.9841 - val_loss: 0.0455 - val_acc: 0.9829\n",
      "Epoch 9/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9847Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 25s 395us/step - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0459 - val_acc: 0.9825\n",
      "Epoch 10/10\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9849Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 25s 394us/step - loss: 0.0377 - acc: 0.9849 - val_loss: 0.0460 - val_acc: 0.9826\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0820 - acc: 0.9737Epoch 00001: val_loss improved from inf to 0.05351, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 26s 407us/step - loss: 0.0820 - acc: 0.9737 - val_loss: 0.0535 - val_acc: 0.9801\n",
      "Epoch 2/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9795Epoch 00002: val_loss improved from 0.05351 to 0.05000, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 396us/step - loss: 0.0566 - acc: 0.9795 - val_loss: 0.0500 - val_acc: 0.9817\n",
      "Epoch 3/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0521 - acc: 0.9807Epoch 00003: val_loss improved from 0.05000 to 0.04771, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 394us/step - loss: 0.0521 - acc: 0.9808 - val_loss: 0.0477 - val_acc: 0.9822\n",
      "Epoch 4/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9817Epoch 00004: val_loss improved from 0.04771 to 0.04744, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 394us/step - loss: 0.0490 - acc: 0.9817 - val_loss: 0.0474 - val_acc: 0.9823\n",
      "Epoch 5/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9823Epoch 00005: val_loss improved from 0.04744 to 0.04571, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 389us/step - loss: 0.0465 - acc: 0.9823 - val_loss: 0.0457 - val_acc: 0.9827\n",
      "Epoch 6/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9828Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 392us/step - loss: 0.0443 - acc: 0.9828 - val_loss: 0.0462 - val_acc: 0.9825\n",
      "Epoch 7/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9831Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 396us/step - loss: 0.0431 - acc: 0.9831 - val_loss: 0.0459 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9839Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 388us/step - loss: 0.0414 - acc: 0.9839 - val_loss: 0.0465 - val_acc: 0.9829\n",
      "Epoch 9/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9845Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 390us/step - loss: 0.0395 - acc: 0.9845 - val_loss: 0.0473 - val_acc: 0.9829\n",
      "Epoch 10/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 389us/step - loss: 0.0384 - acc: 0.9847 - val_loss: 0.0470 - val_acc: 0.9830\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0836 - acc: 0.9730Epoch 00001: val_loss improved from inf to 0.05771, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 395us/step - loss: 0.0836 - acc: 0.9730 - val_loss: 0.0577 - val_acc: 0.9793\n",
      "Epoch 2/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9801Epoch 00002: val_loss improved from 0.05771 to 0.05253, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 391us/step - loss: 0.0553 - acc: 0.9801 - val_loss: 0.0525 - val_acc: 0.9801\n",
      "Epoch 3/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9814Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 25s 395us/step - loss: 0.0509 - acc: 0.9814 - val_loss: 0.0532 - val_acc: 0.9805\n",
      "Epoch 4/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0480 - acc: 0.9820Epoch 00004: val_loss improved from 0.05253 to 0.04820, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 389us/step - loss: 0.0480 - acc: 0.9820 - val_loss: 0.0482 - val_acc: 0.9819\n",
      "Epoch 5/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9826Epoch 00005: val_loss improved from 0.04820 to 0.04774, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 25s 393us/step - loss: 0.0456 - acc: 0.9826 - val_loss: 0.0477 - val_acc: 0.9818\n",
      "Epoch 6/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 26s 400us/step - loss: 0.0441 - acc: 0.9832 - val_loss: 0.0480 - val_acc: 0.9822\n",
      "Epoch 7/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9838Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 26s 411us/step - loss: 0.0420 - acc: 0.9838 - val_loss: 0.0496 - val_acc: 0.9819\n",
      "Epoch 8/10\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9844Epoch 00008: val_loss improved from 0.04774 to 0.04749, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 26s 409us/step - loss: 0.0405 - acc: 0.9843 - val_loss: 0.0475 - val_acc: 0.9822\n",
      "Epoch 9/10\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9849Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 26s 406us/step - loss: 0.0387 - acc: 0.9849 - val_loss: 0.0479 - val_acc: 0.9823\n",
      "Epoch 10/10\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9854Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 27s 417us/step - loss: 0.0370 - acc: 0.9854 - val_loss: 0.0476 - val_acc: 0.9821\n",
      "-------------------------------\n",
      "0 0.101822531147 0.96195136201\n",
      "1 0.0240708353992 0.990088783633\n",
      "2 0.0532537928131 0.980156701547\n",
      "3 0.0099341259875 0.997016202231\n",
      "4 0.066221920481 0.973594433026\n",
      "5 0.0220439441586 0.992863924216\n",
      "final 0.0462245249978 0.98261190111\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_muse_adj_1_csv_de_fr.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_muse_adj_1_feat_de_fr.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.364,  0.   ,  0.018,  0.   ,  0.031,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.12 ,  0.   ,  0.002,  0.   ,  0.005,  0.005],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.141,  0.002,  0.013,  0.001,  0.029,  0.008],\n",
       "       [ 0.016,  0.   ,  0.001,  0.   ,  0.002,  0.   ],\n",
       "       [ 0.106,  0.   ,  0.003,  0.006,  0.009,  0.003],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.008,  0.   ,  0.001,  0.   ,  0.002,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
