{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\",\n",
       "       '\"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"',\n",
       "       '\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the \"\"points of interest\"\" section you added because it seemed kind of spammy. I know you probably didn\\'t mean to disobey the rules, but generally, a point of interest tends to be rather touristy, and quite irrelevant to an area culture. That\\'s just my opinion, though.\\n\\nIf you want to reply, just put your reply here and add {{talkback|Jamiegraham08}} on my talkpage.   \"',\n",
       "       \"Asking some his nationality is a Racial offence. Wow wasn't aware of it.  Blocking me has shown your support towards your community. Thanku for that\",\n",
       "       'The reader here is not going by my say so for ethereal vocal style and dark lyrical content. The cited sources in the External Links are saying those things. If you feel the sources are unreliable or I did not represent what they said correctly rewrite or delete it.'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maryse 54857\n",
      "page195 133177\n",
      "chrysler 28153\n",
      "eläkeläiset 23457\n",
      "accustaios 71516\n",
      "u–th 145898\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 6145\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0891 - acc: 0.9724Epoch 00001: val_loss improved from inf to 0.05440, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 341s 5ms/step - loss: 0.0891 - acc: 0.9724 - val_loss: 0.0544 - val_acc: 0.9806\n",
      "Epoch 2/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0566 - acc: 0.9800Epoch 00002: val_loss improved from 0.05440 to 0.05029, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 339s 5ms/step - loss: 0.0566 - acc: 0.9800 - val_loss: 0.0503 - val_acc: 0.9817\n",
      "Epoch 3/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9807Epoch 00003: val_loss improved from 0.05029 to 0.04889, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 339s 5ms/step - loss: 0.0531 - acc: 0.9807 - val_loss: 0.0489 - val_acc: 0.9822\n",
      "Epoch 4/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0511 - acc: 0.9814Epoch 00004: val_loss improved from 0.04889 to 0.04827, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 335s 5ms/step - loss: 0.0511 - acc: 0.9814 - val_loss: 0.0483 - val_acc: 0.9823\n",
      "Epoch 5/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0492 - acc: 0.9819Epoch 00005: val_loss improved from 0.04827 to 0.04712, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 331s 5ms/step - loss: 0.0492 - acc: 0.9819 - val_loss: 0.0471 - val_acc: 0.9826\n",
      "Epoch 6/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9822Epoch 00006: val_loss improved from 0.04712 to 0.04536, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 329s 5ms/step - loss: 0.0477 - acc: 0.9822 - val_loss: 0.0454 - val_acc: 0.9828\n",
      "Epoch 7/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0465 - acc: 0.9825Epoch 00007: val_loss improved from 0.04536 to 0.04471, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0465 - acc: 0.9825 - val_loss: 0.0447 - val_acc: 0.9829\n",
      "Epoch 8/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9830Epoch 00008: val_loss improved from 0.04471 to 0.04470, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0451 - acc: 0.9830 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "Epoch 9/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9829Epoch 00009: val_loss improved from 0.04470 to 0.04382, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0443 - acc: 0.9829 - val_loss: 0.0438 - val_acc: 0.9834\n",
      "Epoch 10/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9835Epoch 00010: val_loss improved from 0.04382 to 0.04372, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0434 - acc: 0.9835 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "Epoch 11/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9837Epoch 00011: val_loss improved from 0.04372 to 0.04359, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 329s 5ms/step - loss: 0.0427 - acc: 0.9837 - val_loss: 0.0436 - val_acc: 0.9834\n",
      "Epoch 12/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9835Epoch 00012: val_loss did not improve\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0422 - acc: 0.9835 - val_loss: 0.0444 - val_acc: 0.9832\n",
      "Epoch 13/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9841Epoch 00013: val_loss did not improve\n",
      "63900/63900 [==============================] - 330s 5ms/step - loss: 0.0414 - acc: 0.9841 - val_loss: 0.0439 - val_acc: 0.9836\n",
      "Epoch 14/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9841Epoch 00014: val_loss did not improve\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0411 - acc: 0.9841 - val_loss: 0.0440 - val_acc: 0.9830\n",
      "Epoch 15/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9844Epoch 00015: val_loss improved from 0.04359 to 0.04339, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 328s 5ms/step - loss: 0.0405 - acc: 0.9844 - val_loss: 0.0434 - val_acc: 0.9837\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0907 - acc: 0.9720Epoch 00001: val_loss improved from inf to 0.05439, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 332s 5ms/step - loss: 0.0907 - acc: 0.9720 - val_loss: 0.0544 - val_acc: 0.9806\n",
      "Epoch 2/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9797Epoch 00002: val_loss improved from 0.05439 to 0.05107, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0565 - acc: 0.9797 - val_loss: 0.0511 - val_acc: 0.9816\n",
      "Epoch 3/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0539 - acc: 0.9805Epoch 00003: val_loss improved from 0.05107 to 0.04933, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0539 - acc: 0.9805 - val_loss: 0.0493 - val_acc: 0.9819\n",
      "Epoch 4/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9811Epoch 00004: val_loss improved from 0.04933 to 0.04811, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0519 - acc: 0.9811 - val_loss: 0.0481 - val_acc: 0.9823\n",
      "Epoch 5/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9817Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 336s 5ms/step - loss: 0.0499 - acc: 0.9817 - val_loss: 0.0503 - val_acc: 0.9820\n",
      "Epoch 6/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9823Epoch 00006: val_loss improved from 0.04811 to 0.04659, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0477 - acc: 0.9823 - val_loss: 0.0466 - val_acc: 0.9828\n",
      "Epoch 7/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9823Epoch 00007: val_loss improved from 0.04659 to 0.04654, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0467 - acc: 0.9823 - val_loss: 0.0465 - val_acc: 0.9826\n",
      "Epoch 8/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9825Epoch 00008: val_loss improved from 0.04654 to 0.04547, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0456 - acc: 0.9825 - val_loss: 0.0455 - val_acc: 0.9831\n",
      "Epoch 9/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9829Epoch 00009: val_loss improved from 0.04547 to 0.04427, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 331s 5ms/step - loss: 0.0444 - acc: 0.9830 - val_loss: 0.0443 - val_acc: 0.9835\n",
      "Epoch 10/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9833Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 330s 5ms/step - loss: 0.0438 - acc: 0.9833 - val_loss: 0.0445 - val_acc: 0.9835\n",
      "Epoch 11/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9833Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0433 - acc: 0.9833 - val_loss: 0.0448 - val_acc: 0.9834\n",
      "Epoch 12/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9837Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0426 - acc: 0.9837 - val_loss: 0.0443 - val_acc: 0.9833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00013: val_loss did not improve\n",
      "63901/63901 [==============================] - 328s 5ms/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Epoch 14/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9841Epoch 00014: val_loss improved from 0.04427 to 0.04426, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 327s 5ms/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0443 - val_acc: 0.9835\n",
      "Epoch 15/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9843Epoch 00015: val_loss did not improve\n",
      "63901/63901 [==============================] - 328s 5ms/step - loss: 0.0404 - acc: 0.9843 - val_loss: 0.0443 - val_acc: 0.9833\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0870 - acc: 0.9737Epoch 00001: val_loss improved from inf to 0.05524, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 330s 5ms/step - loss: 0.0870 - acc: 0.9737 - val_loss: 0.0552 - val_acc: 0.9801\n",
      "Epoch 2/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0550 - acc: 0.9803Epoch 00002: val_loss improved from 0.05524 to 0.05220, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0549 - acc: 0.9803 - val_loss: 0.0522 - val_acc: 0.9809\n",
      "Epoch 3/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9812Epoch 00003: val_loss improved from 0.05220 to 0.05059, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0520 - acc: 0.9812 - val_loss: 0.0506 - val_acc: 0.9813\n",
      "Epoch 4/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9818Epoch 00004: val_loss improved from 0.05059 to 0.04922, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0498 - acc: 0.9818 - val_loss: 0.0492 - val_acc: 0.9817\n",
      "Epoch 5/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9822Epoch 00005: val_loss improved from 0.04922 to 0.04798, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0486 - acc: 0.9822 - val_loss: 0.0480 - val_acc: 0.9820\n",
      "Epoch 6/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9825Epoch 00006: val_loss improved from 0.04798 to 0.04739, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0469 - acc: 0.9825 - val_loss: 0.0474 - val_acc: 0.9817\n",
      "Epoch 7/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9826Epoch 00007: val_loss improved from 0.04739 to 0.04658, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0459 - acc: 0.9826 - val_loss: 0.0466 - val_acc: 0.9823\n",
      "Epoch 8/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9831Epoch 00008: val_loss improved from 0.04658 to 0.04595, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0448 - acc: 0.9831 - val_loss: 0.0459 - val_acc: 0.9823\n",
      "Epoch 9/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00009: val_loss improved from 0.04595 to 0.04502, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0450 - val_acc: 0.9828\n",
      "Epoch 10/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0434 - acc: 0.9833Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0434 - acc: 0.9833 - val_loss: 0.0451 - val_acc: 0.9827\n",
      "Epoch 11/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9838Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0424 - acc: 0.9838 - val_loss: 0.0450 - val_acc: 0.9828\n",
      "Epoch 12/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9840Epoch 00012: val_loss improved from 0.04502 to 0.04475, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0418 - acc: 0.9840 - val_loss: 0.0447 - val_acc: 0.9827\n",
      "Epoch 13/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9841Epoch 00013: val_loss improved from 0.04475 to 0.04471, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 339s 5ms/step - loss: 0.0409 - acc: 0.9841 - val_loss: 0.0447 - val_acc: 0.9830\n",
      "Epoch 14/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9844Epoch 00014: val_loss did not improve\n",
      "63901/63901 [==============================] - 342s 5ms/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0451 - val_acc: 0.9830\n",
      "Epoch 15/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9845Epoch 00015: val_loss did not improve\n",
      "63901/63901 [==============================] - 329s 5ms/step - loss: 0.0401 - acc: 0.9845 - val_loss: 0.0448 - val_acc: 0.9830\n",
      "-------------------------------\n",
      "0 0.0967779830442 0.963735380956\n",
      "1 0.0232349589451 0.99044350085\n",
      "2 0.0514027358529 0.981043494591\n",
      "3 0.00883653497705 0.997203993699\n",
      "4 0.0637558683738 0.974606420382\n",
      "5 0.0207022538638 0.99327080573\n",
      "final 0.0441183891761 0.983383932701\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 15\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_muse_2_csv_adj.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_muse_adj_2_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.316,  0.   ,  0.02 ,  0.   ,  0.031,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.006,  0.   ,  0.   ,  0.   ,  0.001,  0.001],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.345,  0.004,  0.098,  0.002,  0.128,  0.01 ],\n",
       "       [ 0.006,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.171,  0.001,  0.01 ,  0.018,  0.019,  0.003],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.014,  0.   ,  0.002,  0.   ,  0.003,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
