{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 30000\n",
    "maxlen = 200\n",
    "\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"Nonsense?  kiss off, geek. what I said is true.  I'll have your account terminated.\",\n",
       "       '\"\\n\\n Please do not vandalize pages, as you did with this edit to W. S. Merwin. If you continue to do so, you will be blocked from editing.    \"',\n",
       "       '\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the \"\"points of interest\"\" section you added because it seemed kind of spammy. I know you probably didn\\'t mean to disobey the rules, but generally, a point of interest tends to be rather touristy, and quite irrelevant to an area culture. That\\'s just my opinion, though.\\n\\nIf you want to reply, just put your reply here and add {{talkback|Jamiegraham08}} on my talkpage.   \"',\n",
       "       \"Asking some his nationality is a Racial offence. Wow wasn't aware of it.  Blocking me has shown your support towards your community. Thanku for that\",\n",
       "       'The reader here is not going by my say so for ethereal vocal style and dark lyrical content. The cited sources in the External Links are saying those things. If you feel the sources are unreliable or I did not represent what they said correctly rewrite or delete it.'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 200) (226998, 200)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speedup 59644\n",
      "inadmissible 27448\n",
      "oguz 80096\n",
      "dobs 142110\n",
      "warfa 146745\n",
      "abe 16953\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 3275\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(96, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0847 - acc: 0.9736Epoch 00001: val_loss improved from inf to 0.05635, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 537s 8ms/step - loss: 0.0847 - acc: 0.9736 - val_loss: 0.0563 - val_acc: 0.9801\n",
      "Epoch 2/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0557 - acc: 0.9801Epoch 00002: val_loss improved from 0.05635 to 0.05128, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 527s 8ms/step - loss: 0.0557 - acc: 0.9801 - val_loss: 0.0513 - val_acc: 0.9813\n",
      "Epoch 3/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9806Epoch 00003: val_loss improved from 0.05128 to 0.04989, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 527s 8ms/step - loss: 0.0531 - acc: 0.9806 - val_loss: 0.0499 - val_acc: 0.9821\n",
      "Epoch 4/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9813Epoch 00004: val_loss improved from 0.04989 to 0.04785, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 528s 8ms/step - loss: 0.0510 - acc: 0.9813 - val_loss: 0.0478 - val_acc: 0.9828\n",
      "Epoch 5/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9818Epoch 00005: val_loss did not improve\n",
      "63900/63900 [==============================] - 528s 8ms/step - loss: 0.0486 - acc: 0.9818 - val_loss: 0.0486 - val_acc: 0.9821\n",
      "Epoch 6/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9824Epoch 00006: val_loss improved from 0.04785 to 0.04777, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 529s 8ms/step - loss: 0.0469 - acc: 0.9824 - val_loss: 0.0478 - val_acc: 0.9823\n",
      "Epoch 7/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9828Epoch 00007: val_loss improved from 0.04777 to 0.04498, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 530s 8ms/step - loss: 0.0452 - acc: 0.9828 - val_loss: 0.0450 - val_acc: 0.9832\n",
      "Epoch 8/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9829Epoch 00008: val_loss did not improve\n",
      "63900/63900 [==============================] - 529s 8ms/step - loss: 0.0442 - acc: 0.9829 - val_loss: 0.0454 - val_acc: 0.9830\n",
      "Epoch 9/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9836Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 530s 8ms/step - loss: 0.0427 - acc: 0.9836 - val_loss: 0.0456 - val_acc: 0.9827\n",
      "Epoch 10/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9841Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 530s 8ms/step - loss: 0.0413 - acc: 0.9841 - val_loss: 0.0465 - val_acc: 0.9821\n",
      "Epoch 11/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9842Epoch 00011: val_loss improved from 0.04498 to 0.04479, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 530s 8ms/step - loss: 0.0404 - acc: 0.9842 - val_loss: 0.0448 - val_acc: 0.9831\n",
      "Epoch 12/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9846Epoch 00012: val_loss improved from 0.04479 to 0.04461, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 531s 8ms/step - loss: 0.0395 - acc: 0.9846 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "Epoch 13/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9849Epoch 00013: val_loss improved from 0.04461 to 0.04456, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 531s 8ms/step - loss: 0.0388 - acc: 0.9849 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Epoch 14/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00014: val_loss improved from 0.04456 to 0.04411, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 531s 8ms/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0441 - val_acc: 0.9837\n",
      "Epoch 15/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9851Epoch 00015: val_loss did not improve\n",
      "63900/63900 [==============================] - 531s 8ms/step - loss: 0.0373 - acc: 0.9851 - val_loss: 0.0446 - val_acc: 0.9832\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0801 - acc: 0.9746Epoch 00001: val_loss improved from inf to 0.05291, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 529s 8ms/step - loss: 0.0801 - acc: 0.9746 - val_loss: 0.0529 - val_acc: 0.9808\n",
      "Epoch 2/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9804Epoch 00002: val_loss improved from 0.05291 to 0.04970, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0546 - acc: 0.9804 - val_loss: 0.0497 - val_acc: 0.9816\n",
      "Epoch 3/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9811Epoch 00003: val_loss improved from 0.04970 to 0.04796, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0512 - acc: 0.9811 - val_loss: 0.0480 - val_acc: 0.9822\n",
      "Epoch 4/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0477 - acc: 0.9820Epoch 00004: val_loss improved from 0.04796 to 0.04523, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0477 - acc: 0.9820 - val_loss: 0.0452 - val_acc: 0.9830\n",
      "Epoch 5/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9824Epoch 00005: val_loss improved from 0.04523 to 0.04485, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0457 - acc: 0.9824 - val_loss: 0.0449 - val_acc: 0.9833\n",
      "Epoch 6/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9831Epoch 00006: val_loss improved from 0.04485 to 0.04477, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0439 - acc: 0.9831 - val_loss: 0.0448 - val_acc: 0.9826\n",
      "Epoch 7/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9835Epoch 00007: val_loss improved from 0.04477 to 0.04392, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0428 - acc: 0.9835 - val_loss: 0.0439 - val_acc: 0.9833\n",
      "Epoch 8/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9839Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.0462 - val_acc: 0.9833\n",
      "Epoch 9/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9841Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0404 - acc: 0.9841 - val_loss: 0.0442 - val_acc: 0.9836\n",
      "Epoch 10/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0445 - val_acc: 0.9832\n",
      "Epoch 11/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9850Epoch 00011: val_loss improved from 0.04392 to 0.04345, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0384 - acc: 0.9850 - val_loss: 0.0435 - val_acc: 0.9836\n",
      "Epoch 12/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9854Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0440 - val_acc: 0.9834\n",
      "Epoch 13/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9857Epoch 00013: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0445 - val_acc: 0.9832\n",
      "Epoch 14/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00014: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0449 - val_acc: 0.9833\n",
      "Epoch 15/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9859Epoch 00015: val_loss did not improve\n",
      "63901/63901 [==============================] - 528s 8ms/step - loss: 0.0355 - acc: 0.9859 - val_loss: 0.0455 - val_acc: 0.9834\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0832 - acc: 0.9740Epoch 00001: val_loss improved from inf to 0.05598, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 527s 8ms/step - loss: 0.0832 - acc: 0.9740 - val_loss: 0.0560 - val_acc: 0.9799\n",
      "Epoch 2/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9806Epoch 00002: val_loss improved from 0.05598 to 0.05568, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0548 - acc: 0.9806 - val_loss: 0.0557 - val_acc: 0.9796\n",
      "Epoch 3/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0516 - acc: 0.9812Epoch 00003: val_loss improved from 0.05568 to 0.04976, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0516 - acc: 0.9812 - val_loss: 0.0498 - val_acc: 0.9815\n",
      "Epoch 4/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9821Epoch 00004: val_loss improved from 0.04976 to 0.04793, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0491 - acc: 0.9821 - val_loss: 0.0479 - val_acc: 0.9819\n",
      "Epoch 5/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9825Epoch 00005: val_loss improved from 0.04793 to 0.04659, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0472 - acc: 0.9824 - val_loss: 0.0466 - val_acc: 0.9823\n",
      "Epoch 6/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9830Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 526s 8ms/step - loss: 0.0454 - acc: 0.9830 - val_loss: 0.0467 - val_acc: 0.9819\n",
      "Epoch 7/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00007: val_loss improved from 0.04659 to 0.04561, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0456 - val_acc: 0.9823\n",
      "Epoch 8/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9836Epoch 00008: val_loss improved from 0.04561 to 0.04464, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0427 - acc: 0.9836 - val_loss: 0.0446 - val_acc: 0.9829\n",
      "Epoch 9/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0449 - val_acc: 0.9830\n",
      "Epoch 10/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9842Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0404 - acc: 0.9842 - val_loss: 0.0457 - val_acc: 0.9824\n",
      "Epoch 11/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9847Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 526s 8ms/step - loss: 0.0394 - acc: 0.9847 - val_loss: 0.0452 - val_acc: 0.9830\n",
      "Epoch 12/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9849Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0385 - acc: 0.9849 - val_loss: 0.0462 - val_acc: 0.9825\n",
      "Epoch 13/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9853Epoch 00013: val_loss did not improve\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0374 - acc: 0.9853 - val_loss: 0.0449 - val_acc: 0.9830\n",
      "Epoch 14/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9855Epoch 00014: val_loss did not improve\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0366 - acc: 0.9855 - val_loss: 0.0454 - val_acc: 0.9825\n",
      "Epoch 15/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9858Epoch 00015: val_loss did not improve\n",
      "63901/63901 [==============================] - 525s 8ms/step - loss: 0.0356 - acc: 0.9858 - val_loss: 0.0451 - val_acc: 0.9829\n",
      "-------------------------------\n",
      "0 0.0961975709873 0.964194426767\n",
      "1 0.0230851894655 0.990735620912\n",
      "2 0.0516851471121 0.981304316074\n",
      "3 0.00923440735499 0.996943172215\n",
      "4 0.0639242840321 0.973823955932\n",
      "5 0.0202808159578 0.993322970026\n",
      "final 0.044067902485 0.983387410321\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 15\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_muse_2_csv_adj.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_muse_adj_2_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.502,  0.   ,  0.023,  0.   ,  0.021,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.019,  0.   ,  0.001,  0.   ,  0.001,  0.006],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.18 ,  0.004,  0.022,  0.002,  0.033,  0.008],\n",
       "       [ 0.011,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.105,  0.   ,  0.002,  0.01 ,  0.004,  0.002],\n",
       "       [ 0.011,  0.   ,  0.001,  0.   ,  0.001,  0.002],\n",
       "       [ 0.008,  0.   ,  0.002,  0.   ,  0.001,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
