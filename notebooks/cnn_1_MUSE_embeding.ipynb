{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 100) (226998, 100)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twickenham 95255\n",
      "tongue 5030\n",
      "squabblers 123368\n",
      "hostiles 117470\n",
      "allegiant 80060\n",
      "heya 17475\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 1082\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(64,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/12\n",
      "63616/63900 [============================>.] - ETA: 0s - loss: 0.0985 - acc: 0.9716Epoch 00001: val_loss improved from inf to 0.05691, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 13s 198us/step - loss: 0.0983 - acc: 0.9716 - val_loss: 0.0569 - val_acc: 0.9800\n",
      "Epoch 2/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0596 - acc: 0.9790Epoch 00002: val_loss improved from 0.05691 to 0.05422, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 161us/step - loss: 0.0596 - acc: 0.9790 - val_loss: 0.0542 - val_acc: 0.9805\n",
      "Epoch 3/12\n",
      "63552/63900 [============================>.] - ETA: 0s - loss: 0.0544 - acc: 0.9800Epoch 00003: val_loss improved from 0.05422 to 0.05069, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 161us/step - loss: 0.0545 - acc: 0.9800 - val_loss: 0.0507 - val_acc: 0.9813\n",
      "Epoch 4/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9808Epoch 00004: val_loss improved from 0.05069 to 0.04932, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 158us/step - loss: 0.0517 - acc: 0.9808 - val_loss: 0.0493 - val_acc: 0.9819\n",
      "Epoch 5/12\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0495 - acc: 0.9814Epoch 00005: val_loss improved from 0.04932 to 0.04879, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 162us/step - loss: 0.0496 - acc: 0.9814 - val_loss: 0.0488 - val_acc: 0.9819\n",
      "Epoch 6/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0483 - acc: 0.9820Epoch 00006: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 161us/step - loss: 0.0483 - acc: 0.9820 - val_loss: 0.0491 - val_acc: 0.9815\n",
      "Epoch 7/12\n",
      "63808/63900 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9825Epoch 00007: val_loss improved from 0.04879 to 0.04801, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 160us/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0480 - val_acc: 0.9822\n",
      "Epoch 8/12\n",
      "63616/63900 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9829Epoch 00008: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 155us/step - loss: 0.0444 - acc: 0.9829 - val_loss: 0.0497 - val_acc: 0.9823\n",
      "Epoch 9/12\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9836Epoch 00009: val_loss improved from 0.04801 to 0.04767, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 152us/step - loss: 0.0429 - acc: 0.9836 - val_loss: 0.0477 - val_acc: 0.9823\n",
      "Epoch 10/12\n",
      "63744/63900 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9836Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 157us/step - loss: 0.0420 - acc: 0.9837 - val_loss: 0.0481 - val_acc: 0.9826\n",
      "Epoch 11/12\n",
      "63680/63900 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9842Epoch 00011: val_loss did not improve\n",
      "63900/63900 [==============================] - 10s 156us/step - loss: 0.0408 - acc: 0.9842 - val_loss: 0.0477 - val_acc: 0.9819\n",
      "Epoch 12/12\n",
      "63616/63900 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9843Epoch 00012: val_loss improved from 0.04767 to 0.04740, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 10s 152us/step - loss: 0.0397 - acc: 0.9843 - val_loss: 0.0474 - val_acc: 0.9827\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/12\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0991 - acc: 0.9693Epoch 00001: val_loss improved from inf to 0.05534, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 11s 169us/step - loss: 0.0990 - acc: 0.9693 - val_loss: 0.0553 - val_acc: 0.9802\n",
      "Epoch 2/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9788Epoch 00002: val_loss improved from 0.05534 to 0.05330, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 154us/step - loss: 0.0599 - acc: 0.9788 - val_loss: 0.0533 - val_acc: 0.9803\n",
      "Epoch 3/12\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0549 - acc: 0.9801Epoch 00003: val_loss improved from 0.05330 to 0.05128, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0549 - acc: 0.9801 - val_loss: 0.0513 - val_acc: 0.9812\n",
      "Epoch 4/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9808Epoch 00004: val_loss improved from 0.05128 to 0.04976, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0522 - acc: 0.9808 - val_loss: 0.0498 - val_acc: 0.9815\n",
      "Epoch 5/12\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9815Epoch 00005: val_loss improved from 0.04976 to 0.04824, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0497 - acc: 0.9815 - val_loss: 0.0482 - val_acc: 0.9819\n",
      "Epoch 6/12\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9819Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 154us/step - loss: 0.0482 - acc: 0.9819 - val_loss: 0.0483 - val_acc: 0.9821\n",
      "Epoch 7/12\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9828Epoch 00007: val_loss improved from 0.04824 to 0.04787, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 154us/step - loss: 0.0462 - acc: 0.9828 - val_loss: 0.0479 - val_acc: 0.9821\n",
      "Epoch 8/12\n",
      "63552/63901 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9827Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 154us/step - loss: 0.0453 - acc: 0.9828 - val_loss: 0.0484 - val_acc: 0.9823\n",
      "Epoch 9/12\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00009: val_loss improved from 0.04787 to 0.04709, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0440 - acc: 0.9833 - val_loss: 0.0471 - val_acc: 0.9823\n",
      "Epoch 10/12\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 154us/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0474 - val_acc: 0.9822\n",
      "Epoch 11/12\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9839Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 154us/step - loss: 0.0419 - acc: 0.9839 - val_loss: 0.0487 - val_acc: 0.9822\n",
      "Epoch 12/12\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0406 - acc: 0.9842Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 157us/step - loss: 0.0406 - acc: 0.9842 - val_loss: 0.0496 - val_acc: 0.9824\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/12\n",
      "63552/63901 [============================>.] - ETA: 0s - loss: 0.1025 - acc: 0.9691Epoch 00001: val_loss improved from inf to 0.05825, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 161us/step - loss: 0.1022 - acc: 0.9691 - val_loss: 0.0583 - val_acc: 0.9794\n",
      "Epoch 2/12\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0586 - acc: 0.9793Epoch 00002: val_loss improved from 0.05825 to 0.05462, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 157us/step - loss: 0.0587 - acc: 0.9793 - val_loss: 0.0546 - val_acc: 0.9801\n",
      "Epoch 3/12\n",
      "63808/63901 [============================>.] - ETA: 0s - loss: 0.0538 - acc: 0.9805Epoch 00003: val_loss improved from 0.05462 to 0.05172, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 156us/step - loss: 0.0538 - acc: 0.9805 - val_loss: 0.0517 - val_acc: 0.9808\n",
      "Epoch 4/12\n",
      "63552/63901 [============================>.] - ETA: 0s - loss: 0.0509 - acc: 0.9815Epoch 00004: val_loss improved from 0.05172 to 0.05110, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 160us/step - loss: 0.0509 - acc: 0.9815 - val_loss: 0.0511 - val_acc: 0.9807\n",
      "Epoch 5/12\n",
      "63616/63901 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9818Epoch 00005: val_loss improved from 0.05110 to 0.05034, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0490 - acc: 0.9819 - val_loss: 0.0503 - val_acc: 0.9810\n",
      "Epoch 6/12\n",
      "63552/63901 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9822Epoch 00006: val_loss improved from 0.05034 to 0.04983, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0472 - acc: 0.9822 - val_loss: 0.0498 - val_acc: 0.9813\n",
      "Epoch 7/12\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9828Epoch 00007: val_loss improved from 0.04983 to 0.04921, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 156us/step - loss: 0.0458 - acc: 0.9828 - val_loss: 0.0492 - val_acc: 0.9813\n",
      "Epoch 8/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9830Epoch 00008: val_loss improved from 0.04921 to 0.04906, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 156us/step - loss: 0.0445 - acc: 0.9830 - val_loss: 0.0491 - val_acc: 0.9815\n",
      "Epoch 9/12\n",
      "63680/63901 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9837Epoch 00009: val_loss improved from 0.04906 to 0.04901, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0433 - acc: 0.9837 - val_loss: 0.0490 - val_acc: 0.9814\n",
      "Epoch 10/12\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9839Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 156us/step - loss: 0.0423 - acc: 0.9839 - val_loss: 0.0494 - val_acc: 0.9817\n",
      "Epoch 11/12\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9842Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0412 - acc: 0.9842 - val_loss: 0.0503 - val_acc: 0.9817\n",
      "Epoch 12/12\n",
      "63744/63901 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9845Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 10s 155us/step - loss: 0.0402 - acc: 0.9846 - val_loss: 0.0502 - val_acc: 0.9806\n",
      "-------------------------------\n",
      "0 0.10565566346 0.960928941795\n",
      "1 0.0238695047267 0.990026186477\n",
      "2 0.055478651454 0.979968910079\n",
      "3 0.0103124247562 0.997110097965\n",
      "4 0.0687132290414 0.972050369845\n",
      "5 0.0229537332898 0.992770028482\n",
      "final 0.0478305344548 0.98214242244\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 12\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_muse_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_muse_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.361,  0.   ,  0.011,  0.   ,  0.023,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.014,  0.   ,  0.   ,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.046,  0.001,  0.011,  0.002,  0.013,  0.004],\n",
       "       [ 0.012,  0.   ,  0.002,  0.   ,  0.002,  0.   ],\n",
       "       [ 0.018,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.005,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.044,  0.   ,  0.008,  0.001,  0.01 ,  0.001]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
