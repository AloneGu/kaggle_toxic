{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #text = BeautifulSoup(review,'html.parser').get_text()\n",
    "    #\n",
    "    # 2. Remove non-letters\n",
    "    text = re.sub(\"[^A-za-z0-9^,?!.\\/'+-=]\",\" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ \"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They were not  vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please do not  remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.   talk  21:51, January 11, 2016  UTC \",\n",
       "       \"Hey man, I'm really not trying to edit war. It  just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       '  More I cannot  make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of   types of accidents    -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.  There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It  listed in the relevant form eg Wikipedia:Good_article_nominations Transport   ',\n",
       "       'You, sir, are my hero. Any chance you remember what page that  on?'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recur 33971\n",
      "outlived 94050\n",
      "percussion 24043\n",
      "euosmia 180595\n",
      "waddle 56092\n",
      "drat 46531\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n",
      "[ 0.0137334   0.0554924   0.0455881   0.0301357  -0.0182521   0.0385928\n",
      "  0.0347148  -0.0847695  -0.036347   -0.00864285  0.00292431  0.0167314\n",
      "  0.0195147  -0.0372235  -0.014314    0.0173197   0.00033499 -0.0477708\n",
      "  0.0113374   0.0266912  -0.0615091   0.0665893  -0.125759   -0.069915\n",
      " -0.0024989   0.022528    0.00747391  0.0752322  -0.0552592   0.0327767\n",
      " -0.0275065   0.144234   -0.130117    0.0105687   0.00473044 -0.0610046\n",
      " -0.0559855   0.0619029   0.0353677  -0.0334999  -0.0226966  -0.00395828\n",
      " -0.0283532   0.0217597  -0.0418534   0.109104    0.0736382   0.00400721\n",
      " -0.0209592  -0.0116593   0.0260413  -0.0188025   0.0445063  -0.0389139\n",
      "  0.0402938   0.0368409   0.116023    0.0378068   0.0615779   0.0601903\n",
      "  0.0328234  -0.0483939   0.0331058  -0.00478472 -0.0229684   0.0221889\n",
      " -0.0747123   0.0113791   0.0517195  -0.0209997  -0.0373122   0.0159027\n",
      "  0.0738867  -0.0272447  -0.15535    -0.0287317   0.0143244   0.093508\n",
      "  0.0261212  -0.0315336  -0.0178931   0.049338    0.0325233  -0.101776\n",
      " -0.0343944  -0.0715855  -0.0756756   0.0464864  -0.0495292  -0.028531\n",
      "  0.101704    0.0813674   0.0523961  -0.084437    0.0233384  -0.0965087\n",
      "  0.0465284   0.0650641   0.0544679  -0.0228411  -0.0875562   0.0265608\n",
      "  0.0288188   0.0208174   0.00985346 -0.0303444  -0.0890547  -0.0215269\n",
      "  0.0327438  -0.0529924   0.00414673  0.0457334  -0.110981    0.0217876\n",
      "  0.0403129  -0.0348963   0.0596399   0.0637683   0.0753927   0.0261392\n",
      "  0.0963978   0.0493342  -0.0457792   0.139566    0.0176336   0.0570405\n",
      "  0.0442731   0.0580038   0.025341    0.0158113  -0.0421707   0.0131787\n",
      "  0.0332905   0.0258399   0.00777283 -0.0308482   0.0425262   0.0304446\n",
      "  0.0451829   0.0545712  -0.0620749   0.0183671   0.0777092  -0.0712911\n",
      "  0.0261243   0.0453817  -0.0835004   0.030951    0.0442502  -0.0329491\n",
      "  0.00636268 -0.0280386  -0.00686612 -0.0137984  -0.00504542 -0.03038\n",
      "  0.0423121  -0.0174176   0.204825   -0.00423121  0.0312328   0.0658707\n",
      " -0.0933092  -0.0154333   0.124245   -0.00172716 -0.00903085 -0.0510735\n",
      "  0.043738    0.0508174  -0.0500682  -0.0769485  -0.0323516   0.0213243\n",
      "  0.00939705 -0.0118726   0.0653929   0.00626215 -0.002706   -0.0148779\n",
      " -0.0157559   0.051838    0.0172559   0.151172    0.0323375   0.0495406\n",
      "  0.0283223   0.0191622   0.0516239  -0.0264136   0.00160694 -0.0751252\n",
      "  0.0199661   0.08394     0.0292538  -0.0523349  -0.0344334   0.0151347\n",
      " -0.0753545   0.0338726   0.0202455  -0.0582905  -0.132062    0.0284675\n",
      " -0.106914   -0.0284862  -0.0819485   0.0557218   0.0720403  -0.0460812\n",
      "  0.0737643   0.0343738   0.0423733   0.0914132  -0.0222386  -0.0583173\n",
      " -0.0191118  -0.113936   -0.0627973  -0.0202704   0.021929   -0.0464596\n",
      " -0.104777   -0.0475453   0.116076    0.00148308 -0.0448274   0.114979\n",
      " -0.00536576  0.00198231  0.00550757  0.0214129  -0.0920783   0.0133217\n",
      " -0.0657102  -0.0161932  -0.0667728  -0.0910844  -0.00707751 -0.00573922\n",
      " -0.0809584   0.0643034   0.0480154  -0.0452708   0.112357    0.116573\n",
      "  0.140782   -0.00062847  0.0514061  -0.0682063  -0.0157249   0.111057\n",
      " -0.0129283  -0.0190529  -0.0416547   0.0220406  -0.00198602 -0.0466087\n",
      "  0.0705343  -0.0166271   0.0500376   0.123882    0.0188373  -0.0474459\n",
      " -0.0897274  -0.0248861  -0.0506492   0.139134    0.00869064 -0.0600451\n",
      " -0.0222352   0.0452746  -0.00452899  0.0167024   0.01515    -0.0995897\n",
      "  0.124891    0.0685312   0.0421631  -0.0172826   0.0846549   0.0579083\n",
      "  0.0233793  -0.0212448  -0.0287898  -0.038436    0.0443993   0.0742001\n",
      " -0.0387648   0.0815203   0.0154142  -0.104777   -0.0444681  -0.0341807\n",
      " -0.0278108  -0.0995515   0.0322037  -0.00061475  0.0652897  -0.0135747 ]\n",
      "[ 0.0232281   0.0406795   0.00814886  0.126484    0.0048425   0.0333417\n",
      "  0.0743128  -0.0851157   0.00038209  0.0549285   0.00221096  0.115641\n",
      " -0.0400891  -0.0128689   0.0633394  -0.0238348   0.059047   -0.0107363\n",
      "  0.013712   -0.0946492  -0.0692117   0.0102299  -0.093025   -0.00856172\n",
      "  0.0313138   0.0376017   0.132739   -0.0344182  -0.0586819  -0.0228971\n",
      " -0.013827    0.0270869  -0.0504109   0.0425766  -0.0566516  -0.0339159\n",
      " -0.0263659   0.13468     0.00186968 -0.0287336   0.00435798 -0.0616811\n",
      " -0.0103132   0.0311043   0.0121752   0.023832    0.0553448  -0.0332147\n",
      "  0.00575388 -0.0254289   0.018202   -0.0731971   0.0113894   0.0140133\n",
      " -0.0942534   0.0263604  -0.0050793   0.0877805  -0.14708     0.077684\n",
      "  0.00313285 -0.0254989   0.076268    0.0315479   0.0129279   0.0967135\n",
      " -0.0923767   0.0111294  -0.0234328  -0.0198463  -0.0592107   0.0411708\n",
      "  0.00173893 -0.0584635  -0.115357   -0.028853    0.0748656   0.0656938\n",
      "  0.0324204  -0.050578    0.00052144  0.0363153  -0.0195645   0.0215961\n",
      " -0.0328725  -0.0382125  -0.0223833   0.0707165  -0.00495237 -0.0170337\n",
      " -0.0189592  -0.030699    0.00475685 -0.11388     0.00703411 -0.0335945\n",
      "  0.0542836   0.00303202  0.119817    0.00362914  0.0893979   0.0317403\n",
      "  0.0637864  -0.0625717  -0.0565936  -0.0447092  -0.0246564   0.0164475\n",
      " -0.0256783  -0.0460263  -0.0280003  -0.0257592  -0.0750533   0.00175455\n",
      "  0.0693277   0.00044122  0.0230715   0.0515744   0.0373765   0.0747018\n",
      "  0.0590913   0.0371957  -0.0700579   0.100119    0.00769607  0.0850133\n",
      "  0.110065    0.044774    0.0295887   0.0720199  -0.0329998   0.0078851\n",
      " -0.047183    0.103046    0.0535568  -0.0169818  -0.0143275   0.0375983\n",
      " -0.00749953 -0.027453   -0.0172941   0.0893228   0.0819936  -0.0333331\n",
      "  0.0478825   0.0383251  -0.111027   -0.0116323  -0.0120414   0.0167171\n",
      " -0.0258943  -0.118493    0.00547238 -0.016471   -0.00727979 -0.0450163\n",
      "  0.0393726   0.00821027  0.150397    0.0129688   0.0491381   0.0994944\n",
      " -0.0824815  -0.00167348  0.0531713   0.0347765  -0.0354589  -0.0981227\n",
      "  0.09714     0.142637   -0.014434   -0.0175022  -0.033631    0.116323\n",
      " -0.0381954   0.06726    -0.008617   -0.0237689   0.013221    0.0206963\n",
      " -0.0307017   0.00295897  0.0513253   0.0870196   0.0951917   0.0466404\n",
      "  0.018923    0.0204455   0.113215   -0.0515061  -0.0103162  -0.0607598\n",
      " -0.0592278   0.00244356 -0.00028886 -0.0934447  -0.0173852   0.0599409\n",
      " -0.0258571  -0.0285261   0.050967   -0.0596646  -0.0388744  -0.0426005\n",
      " -0.017882   -0.0561637  -0.0390416   0.0643562   0.0848427   0.0187599\n",
      "  0.0498308   0.0117896   0.0230493   0.0741695   0.0349198  -0.0116354\n",
      " -0.0478142  -0.0712487  -0.0211791  -0.0189919  -0.00788066 -0.0128614\n",
      " -0.082461   -0.0309238  -0.0604459   0.070464   -0.0278242   0.00524581\n",
      " -0.00461935 -0.0202063  -0.0667345  -0.0334846  -0.0567267  -0.0687306\n",
      " -0.113235    0.0557713  -0.0340623  -0.100624    0.0291703   0.00734257\n",
      " -0.00059941 -0.00881149  0.0671474  -0.0225951  -0.0108086   0.171859\n",
      "  0.096072   -0.0192335   0.0573614   0.00518269  0.013411    0.0656494\n",
      " -0.0833652  -0.0489539   0.00559965  0.0251996  -0.0277905  -0.0732926\n",
      "  0.0428734  -0.0544269   0.0114016   0.0998527  -0.0774145   0.00432488\n",
      " -0.117145   -0.0323712  -0.0557099   0.129402   -0.0403655   0.0411264\n",
      "  0.0601764   0.105776    0.0527379  -0.0144333   0.0037414  -0.0233015\n",
      "  0.0728081   0.0440506  -0.0686146   0.0457431   0.0691469   0.00605824\n",
      "  0.0349505   0.0434057  -0.0166202   0.0426858   0.0307549   0.0522636\n",
      " -0.172674    0.135554    0.0268552   0.029893   -0.0305673   0.0626126\n",
      "  0.0385742   0.017581   -0.0933594  -0.0207052   0.128395   -0.0459614 ]\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print(word_vec_dict['is'])\n",
    "print(word_vec_dict['are'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4697\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0914 - acc: 0.9733Epoch 00001: val_loss improved from inf to 0.05336, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 146s 1ms/step - loss: 0.0914 - acc: 0.9733 - val_loss: 0.0534 - val_acc: 0.9802\n",
      "Epoch 2/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9812Epoch 00002: val_loss improved from 0.05336 to 0.04934, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 144s 1ms/step - loss: 0.0522 - acc: 0.9812 - val_loss: 0.0493 - val_acc: 0.9816\n",
      "Epoch 3/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9820Epoch 00003: val_loss improved from 0.04934 to 0.04768, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 145s 1ms/step - loss: 0.0491 - acc: 0.9820 - val_loss: 0.0477 - val_acc: 0.9820\n",
      "Epoch 4/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9825Epoch 00004: val_loss improved from 0.04768 to 0.04596, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 145s 1ms/step - loss: 0.0468 - acc: 0.9825 - val_loss: 0.0460 - val_acc: 0.9827\n",
      "Epoch 5/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9831Epoch 00005: val_loss improved from 0.04596 to 0.04473, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 146s 1ms/step - loss: 0.0450 - acc: 0.9831 - val_loss: 0.0447 - val_acc: 0.9830\n",
      "Epoch 6/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9833Epoch 00006: val_loss improved from 0.04473 to 0.04417, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 147s 1ms/step - loss: 0.0440 - acc: 0.9833 - val_loss: 0.0442 - val_acc: 0.9832\n",
      "Epoch 7/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9837Epoch 00007: val_loss improved from 0.04417 to 0.04367, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 147s 1ms/step - loss: 0.0429 - acc: 0.9837 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "Epoch 8/8\n",
      "127488/127656 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9838Epoch 00008: val_loss improved from 0.04367 to 0.04342, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 146s 1ms/step - loss: 0.0421 - acc: 0.9839 - val_loss: 0.0434 - val_acc: 0.9833\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0921 - acc: 0.9724Epoch 00001: val_loss improved from inf to 0.05309, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 147s 1ms/step - loss: 0.0921 - acc: 0.9724 - val_loss: 0.0531 - val_acc: 0.9806\n",
      "Epoch 2/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9808Epoch 00002: val_loss improved from 0.05309 to 0.05027, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0529 - acc: 0.9808 - val_loss: 0.0503 - val_acc: 0.9815\n",
      "Epoch 3/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9816Epoch 00003: val_loss improved from 0.05027 to 0.04753, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0499 - acc: 0.9816 - val_loss: 0.0475 - val_acc: 0.9824\n",
      "Epoch 4/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0476 - acc: 0.9822Epoch 00004: val_loss improved from 0.04753 to 0.04582, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 147s 1ms/step - loss: 0.0476 - acc: 0.9823 - val_loss: 0.0458 - val_acc: 0.9829\n",
      "Epoch 5/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9827Epoch 00005: val_loss improved from 0.04582 to 0.04481, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 147s 1ms/step - loss: 0.0459 - acc: 0.9827 - val_loss: 0.0448 - val_acc: 0.9830\n",
      "Epoch 6/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9830Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0447 - acc: 0.9830 - val_loss: 0.0449 - val_acc: 0.9832\n",
      "Epoch 7/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9834Epoch 00007: val_loss improved from 0.04481 to 0.04440, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 147s 1ms/step - loss: 0.0437 - acc: 0.9834 - val_loss: 0.0444 - val_acc: 0.9834\n",
      "Epoch 8/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9836Epoch 00008: val_loss improved from 0.04440 to 0.04374, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 147s 1ms/step - loss: 0.0429 - acc: 0.9836 - val_loss: 0.0437 - val_acc: 0.9836\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0911 - acc: 0.9723Epoch 00001: val_loss improved from inf to 0.05072, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 147s 1ms/step - loss: 0.0910 - acc: 0.9724 - val_loss: 0.0507 - val_acc: 0.9819\n",
      "Epoch 2/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0532 - acc: 0.9806Epoch 00002: val_loss improved from 0.05072 to 0.04876, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0532 - acc: 0.9806 - val_loss: 0.0488 - val_acc: 0.9823\n",
      "Epoch 3/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9818Epoch 00003: val_loss improved from 0.04876 to 0.04432, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0487 - acc: 0.9818 - val_loss: 0.0443 - val_acc: 0.9835\n",
      "Epoch 4/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9824Epoch 00004: val_loss improved from 0.04432 to 0.04340, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0463 - acc: 0.9824 - val_loss: 0.0434 - val_acc: 0.9838\n",
      "Epoch 5/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9829Epoch 00005: val_loss improved from 0.04340 to 0.04294, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 145s 1ms/step - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0429 - val_acc: 0.9839\n",
      "Epoch 6/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9833Epoch 00006: val_loss improved from 0.04294 to 0.04260, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0438 - acc: 0.9833 - val_loss: 0.0426 - val_acc: 0.9840\n",
      "Epoch 7/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9834Epoch 00007: val_loss improved from 0.04260 to 0.04202, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 145s 1ms/step - loss: 0.0431 - acc: 0.9834 - val_loss: 0.0420 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9837Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0421 - acc: 0.9837 - val_loss: 0.0423 - val_acc: 0.9839\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0912 - acc: 0.9726Epoch 00001: val_loss improved from inf to 0.05308, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 146s 1ms/step - loss: 0.0912 - acc: 0.9727 - val_loss: 0.0531 - val_acc: 0.9807\n",
      "Epoch 2/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0535 - acc: 0.9806Epoch 00002: val_loss improved from 0.05308 to 0.04879, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 143s 1ms/step - loss: 0.0535 - acc: 0.9806 - val_loss: 0.0488 - val_acc: 0.9820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9816Epoch 00003: val_loss improved from 0.04879 to 0.04849, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0497 - acc: 0.9816 - val_loss: 0.0485 - val_acc: 0.9818\n",
      "Epoch 4/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0475 - acc: 0.9822Epoch 00004: val_loss improved from 0.04849 to 0.04536, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0475 - acc: 0.9822 - val_loss: 0.0454 - val_acc: 0.9832\n",
      "Epoch 5/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9828Epoch 00005: val_loss improved from 0.04536 to 0.04445, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0462 - acc: 0.9828 - val_loss: 0.0444 - val_acc: 0.9833\n",
      "Epoch 6/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9830Epoch 00006: val_loss improved from 0.04445 to 0.04394, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0451 - acc: 0.9830 - val_loss: 0.0439 - val_acc: 0.9834\n",
      "Epoch 7/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9833Epoch 00007: val_loss improved from 0.04394 to 0.04340, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0442 - acc: 0.9833 - val_loss: 0.0434 - val_acc: 0.9837\n",
      "Epoch 8/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9835Epoch 00008: val_loss improved from 0.04340 to 0.04276, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0433 - acc: 0.9835 - val_loss: 0.0428 - val_acc: 0.9839\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0941 - acc: 0.9722Epoch 00001: val_loss improved from inf to 0.05322, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 143s 1ms/step - loss: 0.0940 - acc: 0.9722 - val_loss: 0.0532 - val_acc: 0.9809\n",
      "Epoch 2/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0537 - acc: 0.9807Epoch 00002: val_loss improved from 0.05322 to 0.04950, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0536 - acc: 0.9807 - val_loss: 0.0495 - val_acc: 0.9820\n",
      "Epoch 3/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0499 - acc: 0.9817Epoch 00003: val_loss improved from 0.04950 to 0.04757, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0500 - acc: 0.9817 - val_loss: 0.0476 - val_acc: 0.9821\n",
      "Epoch 4/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9823Epoch 00004: val_loss improved from 0.04757 to 0.04624, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0477 - acc: 0.9823 - val_loss: 0.0462 - val_acc: 0.9829\n",
      "Epoch 5/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9826Epoch 00005: val_loss improved from 0.04624 to 0.04473, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0463 - acc: 0.9826 - val_loss: 0.0447 - val_acc: 0.9832\n",
      "Epoch 6/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9829Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0450 - acc: 0.9829 - val_loss: 0.0457 - val_acc: 0.9830\n",
      "Epoch 7/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9833Epoch 00007: val_loss improved from 0.04473 to 0.04359, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0438 - acc: 0.9833 - val_loss: 0.0436 - val_acc: 0.9837\n",
      "Epoch 8/8\n",
      "127488/127657 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9836Epoch 00008: val_loss improved from 0.04359 to 0.04252, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 141s 1ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0425 - val_acc: 0.9837\n",
      "-------------------------------\n",
      "0 0.0945312512355 0.964592563812\n",
      "1 0.0222882124998 0.990775266182\n",
      "2 0.0497816613383 0.981619467196\n",
      "3 0.00884113191553 0.997223806331\n",
      "4 0.0616117171502 0.975189727457\n",
      "5 0.0202950420096 0.992880911945\n",
      "final 0.0428915026915 0.983713623821\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=99):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=2*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 256\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(5)\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/gru_muse_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/gru_muse_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "\n",
    "# 0.04336 0.04367 0.04425"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.052,  0.   ,  0.005,  0.   ,  0.01 ,  0.001],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.79 ,  0.009,  0.396,  0.001,  0.213,  0.004],\n",
       "       [ 0.014,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.005,  0.   ,  0.001,  0.   ,  0.001,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
