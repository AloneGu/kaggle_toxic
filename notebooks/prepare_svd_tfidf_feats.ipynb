{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import re\n",
    "  \n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values\n",
    "train['comment_text'] = train['comment_text'].fillna('nan')\n",
    "test['comment_text'] = test['comment_text'].fillna('nan')\n",
    "print('load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def simple_eval(x,y,model_f):\n",
    "    y_cnt = len(y)\n",
    "    split_idx = y_cnt * 4 // 5  # 80%\n",
    "    train_x,test_x = x[:split_idx],x[split_idx:]\n",
    "    train_y,test_y = y[:split_idx],y[split_idx:]\n",
    "    for i in range(6):\n",
    "        model = model_f()\n",
    "        model.fit(train_x,train_y[:,i])\n",
    "        train_pred = model.predict_proba(train_x)\n",
    "        val_pred = model.predict_proba(test_x)\n",
    "        print(list_classes[i])\n",
    "        print('train log loss',log_loss(train_y[:,i],train_pred))\n",
    "        print('valid log loss',log_loss(test_y[:,i],val_pred))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 15000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec1 = TfidfVectorizer(sublinear_tf=True,\n",
    "                            strip_accents='unicode',\n",
    "                            analyzer='word',\n",
    "                            token_pattern=r'\\w{1,}',\n",
    "                            ngram_range=(1, 1),\n",
    "                            max_features=15000)\n",
    "train_tfidf1 = tf_vec1.fit_transform(train['comment_text'].values)\n",
    "test_tfidf1 = tf_vec1.transform(test['comment_text'].values)\n",
    "\n",
    "print(train_tfidf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (159571, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd1 = svd_obj.fit_transform(train_tfidf1)\n",
    "test_svd1 = svd_obj.transform(test_tfidf1)\n",
    "\n",
    "print(type(train_svd1),train_svd1.shape)\n",
    "with open('../features/tfidf_feat1.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd1,test_svd1],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd1,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_eval(train_tfidf1,train_y,LogisticRegression)\n",
    "#print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 20000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec2 = TfidfVectorizer(sublinear_tf=True,\n",
    "                        strip_accents='unicode',\n",
    "                        analyzer='char',\n",
    "                        ngram_range=(1, 5),\n",
    "                        max_features=20000\n",
    "                         )\n",
    "train_tfidf2 = tf_vec2.fit_transform(train['comment_text'].values)\n",
    "test_tfidf2 = tf_vec2.transform(test['comment_text'].values)\n",
    "print(train_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (159571, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd2 = svd_obj.fit_transform(train_tfidf2)\n",
    "test_svd2 = svd_obj.transform(test_tfidf2)\n",
    "\n",
    "print(type(train_svd2),train_svd2.shape)\n",
    "with open('../features/tfidf_feat2.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd2,test_svd2],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd2,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "train log loss 0.0998123893448\n",
      "valid log loss 0.112494814028\n",
      "severe_toxic\n",
      "train log loss 0.0221921968053\n",
      "valid log loss 0.0248359643588\n",
      "obscene\n",
      "train log loss 0.0535317480525\n",
      "valid log loss 0.0605432076228\n",
      "threat\n",
      "train log loss 0.00907702751104\n",
      "valid log loss 0.00971702054356\n",
      "insult\n",
      "train log loss 0.0650014251088\n",
      "valid log loss 0.0741674836352\n",
      "identity_hate\n",
      "train log loss 0.02110441208\n",
      "valid log loss 0.0264481163121\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "simple_eval(train_tfidf2,train_y,LogisticRegression)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.106869747783 0.0765935418469\n",
      "1 0.0263652997404 0.0178789749717\n",
      "2 0.0614181887651 0.0396472928721\n",
      "3 0.00972236584927 0.00514685887537\n",
      "4 0.075266301171 0.0521699030141\n",
      "5 0.0244159962879 0.0162123495819\n",
      "===========this fold done\n",
      "0 0.10657921818 0.0767299325569\n",
      "1 0.0267222155878 0.0178033427337\n",
      "2 0.0586618223194 0.0412339861803\n",
      "3 0.00945594330306 0.00514799394355\n",
      "4 0.0753552088324 0.0527258089475\n",
      "5 0.0248968924598 0.0158238339973\n",
      "===========this fold done\n",
      "0 0.10675209651 0.076658387922\n",
      "1 0.0267690293319 0.017750692984\n",
      "2 0.0595751698665 0.0407993925959\n",
      "3 0.00843140716315 0.00553344660806\n",
      "4 0.0741067501394 0.0530074662521\n",
      "5 0.0262785261672 0.0156536020456\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def gen_base_lr_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = LogisticRegression(C=4.0, solver='sag')\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/lr_feat1.pkl','wb') as fout:\n",
    "    lr_feat1 = gen_base_lr_feat(train_tfidf1,train_y,test_tfidf1,3,rnd=3)\n",
    "    pickle.dump(lr_feat1,fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (159571, 20000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (159571, 35000)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del tf_vec2,tf_vec1,svd_obj\n",
    "except:\n",
    "    pass\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "print(type(train_tfidf2),train_tfidf2.shape)\n",
    "comb_train = csr_matrix(hstack((train_tfidf2,train_tfidf1)))\n",
    "print(type(comb_train),comb_train.shape)\n",
    "comb_test = csr_matrix(hstack((test_tfidf2,test_tfidf1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.099639656659 0.054628393736\n",
      "1 0.0249534202316 0.0127154611361\n",
      "2 0.0564826678753 0.0268183287149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.00938601415621 0.00325840500521\n",
      "4 0.0706398569588 0.0382229957783\n",
      "5 0.0228506563076 0.0104947463601\n",
      "===========this fold done\n",
      "0 0.09861080818 0.0550626182905\n",
      "1 0.0260374885791 0.0124551870458\n",
      "2 0.0536054008344 0.0282680901347\n",
      "3 0.00894000712716 0.00332682671695\n",
      "4 0.072252116926 0.0380555492809\n",
      "5 0.0236037952282 0.0102168299594\n",
      "===========this fold done\n",
      "0 0.0993468768689 0.0547179527111\n",
      "1 0.0257323118392 0.0123429917305\n",
      "2 0.0546772528595 0.0278125910116\n",
      "3 0.00791669641523 0.00360682022169\n",
      "4 0.0706789480791 0.0384201282555\n",
      "5 0.0240889645387 0.0102627641676\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/lr_feat2.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_lr_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,3,rnd=11)\n",
    "    pickle.dump(lr_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.135064637786 0.119426880993\n",
      "1 0.0280498515944 0.022540605607\n",
      "2 0.0822088496144 0.070409846418\n",
      "3 0.0174418668927 0.0114750738812\n",
      "4 0.0859010193469 0.0748272828438\n",
      "5 0.0292018964185 0.0244956319078\n",
      "===========this fold done\n",
      "0 0.136402005569 0.119277291188\n",
      "1 0.0278692392234 0.0225375090818\n",
      "2 0.0831849543181 0.0708722878025\n",
      "3 0.0140445220646 0.0120016718168\n",
      "4 0.0884407651275 0.0742108440618\n",
      "5 0.0303573563701 0.0239013261819\n",
      "===========this fold done\n",
      "0 0.135258956974 0.119395227808\n",
      "1 0.0278011896406 0.0227454464303\n",
      "2 0.0829899448268 0.0708068473162\n",
      "3 0.0142391699087 0.0119526680429\n",
      "4 0.087647809208 0.0748190278716\n",
      "5 0.0322763447175 0.0235071268866\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def gen_base_mnb_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = MultinomialNB(alpha=0.2)\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/mnb_feat1.pkl','wb') as fout:\n",
    "    _feat1 = gen_base_mnb_feat(train_tfidf1,train_y,test_tfidf1,3)\n",
    "    pickle.dump(_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.186172501784 0.17796216859\n",
      "1 0.138356732746 0.139951829086\n",
      "2 0.156714015863 0.143592378619\n",
      "3 0.0372164529446 0.0324088537377\n",
      "4 0.180579797164 0.167767305069\n",
      "5 0.121608716466 0.117026055627\n",
      "===========this fold done\n",
      "0 0.19673424959 0.176980401368\n",
      "1 0.152391556595 0.138566231187\n",
      "2 0.149583943395 0.143122222898\n",
      "3 0.0420097037746 0.0370923417515\n",
      "4 0.176807723136 0.165516545156\n",
      "5 0.1301960023 0.11808540472\n",
      "===========this fold done\n",
      "0 0.196710879649 0.178708104766\n",
      "1 0.141026969453 0.14040481386\n",
      "2 0.158906866955 0.144656850901\n",
      "3 0.0454890042183 0.0409103477265\n",
      "4 0.181096771759 0.16944895783\n",
      "5 0.117908591271 0.112924617458\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/mnb_feat2.pkl','wb') as fout:\n",
    "    _feat2 = gen_base_mnb_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,3,rnd=29)\n",
    "    pickle.dump(_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
