{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "import pickle\n",
    "import re\n",
    "  \n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values\n",
    "train['comment_text'] = train['comment_text'].fillna('nan')\n",
    "test['comment_text'] = test['comment_text'].fillna('nan')\n",
    "print('load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 15000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec1 = TfidfVectorizer(sublinear_tf=True,\n",
    "                            strip_accents='unicode',\n",
    "                            analyzer='word',\n",
    "                            token_pattern=r'\\w{1,}',\n",
    "                            ngram_range=(1, 1),\n",
    "                            max_features=15000)\n",
    "train_tfidf1 = tf_vec1.fit_transform(train['comment_text'].values)\n",
    "test_tfidf1 = tf_vec1.transform(test['comment_text'].values)\n",
    "\n",
    "print(train_tfidf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (159571, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd1 = svd_obj.fit_transform(train_tfidf1)\n",
    "test_svd1 = svd_obj.transform(test_tfidf1)\n",
    "\n",
    "print(type(train_svd1),train_svd1.shape)\n",
    "with open('../features/tfidf_feat1.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd1,test_svd1],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd1,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_eval(train_tfidf1,train_y,LogisticRegression)\n",
    "#print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 20000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec2 = TfidfVectorizer(sublinear_tf=True,\n",
    "                        strip_accents='unicode',\n",
    "                        analyzer='char',\n",
    "                        ngram_range=(1, 5),\n",
    "                        max_features=20000\n",
    "                         )\n",
    "train_tfidf2 = tf_vec2.fit_transform(train['comment_text'].values)\n",
    "test_tfidf2 = tf_vec2.transform(test['comment_text'].values)\n",
    "print(train_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (159571, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd2 = svd_obj.fit_transform(train_tfidf2)\n",
    "test_svd2 = svd_obj.transform(test_tfidf2)\n",
    "\n",
    "print(type(train_svd2),train_svd2.shape)\n",
    "with open('../features/tfidf_feat2.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd2,test_svd2],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd2,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.93791465e-01 6.20853525e-03]\n",
      " [9.80035544e-01 1.99644562e-02]\n",
      " [9.78047041e-01 2.19529591e-02]\n",
      " [9.98319365e-01 1.68063457e-03]\n",
      " [9.29023406e-01 7.09765939e-02]\n",
      " [9.93445038e-01 6.55496179e-03]\n",
      " [7.24703146e-02 9.27529685e-01]\n",
      " [9.41548506e-01 5.84514941e-02]\n",
      " [9.68103493e-01 3.18965074e-02]\n",
      " [9.72104652e-01 2.78953476e-02]\n",
      " [9.99899362e-01 1.00637849e-04]\n",
      " [9.44155988e-01 5.58440124e-02]\n",
      " [9.45490459e-01 5.45095412e-02]\n",
      " [9.77286823e-01 2.27131766e-02]\n",
      " [9.65986600e-01 3.40134002e-02]\n",
      " [9.39576365e-01 6.04236353e-02]\n",
      " [8.59454509e-01 1.40545491e-01]\n",
      " [9.97199600e-01 2.80039959e-03]\n",
      " [9.95466470e-01 4.53352967e-03]\n",
      " [9.87808196e-01 1.21918045e-02]]\n"
     ]
    }
   ],
   "source": [
    "test_m = LogisticRegression()\n",
    "test_m.fit(train_tfidf1,train_y[:,0])\n",
    "res = test_m.predict_proba(train_tfidf1)\n",
    "print(res[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.10979960347266333 0.09896561694173542\n",
      "1 0.02720006099187827 0.023151659144558513\n",
      "2 0.062137294769084014 0.05427196128945869\n",
      "3 0.011000500431473978 0.00847332471862434\n",
      "4 0.07881512428087477 0.0663323385926536\n",
      "5 0.025017944762569293 0.022851846136550197\n",
      "===========this fold done\n",
      "0 0.11462564116130843 0.09850167256894128\n",
      "1 0.027545732809584063 0.02311993734314232\n",
      "2 0.06573899590521984 0.05380418583673979\n",
      "3 0.01018402448560268 0.00853293946760875\n",
      "4 0.07615336435068107 0.06652579799403885\n",
      "5 0.026961266985451943 0.02267936083061755\n",
      "===========this fold done\n",
      "0 0.11197384131136078 0.09891050977666685\n",
      "1 0.0255033002013384 0.023307466432269896\n",
      "2 0.06152059168598452 0.05433685600214215\n",
      "3 0.010495899840595454 0.008544557243353379\n",
      "4 0.07622853510080775 0.06646158983346043\n",
      "5 0.026943229044574846 0.02267943178521963\n",
      "===========this fold done\n",
      "0 0.11316112813087548 0.09858795845238326\n",
      "1 0.025025442075033565 0.0233568464012065\n",
      "2 0.06605445371493615 0.05387568493517784\n",
      "3 0.012082146072289222 0.008394025002832753\n",
      "4 0.07809637356843185 0.06639877891426613\n",
      "5 0.0254159558578662 0.022772443321312792\n",
      "===========this fold done\n",
      "0 0.11211962384996477 0.09882877255880759\n",
      "1 0.0279951804786926 0.023090469162544137\n",
      "2 0.0597436590591387 0.0546168444371174\n",
      "3 0.010182063986498069 0.008481375899071207\n",
      "4 0.07795006171733014 0.06646806726209495\n",
      "5 0.024833304257454666 0.0228001934076366\n",
      "===========this fold done\n",
      "0 0.11006499559211368 0.09888141145772031\n",
      "1 0.02829879776965882 0.023023473840213957\n",
      "2 0.061599162011811225 0.05448481306899078\n",
      "3 0.010448522325659714 0.008471186568072973\n",
      "4 0.07324699497257024 0.06691032496134741\n",
      "5 0.024711473267135967 0.022796161028282944\n",
      "===========this fold done\n",
      "0 0.11280459438669899 0.09870178083658343\n",
      "1 0.024594213322873645 0.023385067911260455\n",
      "2 0.05965163102768948 0.05453450902949046\n",
      "3 0.008945186683399655 0.008603123243165284\n",
      "4 0.07269485982193276 0.0669332577642544\n",
      "5 0.02712052334492516 0.022651945021757342\n",
      "===========this fold done\n",
      "0 0.1109151311060362 0.09889949369251683\n",
      "1 0.029733004824668195 0.022860823232997315\n",
      "2 0.06281392372849481 0.05423918825316018\n",
      "3 0.008807878954497535 0.00864650469578899\n",
      "4 0.07526494951755205 0.06656646857052054\n",
      "5 0.026819506755092887 0.022645440668200952\n",
      "===========this fold done\n",
      "0 0.11215299184339358 0.09876173113421394\n",
      "1 0.025314283112055958 0.023318611975596124\n",
      "2 0.05906144589529447 0.05464302137896752\n",
      "3 0.007999886745214825 0.008716339993046059\n",
      "4 0.07272640600563801 0.06699315575636451\n",
      "5 0.029915318975085925 0.022457750693457106\n",
      "===========this fold done\n",
      "0 0.11093904943508962 0.09883092270698723\n",
      "1 0.026940934627997456 0.023194382093014946\n",
      "2 0.06434634922543371 0.05411469327665484\n",
      "3 0.010829602487432565 0.008501243132434607\n",
      "4 0.0787006952906757 0.06633740179438384\n",
      "5 0.02682355433603507 0.022659678989855967\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "FOLD_CNT = 10\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def gen_base_lr_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = LogisticRegression(solver='sag')\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            #print(hold_out_pred[:10])\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index,i] = list(hold_out_pred[:,1].flatten())\n",
    "            # print('value', np.sum(train_pred[:,i]))\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    #print(train_pred[:10])\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "\n",
    "\n",
    "with open('../features/lr_feat1.pkl','wb') as fout:\n",
    "    lr_feat1 = gen_base_lr_feat(train_tfidf1,train_y,test_tfidf1,FOLD_CNT,rnd=3)\n",
    "    pickle.dump(lr_feat1,fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (159571, 20000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (159571, 35000)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del tf_vec2,tf_vec1,svd_obj\n",
    "except:\n",
    "    pass\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "print(type(train_tfidf2),train_tfidf2.shape)\n",
    "comb_train = csr_matrix(hstack((train_tfidf2,train_tfidf1)))\n",
    "print(type(comb_train),comb_train.shape)\n",
    "comb_test = csr_matrix(hstack((test_tfidf2,test_tfidf1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.098774754641582 0.07915151355358084\n",
      "1 0.025184978556446778 0.019041449745457722\n",
      "2 0.05369561018472291 0.042121817279004936\n",
      "3 0.009941324935575676 0.006501461388060503\n",
      "4 0.0715878844738845 0.054559269860794526\n",
      "5 0.021965914807135157 0.01762462728546486\n",
      "===========this fold done\n",
      "0 0.10164050167789963 0.0789221390183588\n",
      "1 0.025140557773119618 0.01906653365602601\n",
      "2 0.058247756744541926 0.04163096902367481\n",
      "3 0.00910309727472782 0.00655957286087448\n",
      "4 0.06962510456143833 0.054722222963508635\n",
      "5 0.023441650962694533 0.017468953731112593\n",
      "===========this fold done\n",
      "0 0.09692995202090067 0.07945979069668047\n",
      "1 0.023333988370940195 0.019201990765704505\n",
      "2 0.05343418355482056 0.0421398917379652\n",
      "3 0.009145482384380652 0.006555421262305305\n",
      "4 0.06835634614720004 0.05475767677335491\n",
      "5 0.02373366752738558 0.017469133496867083\n",
      "===========this fold done\n",
      "0 0.09941483109188719 0.07907929350847934\n",
      "1 0.023589621237725435 0.019200296095447187\n",
      "2 0.05751688497984832 0.04180958726787386\n",
      "3 0.010651367480790597 0.006473163132128603\n",
      "4 0.0710322653825429 0.05458966595367894\n",
      "5 0.022182813860883444 0.01756014592039203\n",
      "===========this fold done\n",
      "0 0.0996491854656685 0.07916245679406118\n",
      "1 0.026548754748249106 0.018924303789932036\n",
      "2 0.05111703201549521 0.04249077458604021\n",
      "3 0.00926945080121481 0.006520777491552169\n",
      "4 0.07102382653055915 0.05462322873730003\n",
      "5 0.022738041113484236 0.01746746325316931\n",
      "===========this fold done\n",
      "0 0.0968875033598734 0.0792698787959561\n",
      "1 0.025847787396662922 0.018999870745005698\n",
      "2 0.05295180067381403 0.042341154467556104\n",
      "3 0.009219767507079243 0.006523275403160037\n",
      "4 0.06723005734907342 0.054962615720077324\n",
      "5 0.022064297486981195 0.017551961434409402\n",
      "===========this fold done\n",
      "0 0.09897241408036081 0.07918955283315542\n",
      "1 0.022933370167239862 0.01921132235614432\n",
      "2 0.05208410374304598 0.04229165763764713\n",
      "3 0.008105851581106816 0.006612206077522728\n",
      "4 0.06704578355235616 0.054954972146004234\n",
      "5 0.024136076434799567 0.017436858180511883\n",
      "===========this fold done\n",
      "0 0.09781975122806764 0.07932250144276748\n",
      "1 0.02789955100470501 0.018751168850470862\n",
      "2 0.05570292014891504 0.04198116876458293\n",
      "3 0.007971323621193167 0.0066264454121714575\n",
      "4 0.0695129114043963 0.054648863388750024\n",
      "5 0.0233928879419773 0.017462711009894966\n",
      "===========this fold done\n",
      "0 0.09834733936922872 0.07922696928100517\n",
      "1 0.023235335230262385 0.019214903934645367\n",
      "2 0.05101427975362005 0.042454625320723194\n",
      "3 0.007003103246362873 0.006728759773031303\n",
      "4 0.06542170555785053 0.05513701972489748\n",
      "5 0.02573108678277178 0.017360109328593366\n",
      "===========this fold done\n",
      "0 0.09924344938162269 0.07904923263464271\n",
      "1 0.025136634140136952 0.019062319670117376\n",
      "2 0.05500729707793402 0.042091997829367685\n",
      "3 0.009253360225796549 0.006596534764915157\n",
      "4 0.07264598581444423 0.054416851369714796\n",
      "5 0.023307230339849445 0.01751739105008674\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/lr_feat2.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_lr_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,FOLD_CNT,rnd=11)\n",
    "    pickle.dump(lr_feat2,fout)\n",
    "\n",
    "# pre first fold\n",
    "# 0 0.102241368889 0.0801105597102\n",
    "# value 5060.86889559\n",
    "# 1 0.0246173591039 0.0192587631549\n",
    "# value 526.726637916\n",
    "# 2 0.056389939991 0.0419326132626\n",
    "# value 2788.22846853\n",
    "# 3 0.0102994295764 0.00665408777849\n",
    "# value 148.649550485\n",
    "# 4 0.0701977896968 0.0548491407929\n",
    "# value 2614.47061102\n",
    "# 5 0.0234527531648 0.0180436061952\n",
    "# value 462.585903392\n",
    "# ===========this fold done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.14000874149101825 0.13726698257617057\n",
      "1 0.027039264357669908 0.025546707668226757\n",
      "2 0.07576018563509507 0.07267267852502338\n",
      "3 0.012620128711821988 0.01073524095226189\n",
      "4 0.0911816888526827 0.08786882783932934\n",
      "5 0.02985499650055174 0.02550985028338122\n",
      "===========this fold done\n",
      "0 0.15096689362664178 0.1355043751792196\n",
      "1 0.028625319694508863 0.0257714497472438\n",
      "2 0.08066704493352796 0.07223642062251684\n",
      "3 0.011640303024249971 0.010842597880888687\n",
      "4 0.08897328515177895 0.08860494073219534\n",
      "5 0.02812587189372931 0.02517364119950403\n",
      "===========this fold done\n",
      "0 0.1463837220930897 0.13681853513288492\n",
      "1 0.026494556639596354 0.025823140746131883\n",
      "2 0.07342534035611584 0.073192173008876\n",
      "3 0.012409808874573101 0.010766908181262925\n",
      "4 0.1035122550752089 0.08731226634627312\n",
      "5 0.031925065273412295 0.02524823240584563\n",
      "===========this fold done\n",
      "0 0.14319738317461286 0.13684222416699998\n",
      "1 0.0290169345949462 0.025812154217138467\n",
      "2 0.07965439058034172 0.07267193867552764\n",
      "3 0.013188241618561535 0.010691839698499562\n",
      "4 0.09648346398990881 0.08752292051257121\n",
      "5 0.0306076825742491 0.025384690594328966\n",
      "===========this fold done\n",
      "0 0.14515529444068923 0.13672496873184695\n",
      "1 0.029614215518897213 0.025701566410016485\n",
      "2 0.07610503405450728 0.07264173063882906\n",
      "3 0.011743064512990285 0.010917716768381807\n",
      "4 0.09921330021857476 0.08737255973907429\n",
      "5 0.029645879924349763 0.025472794985057813\n",
      "===========this fold done\n",
      "0 0.1410938848042256 0.13775421754724523\n",
      "1 0.02672807458381374 0.025773081438187743\n",
      "2 0.07686852280420409 0.0728126526034661\n",
      "3 0.011932668622194581 0.01080397291989949\n",
      "4 0.09230107294535818 0.08838557943450594\n",
      "5 0.025514061080051664 0.025557902405747682\n",
      "===========this fold done\n",
      "0 0.1433868964141852 0.1366445138594021\n",
      "1 0.025163085856167507 0.026264588802337552\n",
      "2 0.07427710915356106 0.0731081455630749\n",
      "3 0.010843179483400132 0.010979535699704723\n",
      "4 0.08969635027668618 0.0885420826711291\n",
      "5 0.028398795325403323 0.02523860493857137\n",
      "===========this fold done\n",
      "0 0.13930571959922636 0.1369533692407701\n",
      "1 0.030629320052872066 0.025591213906118383\n",
      "2 0.07455962127099448 0.07291264680632786\n",
      "3 0.010470743249286506 0.010983907023994741\n",
      "4 0.09120716443607356 0.0879965018186962\n",
      "5 0.03129166714614998 0.025492788010150137\n",
      "===========this fold done\n",
      "0 0.14980043426623596 0.13621118085753534\n",
      "1 0.027537739390634725 0.02573615377863175\n",
      "2 0.07528129031459299 0.07313480676879325\n",
      "3 0.01026520732782265 0.011056740650258296\n",
      "4 0.08462436084333641 0.0888499049131088\n",
      "5 0.029455394146036023 0.02507934488980684\n",
      "===========this fold done\n",
      "0 0.1433737841046171 0.13673194834937716\n",
      "1 0.026373415766960978 0.026045539057661625\n",
      "2 0.0830508640263075 0.07245058829451502\n",
      "3 0.012201481183988014 0.01081804584138525\n",
      "4 0.09628041160872051 0.08741730036910236\n",
      "5 0.02805276339035532 0.025257085010545303\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "def gen_base_ridge_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = Ridge(alpha=20, copy_X=True, fit_intercept=True, \n",
    "                          solver='auto',max_iter=100,normalize=False, random_state=0,  tol=0.0025)\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict(hold_out_x)\n",
    "            #print(hold_out_pred[:10])\n",
    "            curr_train_pred = model.predict(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index,i] = list(hold_out_pred.flatten())\n",
    "            # print('value', np.sum(train_pred[:,i]))\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict(test_x)\n",
    "            test_pred[:,i] += y_test\n",
    "            #print(test_pred[:5])\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    #print(train_pred[:10])\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/ridge_feat1.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_ridge_feat(train_tfidf1,\n",
    "                                   train_y,\n",
    "                                   test_tfidf1,FOLD_CNT,rnd=12)\n",
    "    pickle.dump(lr_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.12766545335805593 0.12019685935902497\n",
      "1 0.026333119128341766 0.02412952952017802\n",
      "2 0.06982059838266509 0.06318029713445303\n",
      "3 0.01451683467285073 0.010575348934990517\n",
      "4 0.08344636652038491 0.07809041784500809\n",
      "5 0.025253268302457565 0.023793735242276726\n",
      "===========this fold done\n",
      "0 0.13277470093030377 0.11965661063855332\n",
      "1 0.027086558910919305 0.024064818992668092\n",
      "2 0.07187207312496083 0.06288285427325421\n",
      "3 0.011663159760932316 0.0106756764623322\n",
      "4 0.08519207827218431 0.07841832828609735\n",
      "5 0.02869365862950276 0.02356019997576742\n",
      "===========this fold done\n",
      "0 0.12896769710761247 0.11994322779821029\n",
      "1 0.025594092456950997 0.02424890448177865\n",
      "2 0.06562210433278969 0.06381398336896206\n",
      "3 0.012396826235858222 0.010596002748575462\n",
      "4 0.08758378519552666 0.07802941812901802\n",
      "5 0.028694756482673372 0.02360413976241201\n",
      "===========this fold done\n",
      "0 0.12906358394053946 0.12025535739182375\n",
      "1 0.02668143334094101 0.02436632180867595\n",
      "2 0.06944874181472484 0.06347882870750327\n",
      "3 0.013398298886361076 0.010516752085265552\n",
      "4 0.08660896425485548 0.07800299386938382\n",
      "5 0.02748046749629305 0.023764110914592917\n",
      "===========this fold done\n",
      "0 0.13001295928286552 0.12033103411604885\n",
      "1 0.02888667763004081 0.024101236074068135\n",
      "2 0.06789779624864388 0.06357458182601171\n",
      "3 0.01175773412122333 0.010750783335810126\n",
      "4 0.08989264131829076 0.07715207705188652\n",
      "5 0.025119159805748793 0.023807274645847514\n",
      "===========this fold done\n",
      "0 0.12723288416139802 0.12028296635706115\n",
      "1 0.025950721531619852 0.02417897438465217\n",
      "2 0.06842725810166343 0.06351101096938382\n",
      "3 0.01194037375656018 0.010653468763634176\n",
      "4 0.0875891128622272 0.0772465930281819\n",
      "5 0.024434864232327554 0.02385234926759745\n",
      "===========this fold done\n",
      "0 0.12687044179160412 0.1202285641972534\n",
      "1 0.024508079234185468 0.02442677420413297\n",
      "2 0.06629914554126831 0.06369153637068757\n",
      "3 0.013031678778414471 0.010806802072689192\n",
      "4 0.08313536453177581 0.07857117736059215\n",
      "5 0.026914200530356834 0.023573613275029822\n",
      "===========this fold done\n",
      "0 0.12558154025501414 0.12005737560173478\n",
      "1 0.02805134324242293 0.024013464787851026\n",
      "2 0.0666916701125076 0.06338884226798\n",
      "3 0.010884912480083948 0.010808125518717526\n",
      "4 0.08442554921972248 0.07791705586194385\n",
      "5 0.02586646669180549 0.0236710811322247\n",
      "===========this fold done\n",
      "0 0.13019873369954715 0.12026480009528219\n",
      "1 0.024910828218911298 0.024319485482468994\n",
      "2 0.06372394039547674 0.06403092668059328\n",
      "3 0.010589092042031813 0.010876783331315561\n",
      "4 0.07970011727274678 0.0792077201867734\n",
      "5 0.029612605185072218 0.023457106305629243\n",
      "===========this fold done\n",
      "0 0.1300544270965154 0.12004605296320318\n",
      "1 0.025885473350613003 0.024240450303214013\n",
      "2 0.06833439242268687 0.0632589174079289\n",
      "3 0.01216397640663318 0.010651124067626245\n",
      "4 0.08549943697389698 0.07762185027764391\n",
      "5 0.026429352529243524 0.023618613542667442\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/ridge_feat2.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_ridge_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,FOLD_CNT,rnd=13)\n",
    "    pickle.dump(lr_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.1330099922427433 0.12170319887372258\n",
      "1 0.028719400861075443 0.02280552717201566\n",
      "2 0.08214139701490712 0.0721296801925734\n",
      "3 0.016395133322408238 0.010812460779388558\n",
      "4 0.08760471341758294 0.07592536566660288\n",
      "5 0.027171480985197925 0.024136984319296093\n",
      "===========this fold done\n",
      "0 0.13772739502580397 0.1212815395524188\n",
      "1 0.02785324623852354 0.022901787058968576\n",
      "2 0.08350851629293067 0.07189968450295545\n",
      "3 0.014222048983739213 0.010957064303060265\n",
      "4 0.08631714222056791 0.07599747028976188\n",
      "5 0.02923318243026409 0.02393033098439545\n",
      "===========this fold done\n",
      "0 0.13298852807266384 0.12174018749289935\n",
      "1 0.02641081149043995 0.02302963439868193\n",
      "2 0.08193647329052725 0.07200549803651564\n",
      "3 0.015097505379192433 0.010936328584793749\n",
      "4 0.08495575496383799 0.07602224670377068\n",
      "5 0.030562218543322493 0.023916684791236154\n",
      "===========this fold done\n",
      "0 0.1359435686421793 0.12137143833862038\n",
      "1 0.02595354305711452 0.02314508946316274\n",
      "2 0.08344310670049598 0.07194748115816076\n",
      "3 0.015684088425917283 0.010869324395291235\n",
      "4 0.08751045373520845 0.07594682442146707\n",
      "5 0.02880110457885136 0.024028712891518384\n",
      "===========this fold done\n",
      "0 0.13601025739790373 0.12162699891919121\n",
      "1 0.02997246490887708 0.022739887454730594\n",
      "2 0.08094251459898807 0.07235025064916266\n",
      "3 0.011852187680374403 0.011072965486947984\n",
      "4 0.08923238957681005 0.07580907280532667\n",
      "5 0.027854243751941427 0.024012321760320816\n",
      "===========this fold done\n",
      "0 0.13306159135599224 0.12174903305361075\n",
      "1 0.029687358665072036 0.022778436456520654\n",
      "2 0.08288147580425682 0.07217817936873556\n",
      "3 0.014150972307397723 0.010861566225274209\n",
      "4 0.08561337196323301 0.07615836049801242\n",
      "5 0.02864946214661141 0.024024065290561378\n",
      "===========this fold done\n",
      "0 0.13531843766159485 0.12151204887582964\n",
      "1 0.023789275345469957 0.023217790647531402\n",
      "2 0.0807539551942854 0.0722400308239069\n",
      "3 0.012762228406237024 0.010965400539566253\n",
      "4 0.08401337487045418 0.0762675914665526\n",
      "5 0.030361923360371072 0.023896923496882612\n",
      "===========this fold done\n",
      "0 0.13613164294927463 0.12147050692517124\n",
      "1 0.030854613635356905 0.022620833329393016\n",
      "2 0.08638829004923575 0.07181997009905991\n",
      "3 0.012983849583940948 0.011003382228324879\n",
      "4 0.08737207101696354 0.07600585322881635\n",
      "5 0.03068689737147542 0.023809493105494513\n",
      "===========this fold done\n",
      "0 0.13459255597889735 0.12168152760236556\n",
      "1 0.025477068208782306 0.02321305421330399\n",
      "2 0.07779742405002664 0.07270009354287045\n",
      "3 0.011645981103195265 0.011087767095146066\n",
      "4 0.08249804002167321 0.07659445530253692\n",
      "5 0.032325415220737064 0.023771070790029214\n",
      "===========this fold done\n",
      "0 0.13353811363277554 0.1216212314330604\n",
      "1 0.027365995341314828 0.02302083321691358\n",
      "2 0.08306526279869993 0.07201008979467656\n",
      "3 0.014386513487465415 0.010932727118337345\n",
      "4 0.09263706971646686 0.07550101893646965\n",
      "5 0.031167005050726195 0.023807021591501838\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def gen_base_mnb_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = MultinomialNB(alpha=0.2)\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    #print(train_pred[:10])\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/mnb_feat1.pkl','wb') as fout:\n",
    "    _feat1 = gen_base_mnb_feat(train_tfidf1,train_y,test_tfidf1,FOLD_CNT,14)\n",
    "    pickle.dump(_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.19731660264656714 0.18268046705154126\n",
      "1 0.14989905794235941 0.15485050679642454\n",
      "2 0.1575766031779702 0.14978958095199033\n",
      "3 0.051991706448463786 0.050191623037050694\n",
      "4 0.1855847855234005 0.17363575702386003\n",
      "5 0.13598174708131358 0.13299884246536345\n",
      "===========this fold done\n",
      "0 0.1860139725736997 0.1828774805692798\n",
      "1 0.1563333659423143 0.1543739861339933\n",
      "2 0.1657865967822671 0.14871742855262157\n",
      "3 0.056347666829339935 0.05194673620797601\n",
      "4 0.18239981035870842 0.17347864016214082\n",
      "5 0.13826603080138156 0.13140407100644655\n",
      "===========this fold done\n",
      "0 0.1861851453509003 0.18312172808532262\n",
      "1 0.16008520688190342 0.15423081490311122\n",
      "2 0.15941769658884944 0.14903132745760123\n",
      "3 0.055211567188075766 0.04737280931294338\n",
      "4 0.19061930820581233 0.173002319159281\n",
      "5 0.1448061414594751 0.13201805704461247\n",
      "===========this fold done\n",
      "0 0.19688134562145795 0.18244254972501828\n",
      "1 0.1608863222383025 0.15558662653961414\n",
      "2 0.15950919012100376 0.14875808214274655\n",
      "3 0.049858494405430095 0.05069668161050368\n",
      "4 0.17698072837523554 0.17371441762429693\n",
      "5 0.13115400210667677 0.13384611590022025\n",
      "===========this fold done\n",
      "0 0.20018146538763668 0.18237844091081265\n",
      "1 0.16768072052217559 0.15306987580624468\n",
      "2 0.15095488263687862 0.14934524671391428\n",
      "3 0.05812538359997901 0.05081497116508051\n",
      "4 0.1858040495838842 0.17251711381064627\n",
      "5 0.14734591353608112 0.13120182461093158\n",
      "===========this fold done\n",
      "0 0.19318987308677693 0.18349698838603712\n",
      "1 0.1564826670960962 0.1551673527690213\n",
      "2 0.1515572522223781 0.14996986596834244\n",
      "3 0.055200531472157574 0.05213223648172878\n",
      "4 0.175923827945694 0.1742151406035607\n",
      "5 0.1373958062187282 0.1346249739458764\n",
      "===========this fold done\n",
      "0 0.20230190799356987 0.182431613452466\n",
      "1 0.16626924177449326 0.15407229109495313\n",
      "2 0.15852585500056818 0.14921642009954883\n",
      "3 0.060333595911083865 0.05248495331036426\n",
      "4 0.18477647694153246 0.17338866602019457\n",
      "5 0.14190977017224893 0.1323484662512102\n",
      "===========this fold done\n",
      "0 0.19988658496711253 0.18312636964414855\n",
      "1 0.15579082464803515 0.1535197003667968\n",
      "2 0.1695329346889149 0.1492590598886271\n",
      "3 0.05778311025715128 0.05291778808054173\n",
      "4 0.186391705475514 0.17397433164445958\n",
      "5 0.14162390811853634 0.13166024641168622\n",
      "===========this fold done\n",
      "0 0.19149781468321037 0.183803818531247\n",
      "1 0.1439588459223785 0.15605011591319756\n",
      "2 0.15264938354844942 0.15048318981750028\n",
      "3 0.051025384737934235 0.05261783688042713\n",
      "4 0.16971983812273447 0.17531814169409235\n",
      "5 0.12643642020175214 0.1315038553397374\n",
      "===========this fold done\n",
      "0 0.1970599858287657 0.18259249458125945\n",
      "1 0.1659686906773666 0.15515815824611523\n",
      "2 0.16014747145947728 0.14911549884589545\n",
      "3 0.054152192761384894 0.05106181451608945\n",
      "4 0.19497197225987553 0.17303908814567345\n",
      "5 0.13685189331552203 0.13192421321110012\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/mnb_feat2.pkl','wb') as fout:\n",
    "    _feat2 = gen_base_mnb_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,FOLD_CNT,rnd=29)\n",
    "    pickle.dump(_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
