{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values\n",
    "train['comment_text'] = train['comment_text'].fillna('nan')\n",
    "test['comment_text'] = test['comment_text'].fillna('nan')\n",
    "print('load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def simple_eval(x,y,model_f):\n",
    "    y_cnt = len(y)\n",
    "    split_idx = y_cnt * 4 // 5  # 80%\n",
    "    train_x,test_x = x[:split_idx],x[split_idx:]\n",
    "    train_y,test_y = y[:split_idx],y[split_idx:]\n",
    "    for i in range(6):\n",
    "        model = model_f()\n",
    "        model.fit(train_x,train_y[:,i])\n",
    "        train_pred = model.predict_proba(train_x)\n",
    "        val_pred = model.predict_proba(test_x)\n",
    "        print(list_classes[i])\n",
    "        print('train log loss',log_loss(train_y[:,i],train_pred))\n",
    "        print('valid log loss',log_loss(test_y[:,i],val_pred))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 4318593)\n"
     ]
    }
   ],
   "source": [
    "tf_vec1 = TfidfVectorizer(lowercase=True,ngram_range=(1,3),stop_words='english',sublinear_tf=True)\n",
    "train_tfidf1 = tf_vec1.fit_transform(train['comment_text'].values)\n",
    "test_tfidf1 = tf_vec1.transform(test['comment_text'].values)\n",
    "\n",
    "print(train_tfidf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (95851, 30)\n",
      "dump done\n",
      "toxic\n",
      "train log loss 0.156087285225\n",
      "valid log loss 0.15844687924\n",
      "severe_toxic\n",
      "train log loss 0.0248242897728\n",
      "valid log loss 0.0277244484351\n",
      "obscene\n",
      "train log loss 0.0733959670668\n",
      "valid log loss 0.0813155773677\n",
      "threat\n",
      "train log loss 0.0131837753882\n",
      "valid log loss 0.0171517542511\n",
      "insult\n",
      "train log loss 0.0921806540488\n",
      "valid log loss 0.101992104456\n",
      "identity_hate\n",
      "train log loss 0.0300460165205\n",
      "valid log loss 0.0336325282022\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd1 = svd_obj.fit_transform(train_tfidf1)\n",
    "test_svd1 = svd_obj.transform(test_tfidf1)\n",
    "\n",
    "print(type(train_svd1),train_svd1.shape)\n",
    "with open('../features/tfidf_feat1.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd1,test_svd1],fout)\n",
    "print('dump done')\n",
    "simple_eval(train_svd1,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "train log loss 0.138082166744\n",
      "valid log loss 0.173925010542\n",
      "severe_toxic\n",
      "train log loss 0.0297231055233\n",
      "valid log loss 0.037684593817\n",
      "obscene\n",
      "train log loss 0.0802560145545\n",
      "valid log loss 0.104363729341\n",
      "threat\n",
      "train log loss 0.0147055727416\n",
      "valid log loss 0.0190503889489\n",
      "insult\n",
      "train log loss 0.0893909776011\n",
      "valid log loss 0.116306827604\n",
      "identity_hate\n",
      "train log loss 0.0304432395079\n",
      "valid log loss 0.0386920585633\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "simple_eval(train_tfidf1,train_y,LogisticRegression)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 14989)\n"
     ]
    }
   ],
   "source": [
    "tf_vec2 = TfidfVectorizer(lowercase=True,ngram_range=(1,2),stop_words='english',analyzer='char',sublinear_tf=True)\n",
    "train_tfidf2 = tf_vec2.fit_transform(train['comment_text'].values)\n",
    "test_tfidf2 = tf_vec2.transform(test['comment_text'].values)\n",
    "print(train_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (95851, 30)\n",
      "dump done\n",
      "toxic\n",
      "train log loss 0.202172742131\n",
      "valid log loss 0.212628835819\n",
      "severe_toxic\n",
      "train log loss 0.0259215834694\n",
      "valid log loss 0.0321390577282\n",
      "obscene\n",
      "train log loss 0.112231201045\n",
      "valid log loss 0.12610024022\n",
      "threat\n",
      "train log loss 0.0120430378248\n",
      "valid log loss 0.0173528518812\n",
      "insult\n",
      "train log loss 0.11482623699\n",
      "valid log loss 0.130960604286\n",
      "identity_hate\n",
      "train log loss 0.0323169132281\n",
      "valid log loss 0.0402807760645\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd2 = svd_obj.fit_transform(train_tfidf2)\n",
    "test_svd2 = svd_obj.transform(test_tfidf2)\n",
    "\n",
    "print(type(train_svd2),train_svd2.shape)\n",
    "with open('../features/tfidf_feat2.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd1,test_svd1],fout)\n",
    "print('dump done')\n",
    "simple_eval(train_svd2,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "train log loss 0.158536648617\n",
      "valid log loss 0.16843244878\n",
      "severe_toxic\n",
      "train log loss 0.0253842077802\n",
      "valid log loss 0.0289628113994\n",
      "obscene\n",
      "train log loss 0.0811363015801\n",
      "valid log loss 0.0906727570695\n",
      "threat\n",
      "train log loss 0.0121889946573\n",
      "valid log loss 0.0147282865467\n",
      "insult\n",
      "train log loss 0.092172449373\n",
      "valid log loss 0.104807954174\n",
      "identity_hate\n",
      "train log loss 0.0271011137533\n",
      "valid log loss 0.0318123294515\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "simple_eval(train_tfidf2,train_y,LogisticRegression)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.183709322034 0.142118183325\n",
      "1 0.0361012958923 0.0309334465615\n",
      "2 0.102758281541 0.0850492838986\n",
      "3 0.0204264004475 0.0145438654738\n",
      "4 0.114756011046 0.0933248302223\n",
      "5 0.0385575824299 0.0313512875189\n",
      "===========this fold done\n",
      "0 0.179364883477 0.143445253302\n",
      "1 0.0385623187675 0.0304851324524\n",
      "2 0.105337067785 0.0839688180663\n",
      "3 0.0156586948616 0.0163683836061\n",
      "4 0.116403999643 0.0926314876987\n",
      "5 0.0370199932371 0.031905783671\n",
      "===========this fold done\n",
      "0 0.183190267469 0.142483757282\n",
      "1 0.0391438009942 0.0303304573319\n",
      "2 0.110917809094 0.0827475889184\n",
      "3 0.0187648622205 0.0151438581718\n",
      "4 0.121571856591 0.0912802260511\n",
      "5 0.0402040336252 0.0307147657223\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def gen_base_lr_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = LogisticRegression()\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/lr_feat1.pkl','wb') as fout:\n",
    "    lr_feat1 = gen_base_lr_feat(train_tfidf1,train_y,test_tfidf1,3)\n",
    "    pickle.dump(lr_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.167882418744 0.159722854406\n",
      "1 0.0270481166489 0.0261903319928\n",
      "2 0.0841428930945 0.0846953963422\n",
      "3 0.0156749310934 0.0120235577822\n",
      "4 0.0962323175092 0.0957883289087\n",
      "5 0.0304340636466 0.027877073222\n",
      "===========this fold done\n",
      "0 0.166998042835 0.160141838654\n",
      "1 0.0285104675816 0.0255213095161\n",
      "2 0.0893279852502 0.0822358791181\n",
      "3 0.0118354610366 0.0136240587181\n",
      "4 0.0992139442314 0.0942347655798\n",
      "5 0.0295749890155 0.0281134282338\n",
      "===========this fold done\n",
      "0 0.170509789353 0.158269923358\n",
      "1 0.0298753775332 0.025152839486\n",
      "2 0.0921485363036 0.0808628205458\n",
      "3 0.0143592555057 0.0124768583705\n",
      "4 0.106275971973 0.0910016031726\n",
      "5 0.0318065756693 0.0273685559511\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/lr_feat2.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_lr_feat(train_tfidf2,train_y,test_tfidf2,3)\n",
    "    pickle.dump(lr_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.326410504502 0.250603130512\n",
      "1 0.0610047715975 0.0589797224541\n",
      "2 0.206542481348 0.17832550856\n",
      "3 0.030708840219 0.0226513455201\n",
      "4 0.209386724709 0.179440504102\n",
      "5 0.0599511952229 0.053824332119\n",
      "===========this fold done\n",
      "0 0.317623748166 0.251866473661\n",
      "1 0.066978783688 0.0574292325375\n",
      "2 0.217078229126 0.175659921216\n",
      "3 0.0225298737139 0.0256984472411\n",
      "4 0.214682162434 0.178124678911\n",
      "5 0.0593940596629 0.0539140209586\n",
      "===========this fold done\n",
      "0 0.329041097508 0.250898738331\n",
      "1 0.0699024465048 0.0567192903762\n",
      "2 0.229814188687 0.173650227697\n",
      "3 0.0273243375288 0.0240239322357\n",
      "4 0.22895062394 0.174492372673\n",
      "5 0.0633596784626 0.0528066625696\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def gen_base_mnb_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = MultinomialNB()\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/mnb_feat1.pkl','wb') as fout:\n",
    "    _feat1 = gen_base_mnb_feat(train_tfidf1,train_y,test_tfidf1,3)\n",
    "    pickle.dump(_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.337717406616 0.328382416032\n",
      "1 0.124539082177 0.130719536228\n",
      "2 0.25860451231 0.266390921695\n",
      "3 0.0873198985073 0.0690654448704\n",
      "4 0.259330415761 0.263757784541\n",
      "5 0.135487357405 0.130688344246\n",
      "===========this fold done\n",
      "0 0.325397157453 0.331203784829\n",
      "1 0.131145544727 0.131329667175\n",
      "2 0.265647154974 0.266833818714\n",
      "3 0.0614539930119 0.0779543269918\n",
      "4 0.26302682404 0.264245655852\n",
      "5 0.127908651044 0.133603396278\n",
      "===========this fold done\n",
      "0 0.340992651411 0.327385454184\n",
      "1 0.141490939743 0.127799755404\n",
      "2 0.28609524357 0.262618662023\n",
      "3 0.0753647062266 0.0729965149157\n",
      "4 0.280574148454 0.260765958496\n",
      "5 0.137835303626 0.130177800613\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/mnb_feat2.pkl','wb') as fout:\n",
    "    _feat2 = gen_base_mnb_feat(train_tfidf2,train_y,test_tfidf2,3)\n",
    "    pickle.dump(_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
