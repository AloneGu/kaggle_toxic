{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values\n",
    "train['comment_text'] = train['comment_text'].fillna('nan')\n",
    "test['comment_text'] = test['comment_text'].fillna('nan')\n",
    "print('load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def simple_eval(x,y,model_f):\n",
    "    y_cnt = len(y)\n",
    "    split_idx = y_cnt * 4 // 5  # 80%\n",
    "    train_x,test_x = x[:split_idx],x[split_idx:]\n",
    "    train_y,test_y = y[:split_idx],y[split_idx:]\n",
    "    for i in range(6):\n",
    "        model = model_f()\n",
    "        model.fit(train_x,train_y[:,i])\n",
    "        train_pred = model.predict_proba(train_x)\n",
    "        val_pred = model.predict_proba(test_x)\n",
    "        print(list_classes[i])\n",
    "        print('train log loss',log_loss(train_y[:,i],train_pred))\n",
    "        print('valid log loss',log_loss(test_y[:,i],val_pred))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 20000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec1 = TfidfVectorizer(lowercase=True,ngram_range=(1,1),stop_words='english',\n",
    "                          strip_accents='unicode',token_pattern=r'\\w{1,}',\n",
    "                          max_features=20000,sublinear_tf=True)\n",
    "train_tfidf1 = tf_vec1.fit_transform(train['comment_text'].values)\n",
    "test_tfidf1 = tf_vec1.transform(test['comment_text'].values)\n",
    "\n",
    "print(train_tfidf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (95851, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd1 = svd_obj.fit_transform(train_tfidf1)\n",
    "test_svd1 = svd_obj.transform(test_tfidf1)\n",
    "\n",
    "print(type(train_svd1),train_svd1.shape)\n",
    "with open('../features/tfidf_feat1.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd1,test_svd1],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd1,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#simple_eval(train_tfidf1,train_y,LogisticRegression)\n",
    "#print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 20000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec2 = TfidfVectorizer(lowercase=True,ngram_range=(1,4),stop_words='english',\n",
    "                          strip_accents='unicode',\n",
    "                          analyzer='char',sublinear_tf=True,\n",
    "                          max_features=20000\n",
    "                         )\n",
    "train_tfidf2 = tf_vec2.fit_transform(train['comment_text'].values)\n",
    "test_tfidf2 = tf_vec2.transform(test['comment_text'].values)\n",
    "print(train_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (95851, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd2 = svd_obj.fit_transform(train_tfidf2)\n",
    "test_svd2 = svd_obj.transform(test_tfidf2)\n",
    "\n",
    "print(type(train_svd2),train_svd2.shape)\n",
    "with open('../features/tfidf_feat2.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd2,test_svd2],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd2,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "train log loss 0.102668476965\n",
      "valid log loss 0.118305630169\n",
      "severe_toxic\n",
      "train log loss 0.022411922314\n",
      "valid log loss 0.0267855165921\n",
      "obscene\n",
      "train log loss 0.0551266350843\n",
      "valid log loss 0.0666293033904\n",
      "threat\n",
      "train log loss 0.0100382343539\n",
      "valid log loss 0.0129778949002\n",
      "insult\n",
      "train log loss 0.0657594819378\n",
      "valid log loss 0.0811999457175\n",
      "identity_hate\n",
      "train log loss 0.0216025589557\n",
      "valid log loss 0.0273140165199\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "simple_eval(train_tfidf2,train_y,LogisticRegression)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.119380694824 0.0746850700205\n",
      "1 0.0274234604134 0.0177959414703\n",
      "2 0.0607416599157 0.0403743840836\n",
      "3 0.0134692027362 0.00583995249244\n",
      "4 0.0800887331554 0.0537194073094\n",
      "5 0.0264754666052 0.0152924881785\n",
      "===========this fold done\n",
      "0 0.115220868579 0.0760982045533\n",
      "1 0.0288516442682 0.0173982641861\n",
      "2 0.0635477352893 0.0390987948236\n",
      "3 0.00923871874123 0.0069111234668\n",
      "4 0.0816040309818 0.0526277565744\n",
      "5 0.0248571972657 0.0155287935524\n",
      "===========this fold done\n",
      "0 0.116801974208 0.0760718797437\n",
      "1 0.0288413520517 0.0174308477028\n",
      "2 0.0662603023561 0.038419435077\n",
      "3 0.0114051707333 0.00644193432172\n",
      "4 0.0850341080131 0.0521715309297\n",
      "5 0.0276622324554 0.0148449221083\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def gen_base_lr_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = LogisticRegression(C=4.0, solver='sag')\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/lr_feat1.pkl','wb') as fout:\n",
    "    lr_feat1 = gen_base_lr_feat(train_tfidf1,train_y,test_tfidf1,3,rnd=3)\n",
    "    pickle.dump(lr_feat1,fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (95851, 20000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (95851, 40000)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del tf_vec2,tf_vec1,svd_obj\n",
    "except:\n",
    "    pass\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "print(type(train_tfidf2),train_tfidf2.shape)\n",
    "comb_train = csr_matrix(hstack((train_tfidf2,train_tfidf1)))\n",
    "print(type(comb_train),comb_train.shape)\n",
    "comb_test = csr_matrix(hstack((test_tfidf2,test_tfidf1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.104134998628 0.0478121390271\n",
      "1 0.0251872981136 0.0112555860909\n",
      "2 0.0528032659613 0.0254522321099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.0116355559205 0.00330471061956\n",
      "4 0.0708317222992 0.0346417522155\n",
      "5 0.023723862077 0.00947155042997\n",
      "===========this fold done\n",
      "0 0.101401116398 0.0486003144496\n",
      "1 0.0270874049698 0.0108921556735\n",
      "2 0.0570462990754 0.0244360983602\n",
      "3 0.00798403923112 0.00393517985065\n",
      "4 0.0730095129567 0.0339649417963\n",
      "5 0.0227642005701 0.00942174225657\n",
      "===========this fold done\n",
      "0 0.103618997585 0.0483479923041\n",
      "1 0.0265124405488 0.0111355137278\n",
      "2 0.05932751422 0.0238983647454\n",
      "3 0.00981952136598 0.00357960972976\n",
      "4 0.0778400073864 0.0328498576888\n",
      "5 0.0255244757828 0.00901477624363\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/lr_feat2.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_lr_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,3,rnd=11)\n",
    "    pickle.dump(lr_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.141056929622 0.108651103131\n",
      "1 0.0296058661402 0.02355056603\n",
      "2 0.0811652113554 0.0666857558162\n",
      "3 0.0205765279316 0.0121435583698\n",
      "4 0.0882265251768 0.0713652720406\n",
      "5 0.0322594811266 0.0236270422449\n",
      "===========this fold done\n",
      "0 0.137666189062 0.110316129543\n",
      "1 0.0325684311321 0.0229291386099\n",
      "2 0.0863003733897 0.0645677933659\n",
      "3 0.0139678059491 0.0137292755743\n",
      "4 0.0906829840528 0.0700061843421\n",
      "5 0.0329099885628 0.0231661090294\n",
      "===========this fold done\n",
      "0 0.138532079466 0.11024644516\n",
      "1 0.0323311218192 0.0230591485073\n",
      "2 0.0893742591071 0.0635197885879\n",
      "3 0.0179541554089 0.0128313608333\n",
      "4 0.0968412780798 0.0684965722229\n",
      "5 0.0338471869764 0.0229417046308\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def gen_base_mnb_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = MultinomialNB(alpha=0.2)\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/mnb_feat1.pkl','wb') as fout:\n",
    "    _feat1 = gen_base_mnb_feat(train_tfidf1,train_y,test_tfidf1,3)\n",
    "    pickle.dump(_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.17013950897 0.137766542843\n",
      "1 0.0795185056839 0.0708648635884\n",
      "2 0.123236395282 0.108441934387\n",
      "3 0.0300327330205 0.0146157794155\n",
      "4 0.140281672143 0.126474120042\n",
      "5 0.0645588569353 0.0519272156845\n",
      "===========this fold done\n",
      "0 0.162476923761 0.140672833354\n",
      "1 0.0758198359313 0.0697682943836\n",
      "2 0.127067794103 0.106291154116\n",
      "3 0.0192439807651 0.0170760865145\n",
      "4 0.140768228894 0.126127014893\n",
      "5 0.063597455964 0.0519235790907\n",
      "===========this fold done\n",
      "0 0.168494053822 0.13893619362\n",
      "1 0.0779116635651 0.067094604248\n",
      "2 0.127780207434 0.104351880275\n",
      "3 0.0243567855136 0.0160279797648\n",
      "4 0.15544891496 0.120734900875\n",
      "5 0.064355638381 0.0496653106203\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/mnb_feat2.pkl','wb') as fout:\n",
    "    _feat2 = gen_base_mnb_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,3,rnd=29)\n",
    "    pickle.dump(_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
