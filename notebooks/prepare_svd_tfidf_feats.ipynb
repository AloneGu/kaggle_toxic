{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load done\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import log_loss\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "import re\n",
    "  \n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "train_y = train[list_classes].values\n",
    "train['comment_text'] = train['comment_text'].fillna('nan')\n",
    "test['comment_text'] = test['comment_text'].fillna('nan')\n",
    "print('load done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "def simple_eval(x,y,model_f):\n",
    "    y_cnt = len(y)\n",
    "    split_idx = y_cnt * 4 // 5  # 80%\n",
    "    train_x,test_x = x[:split_idx],x[split_idx:]\n",
    "    train_y,test_y = y[:split_idx],y[split_idx:]\n",
    "    for i in range(6):\n",
    "        model = model_f()\n",
    "        model.fit(train_x,train_y[:,i])\n",
    "        train_pred = model.predict_proba(train_x)\n",
    "        val_pred = model.predict_proba(test_x)\n",
    "        print(list_classes[i])\n",
    "        print('train log loss',log_loss(train_y[:,i],train_pred))\n",
    "        print('valid log loss',log_loss(test_y[:,i],val_pred))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 20000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec1 = TfidfVectorizer(lowercase=True,ngram_range=(1,1),stop_words='english',\n",
    "                          strip_accents='unicode',token_pattern=r'\\w{1,}',\n",
    "                          max_features=20000,sublinear_tf=True)\n",
    "train_tfidf1 = tf_vec1.fit_transform(train['comment_text'].values)\n",
    "test_tfidf1 = tf_vec1.transform(test['comment_text'].values)\n",
    "\n",
    "print(train_tfidf1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (159571, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "n_comp = 30\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd1 = svd_obj.fit_transform(train_tfidf1)\n",
    "test_svd1 = svd_obj.transform(test_tfidf1)\n",
    "\n",
    "print(type(train_svd1),train_svd1.shape)\n",
    "with open('../features/tfidf_feat1.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd1,test_svd1],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd1,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_eval(train_tfidf1,train_y,LogisticRegression)\n",
    "#print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 20000)\n"
     ]
    }
   ],
   "source": [
    "tf_vec2 = TfidfVectorizer(lowercase=True,ngram_range=(1,4),stop_words='english',\n",
    "                          strip_accents='unicode',\n",
    "                          analyzer='char',sublinear_tf=True,\n",
    "                          max_features=20000\n",
    "                         )\n",
    "train_tfidf2 = tf_vec2.fit_transform(train['comment_text'].values)\n",
    "test_tfidf2 = tf_vec2.transform(test['comment_text'].values)\n",
    "print(train_tfidf2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (159571, 30)\n",
      "dump done\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "train_svd2 = svd_obj.fit_transform(train_tfidf2)\n",
    "test_svd2 = svd_obj.transform(test_tfidf2)\n",
    "\n",
    "print(type(train_svd2),train_svd2.shape)\n",
    "with open('../features/tfidf_feat2.pkl','wb') as fout:\n",
    "    pickle.dump([train_svd2,test_svd2],fout)\n",
    "print('dump done')\n",
    "#simple_eval(train_svd2,train_y,XGBClassifier)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "train log loss 0.097541815835\n",
      "valid log loss 0.110883987182\n",
      "severe_toxic\n",
      "train log loss 0.0219622082081\n",
      "valid log loss 0.0248464967118\n",
      "obscene\n",
      "train log loss 0.0523280928271\n",
      "valid log loss 0.0597151221297\n",
      "threat\n",
      "train log loss 0.00907409432779\n",
      "valid log loss 0.0097982739249\n",
      "insult\n",
      "train log loss 0.0640320514502\n",
      "valid log loss 0.0732435923241\n",
      "identity_hate\n",
      "train log loss 0.0208597151247\n",
      "valid log loss 0.0262118259185\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "simple_eval(train_tfidf2,train_y,LogisticRegression)\n",
    "print('-----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.112162928933 0.0772868537356\n",
      "1 0.0275421343383 0.0177474643214\n",
      "2 0.0618346678727 0.0385870107629\n",
      "3 0.0107685962438 0.00555280091532\n",
      "4 0.0794985925881 0.0532711245892\n",
      "5 0.0250175069737 0.0158466171446\n",
      "===========this fold done\n",
      "0 0.111863022997 0.0775423846985\n",
      "1 0.0272079451912 0.0178824176308\n",
      "2 0.0589719851043 0.0400635568916\n",
      "3 0.0105118509577 0.00556538323423\n",
      "4 0.0792451375962 0.0539148679389\n",
      "5 0.0254378081868 0.0155320882088\n",
      "===========this fold done\n",
      "0 0.111737584895 0.0774509017363\n",
      "1 0.0270029129652 0.0179156085829\n",
      "2 0.0603379689662 0.0394354345225\n",
      "3 0.00909578620804 0.00609592294031\n",
      "4 0.077722820262 0.0542104890624\n",
      "5 0.0264166247953 0.0154944735549\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def gen_base_lr_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = LogisticRegression(C=4.0, solver='sag')\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/lr_feat1.pkl','wb') as fout:\n",
    "    lr_feat1 = gen_base_lr_feat(train_tfidf1,train_y,test_tfidf1,3,rnd=3)\n",
    "    pickle.dump(lr_feat1,fout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'> (159571, 20000)\n",
      "<class 'scipy.sparse.csr.csr_matrix'> (159571, 40000)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del tf_vec2,tf_vec1,svd_obj\n",
    "except:\n",
    "    pass\n",
    "from scipy.sparse import hstack\n",
    "from scipy.sparse import csr_matrix\n",
    "print(type(train_tfidf2),train_tfidf2.shape)\n",
    "comb_train = csr_matrix(hstack((train_tfidf2,train_tfidf1)))\n",
    "print(type(comb_train),comb_train.shape)\n",
    "comb_test = csr_matrix(hstack((test_tfidf2,test_tfidf1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0992753116269 0.0509009814659\n",
      "1 0.0250997174808 0.0120476879635\n",
      "2 0.0559167648641 0.0249552065923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.00946535659641 0.0031664655426\n",
      "4 0.0707790065213 0.0360462550169\n",
      "5 0.0228416402317 0.00994383327158\n",
      "===========this fold done\n",
      "0 0.0982116370781 0.0514176614187\n",
      "1 0.0258702392808 0.0118901480609\n",
      "2 0.0528490594263 0.0263142293788\n",
      "3 0.00906345637225 0.00325813530532\n",
      "4 0.0719653150783 0.0358532461133\n",
      "5 0.0235059199659 0.00968826654981\n",
      "===========this fold done\n",
      "0 0.0987321530065 0.051220227543\n",
      "1 0.0255597546453 0.0117850323748\n",
      "2 0.0540558517289 0.0257791146956\n",
      "3 0.00798051846313 0.00352866295317\n",
      "4 0.070156678508 0.0362656929407\n",
      "5 0.0237900454711 0.00978369268026\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/lr_feat2.pkl','wb') as fout:\n",
    "    lr_feat2 = gen_base_lr_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,3,rnd=11)\n",
    "    pickle.dump(lr_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.134838087987 0.114813889975\n",
      "1 0.0290536599381 0.0219682691696\n",
      "2 0.0809643154916 0.0671168945423\n",
      "3 0.0171299136645 0.0109046123392\n",
      "4 0.0861348563346 0.0726614396987\n",
      "5 0.0293209998066 0.0232365891808\n",
      "===========this fold done\n",
      "0 0.136720689342 0.114593195684\n",
      "1 0.0283361708217 0.0222039457422\n",
      "2 0.0830842038615 0.0672832938731\n",
      "3 0.0141937554341 0.0113706076216\n",
      "4 0.0896873062395 0.0718669918728\n",
      "5 0.0301157664279 0.0227412117302\n",
      "===========this fold done\n",
      "0 0.135560647951 0.114813144008\n",
      "1 0.0283633193014 0.0224347666845\n",
      "2 0.0831101256533 0.0671101179289\n",
      "3 0.0140281749492 0.0114971282078\n",
      "4 0.0892222960358 0.0723001805665\n",
      "5 0.0323889903141 0.0223151980501\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "def gen_base_mnb_feat(train_x,train_y,test_x,fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(train_x):\n",
    "        # x,y\n",
    "        curr_x,curr_y = train_x[train_index],train_y[train_index]\n",
    "        hold_out_x,hold_out_y = train_x[test_index],train_y[test_index]\n",
    "        \n",
    "        for i in range(6):\n",
    "            model = MultinomialNB(alpha=0.2)\n",
    "            # train and pred\n",
    "            # fit for i\n",
    "            model.fit(curr_x, curr_y[:,i])\n",
    "            \n",
    "            # prepare for i on this fold\n",
    "            hold_out_pred = model.predict_proba(hold_out_x)\n",
    "            curr_train_pred = model.predict_proba(curr_x)\n",
    "            print(i,log_loss(hold_out_y[:,i],hold_out_pred),log_loss(curr_y[:,i],curr_train_pred))\n",
    "            train_pred[test_index][:,i] = hold_out_pred[:,1]\n",
    "            \n",
    "            # prepare test\n",
    "            y_test = model.predict_proba(test_x)[:,1]\n",
    "            test_pred[:,i] += y_test\n",
    "        print('===========this fold done')\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    return [train_pred, test_pred]\n",
    "\n",
    "with open('../features/mnb_feat1.pkl','wb') as fout:\n",
    "    _feat1 = gen_base_mnb_feat(train_tfidf1,train_y,test_tfidf1,3)\n",
    "    pickle.dump(_feat1,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.163815422018 0.150855394436\n",
      "1 0.0970833485844 0.095349318838\n",
      "2 0.134126022259 0.118827119824\n",
      "3 0.0239551423336 0.0167431901075\n",
      "4 0.151859877946 0.137775093853\n",
      "5 0.0834846344072 0.0765224013328\n",
      "===========this fold done\n",
      "0 0.173644757287 0.149794204233\n",
      "1 0.107957363014 0.0952815484169\n",
      "2 0.12886654133 0.118355840651\n",
      "3 0.0243055508628 0.019107162246\n",
      "4 0.150209871985 0.13551310893\n",
      "5 0.0902276189779 0.0770138150816\n",
      "===========this fold done\n",
      "0 0.173611755998 0.151461598431\n",
      "1 0.0978086407079 0.0956687321528\n",
      "2 0.136004584257 0.119273035877\n",
      "3 0.0260453470303 0.0202408504824\n",
      "4 0.15408210874 0.138587483728\n",
      "5 0.0797546572766 0.0732659613781\n",
      "===========this fold done\n"
     ]
    }
   ],
   "source": [
    "with open('../features/mnb_feat2.pkl','wb') as fout:\n",
    "    _feat2 = gen_base_mnb_feat(comb_train,\n",
    "                                train_y,\n",
    "                                comb_test,3,rnd=29)\n",
    "    pickle.dump(_feat2,fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
