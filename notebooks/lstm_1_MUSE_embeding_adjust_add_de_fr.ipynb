{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    #https://www.kaggle.com/sreeram004/test-lr-with-convai-dataset\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "receipt 19631\n",
      "pasca 98730\n",
      "conniving 37561\n",
      "guarding 19065\n",
      "buds 28898\n",
      "spench 116064\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435713\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "\n",
    "with open('../wiki.multi.fr.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "\n",
    "with open('../wiki.multi.de.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "        \n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4716\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "# pre null 5293\n",
    "\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.01),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0664 - acc: 0.9774Epoch 00001: val_loss improved from inf to 0.04942, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 397s 6ms/step - loss: 0.0664 - acc: 0.9774 - val_loss: 0.0494 - val_acc: 0.9817\n",
      "Epoch 2/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9812Epoch 00002: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0531 - acc: 0.9812 - val_loss: 0.0516 - val_acc: 0.9818\n",
      "Epoch 3/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9811Epoch 00003: val_loss improved from 0.04942 to 0.04666, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0514 - acc: 0.9811 - val_loss: 0.0467 - val_acc: 0.9824\n",
      "Epoch 4/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9819Epoch 00004: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0482 - acc: 0.9819 - val_loss: 0.0482 - val_acc: 0.9822\n",
      "Epoch 5/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9822Epoch 00005: val_loss improved from 0.04666 to 0.04584, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0467 - acc: 0.9822 - val_loss: 0.0458 - val_acc: 0.9827\n",
      "Epoch 6/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9828Epoch 00006: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0451 - acc: 0.9828 - val_loss: 0.0496 - val_acc: 0.9821\n",
      "Epoch 7/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9831Epoch 00007: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0443 - acc: 0.9831 - val_loss: 0.0482 - val_acc: 0.9825\n",
      "Epoch 8/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9831Epoch 00008: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0432 - acc: 0.9831 - val_loss: 0.0477 - val_acc: 0.9828\n",
      "Epoch 9/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9832Epoch 00009: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0429 - acc: 0.9832 - val_loss: 0.0467 - val_acc: 0.9825\n",
      "Epoch 10/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9837Epoch 00010: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0413 - acc: 0.9837 - val_loss: 0.0475 - val_acc: 0.9825\n",
      "Epoch 11/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9839Epoch 00011: val_loss did not improve\n",
      "71888/71888 [==============================] - 395s 5ms/step - loss: 0.0411 - acc: 0.9839 - val_loss: 0.0481 - val_acc: 0.9819\n",
      "Epoch 12/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0568 - acc: 0.9806Epoch 00012: val_loss did not improve\n",
      "71888/71888 [==============================] - 394s 5ms/step - loss: 0.0568 - acc: 0.9806 - val_loss: 0.1272 - val_acc: 0.9635\n",
      "Epoch 13/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0839 - acc: 0.9723Epoch 00013: val_loss did not improve\n",
      "71888/71888 [==============================] - 394s 5ms/step - loss: 0.0839 - acc: 0.9723 - val_loss: 0.0633 - val_acc: 0.9771\n",
      "Epoch 14/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9764Epoch 00014: val_loss did not improve\n",
      "71888/71888 [==============================] - 394s 5ms/step - loss: 0.0663 - acc: 0.9764 - val_loss: 0.0594 - val_acc: 0.9784\n",
      "Epoch 15/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9776Epoch 00015: val_loss did not improve\n",
      "71888/71888 [==============================] - 394s 5ms/step - loss: 0.0630 - acc: 0.9776 - val_loss: 0.0548 - val_acc: 0.9801\n",
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0633 - acc: 0.9782Epoch 00001: val_loss improved from inf to 0.04983, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0633 - acc: 0.9782 - val_loss: 0.0498 - val_acc: 0.9819\n",
      "Epoch 2/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9817Epoch 00002: val_loss improved from 0.04983 to 0.04396, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0496 - acc: 0.9817 - val_loss: 0.0440 - val_acc: 0.9831\n",
      "Epoch 3/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9823Epoch 00003: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0464 - acc: 0.9823 - val_loss: 0.0441 - val_acc: 0.9832\n",
      "Epoch 4/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9827Epoch 00004: val_loss improved from 0.04396 to 0.04355, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0446 - acc: 0.9827 - val_loss: 0.0436 - val_acc: 0.9837\n",
      "Epoch 5/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9833Epoch 00005: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0432 - acc: 0.9833 - val_loss: 0.0449 - val_acc: 0.9831\n",
      "Epoch 6/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00006: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0451 - val_acc: 0.9832\n",
      "Epoch 7/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9838Epoch 00007: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.0457 - val_acc: 0.9835\n",
      "Epoch 8/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9841Epoch 00008: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0399 - acc: 0.9841 - val_loss: 0.0452 - val_acc: 0.9832\n",
      "Epoch 9/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9843Epoch 00009: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0390 - acc: 0.9843 - val_loss: 0.0442 - val_acc: 0.9833\n",
      "Epoch 10/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0382 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0382 - acc: 0.9847 - val_loss: 0.0451 - val_acc: 0.9836\n",
      "Epoch 11/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9849Epoch 00011: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0375 - acc: 0.9849 - val_loss: 0.0480 - val_acc: 0.9833\n",
      "Epoch 12/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9851Epoch 00012: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0373 - acc: 0.9851 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "Epoch 13/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9850Epoch 00013: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0370 - acc: 0.9850 - val_loss: 0.0457 - val_acc: 0.9835\n",
      "Epoch 14/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9849Epoch 00014: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0378 - acc: 0.9849 - val_loss: 0.0468 - val_acc: 0.9833\n",
      "Epoch 15/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9852Epoch 00015: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0363 - acc: 0.9852 - val_loss: 0.0469 - val_acc: 0.9830\n",
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.05084, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 394s 5ms/step - loss: 0.0618 - acc: 0.9787 - val_loss: 0.0508 - val_acc: 0.9813\n",
      "Epoch 2/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9823Epoch 00002: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0481 - acc: 0.9823 - val_loss: 0.0541 - val_acc: 0.9817\n",
      "Epoch 3/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9827Epoch 00003: val_loss improved from 0.05084 to 0.04737, saving model to weights_base.best.h5\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0453 - acc: 0.9827 - val_loss: 0.0474 - val_acc: 0.9820\n",
      "Epoch 4/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9832Epoch 00004: val_loss did not improve\n",
      "71888/71888 [==============================] - 392s 5ms/step - loss: 0.0435 - acc: 0.9832 - val_loss: 0.0482 - val_acc: 0.9825\n",
      "Epoch 5/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9838Epoch 00005: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.0475 - val_acc: 0.9826\n",
      "Epoch 6/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9840Epoch 00006: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0407 - acc: 0.9840 - val_loss: 0.0489 - val_acc: 0.9819\n",
      "Epoch 7/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845Epoch 00007: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0393 - acc: 0.9845 - val_loss: 0.0488 - val_acc: 0.9824\n",
      "Epoch 8/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9848Epoch 00008: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0485 - val_acc: 0.9823\n",
      "Epoch 9/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9848Epoch 00009: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0385 - acc: 0.9848 - val_loss: 0.0495 - val_acc: 0.9819\n",
      "Epoch 10/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9848Epoch 00010: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0381 - acc: 0.9848 - val_loss: 0.0489 - val_acc: 0.9822\n",
      "Epoch 11/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9856Epoch 00011: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0365 - acc: 0.9856 - val_loss: 0.0497 - val_acc: 0.9820\n",
      "Epoch 12/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9856Epoch 00012: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0520 - val_acc: 0.9826\n",
      "Epoch 13/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9857Epoch 00013: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0353 - acc: 0.9857 - val_loss: 0.0494 - val_acc: 0.9820\n",
      "Epoch 14/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9857Epoch 00014: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0357 - acc: 0.9857 - val_loss: 0.0499 - val_acc: 0.9822\n",
      "Epoch 15/15\n",
      "71872/71888 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845Epoch 00015: val_loss did not improve\n",
      "71888/71888 [==============================] - 393s 5ms/step - loss: 0.0393 - acc: 0.9845 - val_loss: 0.0491 - val_acc: 0.9822\n",
      "Train on 71889 samples, validate on 23962 samples\n",
      "Epoch 1/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9774Epoch 00001: val_loss improved from inf to 0.05332, saving model to weights_base.best.h5\n",
      "71889/71889 [==============================] - 398s 6ms/step - loss: 0.0679 - acc: 0.9774 - val_loss: 0.0533 - val_acc: 0.9809\n",
      "Epoch 2/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9810Epoch 00002: val_loss improved from 0.05332 to 0.04873, saving model to weights_base.best.h5\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0522 - acc: 0.9810 - val_loss: 0.0487 - val_acc: 0.9822\n",
      "Epoch 3/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9819Epoch 00003: val_loss improved from 0.04873 to 0.04737, saving model to weights_base.best.h5\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0486 - acc: 0.9819 - val_loss: 0.0474 - val_acc: 0.9821\n",
      "Epoch 4/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9824Epoch 00004: val_loss improved from 0.04737 to 0.04543, saving model to weights_base.best.h5\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0458 - acc: 0.9824 - val_loss: 0.0454 - val_acc: 0.9830\n",
      "Epoch 5/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9831Epoch 00005: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0439 - acc: 0.9831 - val_loss: 0.0456 - val_acc: 0.9829\n",
      "Epoch 6/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9835Epoch 00006: val_loss improved from 0.04543 to 0.04522, saving model to weights_base.best.h5\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0427 - acc: 0.9835 - val_loss: 0.0452 - val_acc: 0.9827\n",
      "Epoch 7/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9839Epoch 00007: val_loss improved from 0.04522 to 0.04521, saving model to weights_base.best.h5\n",
      "71889/71889 [==============================] - 395s 6ms/step - loss: 0.0416 - acc: 0.9839 - val_loss: 0.0452 - val_acc: 0.9823\n",
      "Epoch 8/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9844Epoch 00008: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0404 - acc: 0.9844 - val_loss: 0.0454 - val_acc: 0.9825\n",
      "Epoch 9/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9844Epoch 00009: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0395 - acc: 0.9844 - val_loss: 0.0460 - val_acc: 0.9823\n",
      "Epoch 10/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "71889/71889 [==============================] - 396s 6ms/step - loss: 0.0393 - acc: 0.9847 - val_loss: 0.0486 - val_acc: 0.9826\n",
      "Epoch 11/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9848Epoch 00011: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0462 - val_acc: 0.9826\n",
      "Epoch 12/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9848Epoch 00012: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0467 - val_acc: 0.9828\n",
      "Epoch 13/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9841Epoch 00013: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0413 - acc: 0.9841 - val_loss: 0.0492 - val_acc: 0.9829\n",
      "Epoch 14/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9822Epoch 00014: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0491 - acc: 0.9822 - val_loss: 0.0560 - val_acc: 0.9804\n",
      "Epoch 15/15\n",
      "71872/71889 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9813Epoch 00015: val_loss did not improve\n",
      "71889/71889 [==============================] - 395s 5ms/step - loss: 0.0507 - acc: 0.9813 - val_loss: 0.0496 - val_acc: 0.9818\n",
      "-------------------------------\n",
      "0 0.0999239420762 0.962473004976\n",
      "1 0.0238181664299 0.989838395009\n",
      "2 0.0518225848649 0.980626180217\n",
      "3 0.0107964430042 0.996828410763\n",
      "4 0.0646612012006 0.973573567308\n",
      "5 0.0219420198412 0.992780461341\n",
      "final 0.0454940595695 0.982686669936\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 15\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(4,3)\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_muse_1_csv_adj_add_de_fr.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_muse_adj_1_add_de_fr_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.525,  0.001,  0.04 ,  0.   ,  0.136,  0.003],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.004,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.423,  0.005,  0.089,  0.006,  0.137,  0.025],\n",
       "       [ 0.005,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.07 ,  0.   ,  0.004,  0.006,  0.008,  0.002],\n",
       "       [ 0.015,  0.   ,  0.002,  0.   ,  0.002,  0.   ],\n",
       "       [ 0.002,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
