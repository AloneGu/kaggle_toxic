{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "max_features = 40000\n",
    "maxlen = 120\n",
    "\n",
    "def clean_text( text ):\n",
    "    #https://www.kaggle.com/sreeram004/test-lr-with-convai-dataset\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 120) (226998, 120)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeanettepink 65201\n",
      "unresolvable 110149\n",
      "phonies 29935\n",
      "rome492000 80976\n",
      "desease 36441\n",
      "sendoffs 76621\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435713\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "\n",
    "with open('../wiki.multi.fr.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "\n",
    "with open('../wiki.multi.de.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "        \n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4716\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "# pre null 5293\n",
    "\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_lstm_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=Adam(0.005),\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63900 samples, validate on 31951 samples\n",
      "Epoch 1/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0697 - acc: 0.9766Epoch 00001: val_loss improved from inf to 0.05552, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 547s 9ms/step - loss: 0.0697 - acc: 0.9766 - val_loss: 0.0555 - val_acc: 0.9791\n",
      "Epoch 2/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0530 - acc: 0.9807Epoch 00002: val_loss improved from 0.05552 to 0.05379, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 616s 10ms/step - loss: 0.0530 - acc: 0.9807 - val_loss: 0.0538 - val_acc: 0.9803\n",
      "Epoch 3/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9818Epoch 00003: val_loss improved from 0.05379 to 0.04818, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 635s 10ms/step - loss: 0.0488 - acc: 0.9818 - val_loss: 0.0482 - val_acc: 0.9825\n",
      "Epoch 4/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0459 - acc: 0.9827Epoch 00004: val_loss did not improve\n",
      "63900/63900 [==============================] - 612s 10ms/step - loss: 0.0459 - acc: 0.9827 - val_loss: 0.0501 - val_acc: 0.9808\n",
      "Epoch 5/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9832Epoch 00005: val_loss improved from 0.04818 to 0.04797, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 623s 10ms/step - loss: 0.0438 - acc: 0.9832 - val_loss: 0.0480 - val_acc: 0.9824\n",
      "Epoch 6/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9838Epoch 00006: val_loss improved from 0.04797 to 0.04656, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 642s 10ms/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.0466 - val_acc: 0.9824\n",
      "Epoch 7/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9842Epoch 00007: val_loss improved from 0.04656 to 0.04435, saving model to weights_base.best.h5\n",
      "63900/63900 [==============================] - 684s 11ms/step - loss: 0.0402 - acc: 0.9842 - val_loss: 0.0444 - val_acc: 0.9832\n",
      "Epoch 8/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0384 - acc: 0.9848Epoch 00008: val_loss did not improve\n",
      "63900/63900 [==============================] - 634s 10ms/step - loss: 0.0384 - acc: 0.9848 - val_loss: 0.0459 - val_acc: 0.9830\n",
      "Epoch 9/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854Epoch 00009: val_loss did not improve\n",
      "63900/63900 [==============================] - 618s 10ms/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0474 - val_acc: 0.9825\n",
      "Epoch 10/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9858Epoch 00010: val_loss did not improve\n",
      "63900/63900 [==============================] - 615s 10ms/step - loss: 0.0351 - acc: 0.9858 - val_loss: 0.0468 - val_acc: 0.9830\n",
      "Epoch 11/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9862Epoch 00011: val_loss did not improve\n",
      "63900/63900 [==============================] - 623s 10ms/step - loss: 0.0344 - acc: 0.9862 - val_loss: 0.0482 - val_acc: 0.9832\n",
      "Epoch 12/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9866Epoch 00012: val_loss did not improve\n",
      "63900/63900 [==============================] - 614s 10ms/step - loss: 0.0329 - acc: 0.9866 - val_loss: 0.0478 - val_acc: 0.9827\n",
      "Epoch 13/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0319 - acc: 0.9868Epoch 00013: val_loss did not improve\n",
      "63900/63900 [==============================] - 620s 10ms/step - loss: 0.0319 - acc: 0.9868 - val_loss: 0.0492 - val_acc: 0.9826\n",
      "Epoch 14/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9871Epoch 00014: val_loss did not improve\n",
      "63900/63900 [==============================] - 626s 10ms/step - loss: 0.0311 - acc: 0.9871 - val_loss: 0.0496 - val_acc: 0.9823\n",
      "Epoch 15/15\n",
      "63872/63900 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.9876Epoch 00015: val_loss did not improve\n",
      "63900/63900 [==============================] - 477s 7ms/step - loss: 0.0300 - acc: 0.9876 - val_loss: 0.0521 - val_acc: 0.9827\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0663 - acc: 0.9778Epoch 00001: val_loss improved from inf to 0.05312, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0663 - acc: 0.9778 - val_loss: 0.0531 - val_acc: 0.9807\n",
      "Epoch 2/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0508 - acc: 0.9812Epoch 00002: val_loss improved from 0.05312 to 0.04660, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0508 - acc: 0.9812 - val_loss: 0.0466 - val_acc: 0.9825\n",
      "Epoch 3/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9823Epoch 00003: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0467 - val_acc: 0.9829\n",
      "Epoch 4/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9830Epoch 00004: val_loss improved from 0.04660 to 0.04534, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0453 - val_acc: 0.9832\n",
      "Epoch 5/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9832Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0430 - acc: 0.9832 - val_loss: 0.0477 - val_acc: 0.9825\n",
      "Epoch 6/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9839Epoch 00006: val_loss improved from 0.04534 to 0.04474, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0415 - acc: 0.9839 - val_loss: 0.0447 - val_acc: 0.9834\n",
      "Epoch 7/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9845Epoch 00007: val_loss improved from 0.04474 to 0.04427, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0400 - acc: 0.9845 - val_loss: 0.0443 - val_acc: 0.9834\n",
      "Epoch 8/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9848Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0381 - acc: 0.9848 - val_loss: 0.0448 - val_acc: 0.9831\n",
      "Epoch 9/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9853Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 363s 6ms/step - loss: 0.0371 - acc: 0.9853 - val_loss: 0.0514 - val_acc: 0.9828\n",
      "Epoch 10/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9858Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0358 - acc: 0.9858 - val_loss: 0.0471 - val_acc: 0.9831\n",
      "Epoch 11/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9844Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0398 - acc: 0.9844 - val_loss: 0.0449 - val_acc: 0.9832\n",
      "Epoch 12/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9856Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0362 - acc: 0.9856 - val_loss: 0.0462 - val_acc: 0.9830\n",
      "Epoch 13/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9862Epoch 00013: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0480 - val_acc: 0.9830\n",
      "Epoch 14/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9867Epoch 00014: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0329 - acc: 0.9867 - val_loss: 0.0483 - val_acc: 0.9828\n",
      "Epoch 15/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00015: val_loss did not improve\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0479 - val_acc: 0.9832\n",
      "Train on 63901 samples, validate on 31950 samples\n",
      "Epoch 1/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0669 - acc: 0.9776Epoch 00001: val_loss improved from inf to 0.05336, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 362s 6ms/step - loss: 0.0669 - acc: 0.9776 - val_loss: 0.0534 - val_acc: 0.9808\n",
      "Epoch 2/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0496 - acc: 0.9817Epoch 00002: val_loss improved from 0.05336 to 0.04679, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0496 - acc: 0.9817 - val_loss: 0.0468 - val_acc: 0.9823\n",
      "Epoch 3/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9829Epoch 00003: val_loss improved from 0.04679 to 0.04652, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 360s 6ms/step - loss: 0.0460 - acc: 0.9829 - val_loss: 0.0465 - val_acc: 0.9819\n",
      "Epoch 4/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9835Epoch 00004: val_loss improved from 0.04652 to 0.04500, saving model to weights_base.best.h5\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0432 - acc: 0.9835 - val_loss: 0.0450 - val_acc: 0.9827\n",
      "Epoch 5/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9841Epoch 00005: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0415 - acc: 0.9841 - val_loss: 0.0483 - val_acc: 0.9826\n",
      "Epoch 6/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9845Epoch 00006: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0503 - val_acc: 0.9820\n",
      "Epoch 7/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9851Epoch 00007: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0379 - acc: 0.9851 - val_loss: 0.0479 - val_acc: 0.9823\n",
      "Epoch 8/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9856Epoch 00008: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0366 - acc: 0.9857 - val_loss: 0.0466 - val_acc: 0.9826\n",
      "Epoch 9/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9861Epoch 00009: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0349 - acc: 0.9861 - val_loss: 0.0480 - val_acc: 0.9823\n",
      "Epoch 10/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9865Epoch 00010: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0337 - acc: 0.9865 - val_loss: 0.0477 - val_acc: 0.9826\n",
      "Epoch 11/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9869Epoch 00011: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0323 - acc: 0.9868 - val_loss: 0.0494 - val_acc: 0.9831\n",
      "Epoch 12/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9872Epoch 00012: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0312 - acc: 0.9873 - val_loss: 0.0493 - val_acc: 0.9826\n",
      "Epoch 13/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9875Epoch 00013: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0307 - acc: 0.9875 - val_loss: 0.0517 - val_acc: 0.9826\n",
      "Epoch 14/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9879Epoch 00014: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0292 - acc: 0.9879 - val_loss: 0.0557 - val_acc: 0.9825\n",
      "Epoch 15/15\n",
      "63872/63901 [============================>.] - ETA: 0s - loss: 0.0281 - acc: 0.9883Epoch 00015: val_loss did not improve\n",
      "63901/63901 [==============================] - 361s 6ms/step - loss: 0.0281 - acc: 0.9883 - val_loss: 0.0557 - val_acc: 0.9823\n",
      "-------------------------------\n",
      "0 0.0977159988481 0.963839709549\n",
      "1 0.0232766617583 0.990360037976\n",
      "2 0.0521361076991 0.980667911655\n",
      "3 0.00876594235773 0.997047500809\n",
      "4 0.064128087052 0.97338577584\n",
      "5 0.0212347472406 0.993302104308\n",
      "final 0.0445429241593 0.983100506689\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_lstm_model()\n",
    "        batch_size = 64\n",
    "        epochs = 15\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train(3,3)\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_muse_1_csv_adj2_add_de_fr.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/lstm_muse_adj2_add_de_fr_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.45 ,  0.003,  0.057,  0.003,  0.146,  0.002],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.006,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.239,  0.001,  0.056,  0.   ,  0.113,  0.011],\n",
       "       [ 0.006,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.15 ,  0.001,  0.002,  0.029,  0.007,  0.002],\n",
       "       [ 0.062,  0.   ,  0.002,  0.   ,  0.006,  0.001],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
