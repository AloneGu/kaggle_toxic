{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from models_def import cnn2d,cnn_v1,cnn_v2,cudnn_gru,lstm_v1,cnn_gru,gru_v1\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "model_defines = [\n",
    "#     [cnn2d,'cnn2d'],\n",
    "#     [cnn_v1,'cnn_v1'],\n",
    "#     [cnn_v2,'cnn_v2'],\n",
    "    [cudnn_gru,'cudnn_gru'],\n",
    "    [lstm_v1,'lstm_v1'],\n",
    "    [cnn_gru,'cnn_gru'],\n",
    "    [gru_v1,'gru_v1']\n",
    "]\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = max_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9793Epoch 00001: val_loss improved from inf to 0.04871, saving model to best_model.h5\n",
      "127656/127656 [==============================] - 166s 1ms/step - loss: 0.0600 - acc: 0.9793 - val_loss: 0.0487 - val_acc: 0.9820\n",
      "Epoch 2/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9832Epoch 00002: val_loss did not improve\n",
      "127656/127656 [==============================] - 168s 1ms/step - loss: 0.0444 - acc: 0.9832 - val_loss: 0.0492 - val_acc: 0.9821\n",
      "Epoch 3/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9854Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 171s 1ms/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0537 - val_acc: 0.9818\n",
      "Epoch 4/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 173s 1ms/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0582 - val_acc: 0.9819\n",
      "Epoch 5/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0291 - acc: 0.9883Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 173s 1ms/step - loss: 0.0291 - acc: 0.9883 - val_loss: 0.0609 - val_acc: 0.9809\n",
      "Epoch 6/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9895Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 173s 1ms/step - loss: 0.0263 - acc: 0.9895 - val_loss: 0.0640 - val_acc: 0.9814\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0599 - acc: 0.9793Epoch 00001: val_loss improved from inf to 0.04994, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 247s 2ms/step - loss: 0.0599 - acc: 0.9793 - val_loss: 0.0499 - val_acc: 0.9810\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9830Epoch 00002: val_loss improved from 0.04994 to 0.04910, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0447 - acc: 0.9830 - val_loss: 0.0491 - val_acc: 0.9821\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0377 - acc: 0.9852 - val_loss: 0.0505 - val_acc: 0.9821\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9866Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0331 - acc: 0.9866 - val_loss: 0.0568 - val_acc: 0.9812\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9882Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0294 - acc: 0.9882 - val_loss: 0.0616 - val_acc: 0.9815\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0273 - acc: 0.9891Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0273 - acc: 0.9891 - val_loss: 0.0659 - val_acc: 0.9812\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.04733, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 174s 1ms/step - loss: 0.0603 - acc: 0.9791 - val_loss: 0.0473 - val_acc: 0.9828\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "127657/127657 [==============================] - 174s 1ms/step - loss: 0.0448 - acc: 0.9831 - val_loss: 0.0476 - val_acc: 0.9825\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0377 - acc: 0.9851 - val_loss: 0.0560 - val_acc: 0.9828\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9868Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0330 - acc: 0.9868 - val_loss: 0.0581 - val_acc: 0.9819\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0293 - acc: 0.9882Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0293 - acc: 0.9882 - val_loss: 0.0557 - val_acc: 0.9809\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0270 - acc: 0.9892Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0270 - acc: 0.9892 - val_loss: 0.0644 - val_acc: 0.9807\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0600 - acc: 0.9794Epoch 00001: val_loss improved from inf to 0.05119, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 174s 1ms/step - loss: 0.0600 - acc: 0.9794 - val_loss: 0.0512 - val_acc: 0.9821\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9830Epoch 00002: val_loss improved from 0.05119 to 0.04800, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 174s 1ms/step - loss: 0.0448 - acc: 0.9830 - val_loss: 0.0480 - val_acc: 0.9819\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0373 - acc: 0.9852 - val_loss: 0.0519 - val_acc: 0.9817\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0329 - acc: 0.9867Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0329 - acc: 0.9867 - val_loss: 0.0585 - val_acc: 0.9815\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9882Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0292 - acc: 0.9882 - val_loss: 0.0644 - val_acc: 0.9814\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9893Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0265 - acc: 0.9893 - val_loss: 0.0670 - val_acc: 0.9812\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9792Epoch 00001: val_loss improved from inf to 0.04923, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 174s 1ms/step - loss: 0.0603 - acc: 0.9792 - val_loss: 0.0492 - val_acc: 0.9817\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9831Epoch 00002: val_loss improved from 0.04923 to 0.04893, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 174s 1ms/step - loss: 0.0447 - acc: 0.9831 - val_loss: 0.0489 - val_acc: 0.9817\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0375 - acc: 0.9851 - val_loss: 0.0538 - val_acc: 0.9805\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0327 - acc: 0.9868Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0327 - acc: 0.9868 - val_loss: 0.0607 - val_acc: 0.9815\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0296 - acc: 0.9882Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 173s 1ms/step - loss: 0.0296 - acc: 0.9882 - val_loss: 0.0600 - val_acc: 0.9806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0268 - acc: 0.9893Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 164s 1ms/step - loss: 0.0268 - acc: 0.9893 - val_loss: 0.0696 - val_acc: 0.9808\n",
      "-------------------------------\n",
      "0 0.106547423812 0.960393805892\n",
      "1 0.0237582529162 0.990154852699\n",
      "2 0.0523350104538 0.980478909075\n",
      "3 0.0121699123789 0.997060869456\n",
      "4 0.0676647065367 0.973084081694\n",
      "5 0.0281385979614 0.991533549329\n",
      "final 0.0484356506765 0.982117678024\n",
      "../features/no_pretrained_cnn2d_5_feat.pkl done\n",
      "==================================\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/6\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.04960, saving model to best_model.h5\n",
      "127656/127656 [==============================] - 93s 726us/step - loss: 0.0625 - acc: 0.9787 - val_loss: 0.0496 - val_acc: 0.9814\n",
      "Epoch 2/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9827Epoch 00002: val_loss improved from 0.04960 to 0.04921, saving model to best_model.h5\n",
      "127656/127656 [==============================] - 92s 722us/step - loss: 0.0454 - acc: 0.9827 - val_loss: 0.0492 - val_acc: 0.9818\n",
      "Epoch 3/6\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9846Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 92s 718us/step - loss: 0.0390 - acc: 0.9846 - val_loss: 0.0512 - val_acc: 0.9813\n",
      "Epoch 4/6\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9860Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 92s 718us/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0563 - val_acc: 0.9811\n",
      "Epoch 5/6\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0313 - acc: 0.9874Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 92s 723us/step - loss: 0.0313 - acc: 0.9874 - val_loss: 0.0592 - val_acc: 0.9811\n",
      "Epoch 6/6\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0288 - acc: 0.9882Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 93s 730us/step - loss: 0.0288 - acc: 0.9882 - val_loss: 0.0635 - val_acc: 0.9805\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0612 - acc: 0.9789Epoch 00001: val_loss improved from inf to 0.04937, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 94s 733us/step - loss: 0.0611 - acc: 0.9789 - val_loss: 0.0494 - val_acc: 0.9815\n",
      "Epoch 2/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9827Epoch 00002: val_loss improved from 0.04937 to 0.04765, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 93s 730us/step - loss: 0.0454 - acc: 0.9827 - val_loss: 0.0477 - val_acc: 0.9824\n",
      "Epoch 3/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 728us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0497 - val_acc: 0.9815\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9862Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0342 - acc: 0.9862 - val_loss: 0.0564 - val_acc: 0.9818\n",
      "Epoch 5/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9873Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0314 - acc: 0.9873 - val_loss: 0.0580 - val_acc: 0.9808\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9884Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0286 - acc: 0.9884 - val_loss: 0.0622 - val_acc: 0.9798\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9786Epoch 00001: val_loss improved from inf to 0.05052, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 93s 732us/step - loss: 0.0623 - acc: 0.9786 - val_loss: 0.0505 - val_acc: 0.9815\n",
      "Epoch 2/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9824Epoch 00002: val_loss improved from 0.05052 to 0.04752, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 93s 731us/step - loss: 0.0462 - acc: 0.9825 - val_loss: 0.0475 - val_acc: 0.9827\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 728us/step - loss: 0.0393 - acc: 0.9845 - val_loss: 0.0504 - val_acc: 0.9823\n",
      "Epoch 4/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9860Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 728us/step - loss: 0.0350 - acc: 0.9860 - val_loss: 0.0553 - val_acc: 0.9822\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9874Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0312 - acc: 0.9875 - val_loss: 0.0584 - val_acc: 0.9819\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9883Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 729us/step - loss: 0.0287 - acc: 0.9883 - val_loss: 0.0647 - val_acc: 0.9815\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0621 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.04948, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 94s 733us/step - loss: 0.0621 - acc: 0.9787 - val_loss: 0.0495 - val_acc: 0.9819\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9827Epoch 00002: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0456 - acc: 0.9827 - val_loss: 0.0503 - val_acc: 0.9816\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9844Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0391 - acc: 0.9844 - val_loss: 0.0509 - val_acc: 0.9815\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9861Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0348 - acc: 0.9861 - val_loss: 0.0554 - val_acc: 0.9811\n",
      "Epoch 5/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9872Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0314 - acc: 0.9872 - val_loss: 0.0605 - val_acc: 0.9805\n",
      "Epoch 6/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9883Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 93s 727us/step - loss: 0.0286 - acc: 0.9883 - val_loss: 0.0623 - val_acc: 0.9805\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0622 - acc: 0.9788Epoch 00001: val_loss improved from inf to 0.05184, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 93s 732us/step - loss: 0.0622 - acc: 0.9788 - val_loss: 0.0518 - val_acc: 0.9808\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9825Epoch 00002: val_loss improved from 0.05184 to 0.04866, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 93s 731us/step - loss: 0.0455 - acc: 0.9825 - val_loss: 0.0487 - val_acc: 0.9823\n",
      "Epoch 3/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9847Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 92s 719us/step - loss: 0.0390 - acc: 0.9847 - val_loss: 0.0513 - val_acc: 0.9821\n",
      "Epoch 4/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9862Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 92s 718us/step - loss: 0.0344 - acc: 0.9862 - val_loss: 0.0557 - val_acc: 0.9817\n",
      "Epoch 5/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9875Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 92s 718us/step - loss: 0.0307 - acc: 0.9875 - val_loss: 0.0636 - val_acc: 0.9810\n",
      "Epoch 6/6\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.9885Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 92s 718us/step - loss: 0.0284 - acc: 0.9885 - val_loss: 0.0674 - val_acc: 0.9805\n",
      "-------------------------------\n",
      "0 0.107626946912 0.961214757067\n",
      "1 0.0239273013294 0.990098451473\n",
      "2 0.0524333514011 0.980623045541\n",
      "3 0.0115861175448 0.997092203471\n",
      "4 0.0690156564816 0.972570203859\n",
      "5 0.0266089366595 0.991633818175\n",
      "final 0.0485330517214 0.982205413264\n",
      "../features/no_pretrained_cnn_v1_5_feat.pkl done\n",
      "==================================\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0603 - acc: 0.9793Epoch 00001: val_loss improved from inf to 0.04938, saving model to best_model.h5\n",
      "127656/127656 [==============================] - 113s 885us/step - loss: 0.0603 - acc: 0.9793 - val_loss: 0.0494 - val_acc: 0.9810\n",
      "Epoch 2/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9832Epoch 00002: val_loss did not improve\n",
      "127656/127656 [==============================] - 112s 876us/step - loss: 0.0440 - acc: 0.9832 - val_loss: 0.0506 - val_acc: 0.9818\n",
      "Epoch 3/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 112s 876us/step - loss: 0.0371 - acc: 0.9852 - val_loss: 0.0573 - val_acc: 0.9816\n",
      "Epoch 4/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9870Epoch 00004: val_loss did not improve\n",
      "127656/127656 [==============================] - 112s 876us/step - loss: 0.0321 - acc: 0.9870 - val_loss: 0.0620 - val_acc: 0.9807\n",
      "Epoch 5/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9884Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 112s 875us/step - loss: 0.0290 - acc: 0.9884 - val_loss: 0.0643 - val_acc: 0.9806\n",
      "Epoch 6/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0263 - acc: 0.9894Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 112s 875us/step - loss: 0.0263 - acc: 0.9894 - val_loss: 0.0695 - val_acc: 0.9801\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.05044, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 112s 880us/step - loss: 0.0605 - acc: 0.9791 - val_loss: 0.0504 - val_acc: 0.9810\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9830Epoch 00002: val_loss improved from 0.05044 to 0.04966, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 112s 877us/step - loss: 0.0445 - acc: 0.9830 - val_loss: 0.0497 - val_acc: 0.9820\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0374 - acc: 0.9850 - val_loss: 0.0540 - val_acc: 0.9811\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0328 - acc: 0.9869Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0328 - acc: 0.9869 - val_loss: 0.0564 - val_acc: 0.9810\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9883Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0292 - acc: 0.9883 - val_loss: 0.0618 - val_acc: 0.9807\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0272 - acc: 0.9891Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0272 - acc: 0.9891 - val_loss: 0.0685 - val_acc: 0.9807\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0606 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.05006, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 112s 877us/step - loss: 0.0606 - acc: 0.9791 - val_loss: 0.0501 - val_acc: 0.9819\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0446 - acc: 0.9831Epoch 00002: val_loss improved from 0.05006 to 0.04963, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 112s 876us/step - loss: 0.0446 - acc: 0.9831 - val_loss: 0.0496 - val_acc: 0.9825\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0375 - acc: 0.9850 - val_loss: 0.0513 - val_acc: 0.9821\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9869Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0321 - acc: 0.9869 - val_loss: 0.0602 - val_acc: 0.9811\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0294 - acc: 0.9881Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0294 - acc: 0.9881 - val_loss: 0.0650 - val_acc: 0.9813\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0265 - acc: 0.9894Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 873us/step - loss: 0.0265 - acc: 0.9894 - val_loss: 0.0675 - val_acc: 0.9805\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9790Epoch 00001: val_loss improved from inf to 0.05007, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 114s 893us/step - loss: 0.0605 - acc: 0.9790 - val_loss: 0.0501 - val_acc: 0.9817\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9828Epoch 00002: val_loss did not improve\n",
      "127657/127657 [==============================] - 115s 902us/step - loss: 0.0447 - acc: 0.9828 - val_loss: 0.0513 - val_acc: 0.9819\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9853Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 116s 911us/step - loss: 0.0371 - acc: 0.9853 - val_loss: 0.0535 - val_acc: 0.9812\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9868Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 118s 924us/step - loss: 0.0325 - acc: 0.9868 - val_loss: 0.0614 - val_acc: 0.9808\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0287 - acc: 0.9883Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 115s 901us/step - loss: 0.0287 - acc: 0.9883 - val_loss: 0.0634 - val_acc: 0.9798\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9895Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 110s 859us/step - loss: 0.0264 - acc: 0.9895 - val_loss: 0.0697 - val_acc: 0.9805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.05068, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 110s 864us/step - loss: 0.0605 - acc: 0.9791 - val_loss: 0.0507 - val_acc: 0.9815\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9829Epoch 00002: val_loss improved from 0.05068 to 0.04952, saving model to best_model.h5\n",
      "127657/127657 [==============================] - 113s 886us/step - loss: 0.0447 - acc: 0.9829 - val_loss: 0.0495 - val_acc: 0.9821\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 111s 870us/step - loss: 0.0369 - acc: 0.9852 - val_loss: 0.0553 - val_acc: 0.9815\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9868Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 112s 875us/step - loss: 0.0325 - acc: 0.9868 - val_loss: 0.0595 - val_acc: 0.9815\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.9883Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 112s 878us/step - loss: 0.0292 - acc: 0.9883 - val_loss: 0.0609 - val_acc: 0.9799\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9895Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 112s 879us/step - loss: 0.0261 - acc: 0.9895 - val_loss: 0.0687 - val_acc: 0.9789\n",
      "-------------------------------\n",
      "0 0.110172104541 0.959391117434\n",
      "1 0.0246386042259 0.990399258011\n",
      "2 0.0535452043504 0.980460108666\n",
      "3 0.0120455032017 0.997098470273\n",
      "4 0.0699121832021 0.97220046249\n",
      "5 0.0278765307952 0.991621284569\n",
      "final 0.0496983550528 0.981861783574\n",
      "../features/no_pretrained_cnn_v2_5_feat.pkl done\n",
      "==================================\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import gc\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def kf_train(model_func,fold_cnt=3,rnd=1,epo=10,batch=64):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = model_func(maxlen,nb_words,EMBEDDING_DIM,embedding_matrix=None,trainable_flag=True,comp=True)\n",
    "        batch_size = batch\n",
    "        epochs = epo\n",
    "        file_path=\"best_model.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        \n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    eval_val(y,train_pred)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "rnd_init = 1\n",
    "for model_func,model_name in model_defines:\n",
    "    for fold_k in [5]:\n",
    "        print('==================================')\n",
    "        epo = 6\n",
    "        batch = 64\n",
    "        train_pred,test_pred = kf_train(model_func,fold_k,rnd_init,epo,batch)\n",
    "        output_feat = '../features/no_pretrained_{}_{}_feat.pkl'.format(model_name,fold_k)\n",
    "        with open(output_feat,'wb') as fout:\n",
    "            pickle.dump([train_pred,test_pred],fout)\n",
    "        print(output_feat,'done')\n",
    "        rnd_init += 1\n",
    "        \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
