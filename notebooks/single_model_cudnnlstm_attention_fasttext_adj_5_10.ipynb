{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file path ../features/other_feat.pkl\n",
      "(159571, 37) (153164, 37)\n",
      "file path ../features/lgb1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/rf1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/gbrt1_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/lr_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/ridge_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat1.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/mnb_feat2.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/wordbatch_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "file path ../features/tilli_lr_feat.pkl\n",
      "(159571, 6) (153164, 6)\n",
      "(159571, 103)\n",
      "[ 5.00000000e+01  4.60000000e+01  2.64000000e+02  0.00000000e+00\n",
      "  2.00000000e+01  0.00000000e+00  3.00000000e+00  1.20000000e+01\n",
      "  4.24000000e+00  0.00000000e+00  6.43939394e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  1.51515152e-02  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  4.60000000e+01  3.40000000e+02  2.50000000e+01\n",
      "  2.50000000e-01  9.20000000e-01  5.00000000e+01  1.00000000e+00\n",
      "  4.00000000e-01  3.00000000e+01  6.00000000e-01  6.00000000e-02\n",
      "  2.40000000e-01  4.97183589e-03  2.19470500e-04  1.91532348e-03\n",
      "  1.12080601e-04  6.59428638e-04  2.94189463e-04  1.47946929e-02\n",
      "  3.26913580e-04  2.42011602e-03  0.00000000e+00  1.37730550e-04\n",
      "  1.21951220e-04  3.99514196e-02  2.54662439e-03  1.33481426e-02\n",
      "  6.83866136e-04  1.44429785e-02  2.70323671e-03  6.48717407e-03\n",
      "  1.43058692e-03  3.50386039e-03  7.49363315e-04  2.99510552e-03\n",
      "  1.17916128e-03  7.35962419e-03  6.12739154e-04  2.56212582e-03\n",
      "  3.21850807e-04  2.84586342e-03  5.57503823e-04 -2.61802127e-02\n",
      " -3.55490310e-03 -2.24073743e-02 -2.24863418e-03 -3.27414011e-02\n",
      " -9.82143021e-03 -8.71819733e-03 -1.09741811e-02 -1.36728622e-02\n",
      " -4.26841221e-03 -2.65620055e-02 -1.09669200e-02  1.47045578e-02\n",
      "  2.72551819e-04  4.38906331e-03  6.81379250e-05  4.40233785e-03\n",
      "  2.41395784e-04  3.02297447e-04  5.99867599e-09  2.28718188e-05\n",
      "  1.14941809e-08  1.50047469e-05  1.12412318e-07  4.78512980e-01\n",
      "  4.94473060e-01  4.93796720e-01  4.86044940e-01  4.89667930e-01\n",
      "  4.97940090e-01  4.09644022e-03  6.09518792e-03  1.30472148e-02\n",
      "  2.28091293e-03  1.25153108e-02  6.42610836e-03]\n",
      "[3.49500713e-02 9.33609959e-02 5.16619944e-02 0.00000000e+00\n",
      " 2.09863589e-02 0.00000000e+00 2.21893491e-03 1.20000000e-02\n",
      " 2.61501211e-03 0.00000000e+00 6.45107832e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.19159566e-02 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 3.20970043e-02 4.39760604e-02 8.88888889e-01\n",
      " 2.64705882e-01 9.19935949e-01 3.49500713e-02 0.00000000e+00\n",
      " 4.00000000e-01 2.40000000e-02 6.00000000e-01 6.00000000e-02\n",
      " 2.40000000e-01 4.83638733e-03 2.17394968e-04 1.79814374e-03\n",
      " 1.11942435e-04 5.97145528e-04 2.77291187e-04 1.57592680e-02\n",
      " 7.78585459e-04 2.59223277e-03 0.00000000e+00 1.62078167e-04\n",
      " 1.96409810e-04 2.66725597e-02 9.76973117e-04 3.09916152e-03\n",
      " 6.83866136e-04 3.15784994e-03 8.58252817e-04 6.41563293e-03\n",
      " 1.38321079e-03 3.37450246e-03 7.06767259e-04 2.88629059e-03\n",
      " 1.12457600e-03 7.31546044e-03 5.68872108e-04 2.40054267e-03\n",
      " 3.05619484e-04 2.77494446e-03 5.34113253e-04 6.67202266e-02\n",
      " 4.87881259e-02 2.28364572e-02 3.87381735e-02 3.56464940e-02\n",
      " 2.62395352e-02 6.39271377e-02 4.78549963e-02 2.95811533e-02\n",
      " 4.17371945e-02 3.53564038e-02 3.71946016e-02 1.47043943e-02\n",
      " 2.72562789e-04 4.38900502e-03 6.98790411e-05 4.40231664e-03\n",
      " 2.41533516e-04 3.02297447e-04 5.99867599e-09 2.28718188e-05\n",
      " 1.14941809e-08 1.50047469e-05 1.12412318e-07 6.12307004e-02\n",
      " 9.00218375e-02 5.42136083e-02 5.61924912e-02 4.89451054e-02\n",
      " 8.05734814e-02 4.06211702e-03 5.70668569e-03 1.26388567e-02\n",
      " 2.22039166e-03 1.22442901e-02 6.32540379e-03]\n",
      "(159571, 6)\n",
      "(159571, 250) (153164, 250)\n",
      "premiership 14429\n",
      "nsarchive 152211\n",
      "marbleworks 163458\n",
      "perview 161446\n",
      "yuck 28348\n",
      "philomela 135866\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 66521\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import string\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "# prepare other feat\n",
    "fl = [\n",
    "    '../features/other_feat.pkl',\n",
    "    '../features/lgb1_feat.pkl',\n",
    "    '../features/rf1_feat.pkl',\n",
    "    '../features/gbrt1_feat.pkl',\n",
    "    '../features/lr_feat1.pkl',\n",
    "    '../features/lr_feat2.pkl',\n",
    "    '../features/ridge_feat1.pkl',\n",
    "    '../features/ridge_feat2.pkl',\n",
    "    '../features/mnb_feat1.pkl',\n",
    "    '../features/mnb_feat2.pkl',\n",
    "    '../features/wordbatch_feat.pkl',\n",
    "    '../features/tilli_lr_feat.pkl',\n",
    "\n",
    "]\n",
    "def get_feat(f):\n",
    "    with open(f,'rb') as fin:\n",
    "        a,b = pickle.load(fin)\n",
    "        return a,b\n",
    "\n",
    "# load feats\n",
    "train_x,test_x = [],[]\n",
    "for feat in fl:\n",
    "    print('file path',feat)\n",
    "    a,b = pickle.load(open(feat,'rb'))\n",
    "    print(a.shape,b.shape)\n",
    "    train_x.append(a)\n",
    "    test_x.append(b)\n",
    "train_x = np.nan_to_num(np.hstack(train_x))\n",
    "test_x = np.nan_to_num(np.hstack(test_x))\n",
    "print(train_x.shape)\n",
    "print(train_x[0])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "print(train_x[0])\n",
    "\n",
    "max_features = 160000\n",
    "maxlen = 250\n",
    "\n",
    "# Contraction replacement patterns\n",
    "cont_patterns = [\n",
    "    (b'(W|w)on\\'t', b'will not'),\n",
    "    (b'(C|c)an\\'t', b'can not'),\n",
    "    (b'(I|i)\\'m', b'i am'),\n",
    "    (b'(A|a)in\\'t', b'is not'),\n",
    "    (b'(\\w+)\\'ll', b'\\g<1> will'),\n",
    "    (b'(\\w+)n\\'t', b'\\g<1> not'),\n",
    "    (b'(\\w+)\\'ve', b'\\g<1> have'),\n",
    "    (b'(\\w+)\\'s', b'\\g<1> is'),\n",
    "    (b'(\\w+)\\'re', b'\\g<1> are'),\n",
    "    (b'(\\w+)\\'d', b'\\g<1> would'),\n",
    "    (b'&lt;3', b' heart '),\n",
    "    (b':d', b' smile '),\n",
    "    (b':dd', b' smile '),\n",
    "    (b':p', b' smile '),\n",
    "    (b'8\\)', b' smile '),\n",
    "    (b':-\\)', b' smile '),\n",
    "    (b':\\)', b' smile '),\n",
    "    (b';\\)', b' smile '),\n",
    "    (b'\\(-:', b' smile '),\n",
    "    (b'\\(:', b' smile '),\n",
    "    (b'yay!', b' good '),\n",
    "    (b'yay', b' good '),\n",
    "    (b'yaay', b' good '),\n",
    "    (b':/', b' worry '),\n",
    "    (b':&gt;', b' angry '),\n",
    "    (b\":'\\)\", b' sad '),\n",
    "    (b':-\\(', b' sad '),\n",
    "    (b':\\(', b' sad '),\n",
    "    (b':s', b' sad '),\n",
    "    (b':-s', b' sad '),\n",
    "    (b'\\d{1,3}.\\d{1,3}.\\d{1,3}.\\d{1,3}', b' '),\n",
    "    (b'(\\[[\\s\\S]*\\])', b' '),\n",
    "    (b'[\\s]*?(www.[\\S]*)', b' ')\n",
    "]\n",
    "patterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]\n",
    "\n",
    "\n",
    "\n",
    "def new_clean(text):\n",
    "    \"\"\" Simple text clean up process\"\"\"\n",
    "    # 1. Go to lower case (only good for english)\n",
    "    # Go to bytes_strings as I had issues removing all \\n in r\"\"\n",
    "    clean = bytes(text.lower(), encoding=\"utf-8\")\n",
    "    \n",
    "    # replace words like hhhhhhhhhhhhhhi with hi\n",
    "    for ch in string.ascii_lowercase:\n",
    "        pattern = bytes(ch+'{3,}', encoding=\"utf-8\")\n",
    "        clean = re.sub(pattern, bytes(ch, encoding=\"utf-8\"), clean)\n",
    "    # 2. Drop \\n and  \\t\n",
    "    clean = clean.replace(b\"\\n\", b\" \")\n",
    "    clean = clean.replace(b\"\\t\", b\" \")\n",
    "    clean = clean.replace(b\"\\b\", b\" \")\n",
    "    clean = clean.replace(b\"\\r\", b\" \")\n",
    "    # 3. Replace english contractions\n",
    "    for (pattern, repl) in patterns:\n",
    "        clean = re.sub(pattern, repl, clean)\n",
    "    # 4. Drop puntuation\n",
    "    # I could have used regex package with regex.sub(b\"\\p{P}\", \" \")\n",
    "    exclude = re.compile(b'[%s]' % re.escape(bytes(string.punctuation, encoding='utf-8')))\n",
    "    clean = b\" \".join([exclude.sub(b'', token) for token in clean.split()])\n",
    "    # 5. Drop numbers - as a scientist I don't think numbers are toxic ;-)\n",
    "    clean = re.sub(b\"\\d+\", b\" \", clean)\n",
    "    # 6. Remove extra spaces - At the end of previous operations we multiplied space accurences\n",
    "    clean = re.sub(b'\\s+', b' ', clean)\n",
    "    # Remove ending space if any\n",
    "    clean = re.sub(b'\\s+$', b'', clean)\n",
    "    # 7. Now replace words by words surrounded by # signs\n",
    "    # e.g. my name is bond would become #my# #name# #is# #bond#\n",
    "    # clean = re.sub(b\"([a-z]+)\", b\"#\\g<1>#\", clean)\n",
    "    clean = re.sub(b\" \", b\"# #\", clean)  # Replace space\n",
    "    clean = b\"#\" + clean + b\"#\"  # add leading and trailing #\n",
    "\n",
    "    return str(clean, 'utf-8')\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    return new_clean(text)\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model(comp):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    inp_2 = Input(shape=[train_x.shape[1]], name=\"other\")\n",
    "    emb = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    emb = SpatialDropout1D(0.4)(emb)\n",
    "    x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(emb)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    conc = concatenate([att, avg_pool, max_pool, inp_2])\n",
    "    conc = Dense(256, activation=\"relu\")(conc)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=[inp,inp_2], outputs=outp)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import gc\n",
    "from keras import backend as K\n",
    "\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        curr_other_x = train_x[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        hold_out_other_x = train_x[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model(True)\n",
    "        batch_size = 64\n",
    "        epochs = 6\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit([curr_x,curr_other_x], curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=([hold_out_x,hold_out_other_x],hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict([X_test,test_x])\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict([hold_out_x,hold_out_other_x])\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9837Epoch 00001: val_loss improved from inf to 0.03882, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 177s 1ms/step - loss: 0.0429 - acc: 0.9837 - val_loss: 0.0388 - val_acc: 0.9843\n",
      "Epoch 2/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9848Epoch 00002: val_loss did not improve\n",
      "127656/127656 [==============================] - 177s 1ms/step - loss: 0.0380 - acc: 0.9848 - val_loss: 0.0392 - val_acc: 0.9836\n",
      "Epoch 3/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9852Epoch 00003: val_loss did not improve\n",
      "127656/127656 [==============================] - 180s 1ms/step - loss: 0.0367 - acc: 0.9852 - val_loss: 0.0391 - val_acc: 0.9836\n",
      "Epoch 4/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9855Epoch 00004: val_loss improved from 0.03882 to 0.03731, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 183s 1ms/step - loss: 0.0356 - acc: 0.9855 - val_loss: 0.0373 - val_acc: 0.9847\n",
      "Epoch 5/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "127656/127656 [==============================] - 183s 1ms/step - loss: 0.0346 - acc: 0.9858 - val_loss: 0.0382 - val_acc: 0.9840\n",
      "Epoch 6/6\n",
      "127616/127656 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "127656/127656 [==============================] - 184s 1ms/step - loss: 0.0336 - acc: 0.9861 - val_loss: 0.0384 - val_acc: 0.9842\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9834Epoch 00001: val_loss improved from inf to 0.03878, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0430 - acc: 0.9834 - val_loss: 0.0388 - val_acc: 0.9845\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9847Epoch 00002: val_loss improved from 0.03878 to 0.03772, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0380 - acc: 0.9847 - val_loss: 0.0377 - val_acc: 0.9848\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9851Epoch 00003: val_loss improved from 0.03772 to 0.03696, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0368 - acc: 0.9851 - val_loss: 0.0370 - val_acc: 0.9852\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0355 - acc: 0.9854 - val_loss: 0.0375 - val_acc: 0.9850\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9857Epoch 00005: val_loss improved from 0.03696 to 0.03658, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0345 - acc: 0.9857 - val_loss: 0.0366 - val_acc: 0.9855\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0335 - acc: 0.9861 - val_loss: 0.0370 - val_acc: 0.9854\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03889, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0432 - acc: 0.9836 - val_loss: 0.0389 - val_acc: 0.9847\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9846Epoch 00002: val_loss improved from 0.03889 to 0.03751, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0383 - acc: 0.9846 - val_loss: 0.0375 - val_acc: 0.9848\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9849Epoch 00003: val_loss improved from 0.03751 to 0.03705, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0370 - acc: 0.9849 - val_loss: 0.0371 - val_acc: 0.9854\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9853Epoch 00004: val_loss improved from 0.03705 to 0.03700, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0358 - acc: 0.9853 - val_loss: 0.0370 - val_acc: 0.9853\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9856Epoch 00005: val_loss improved from 0.03700 to 0.03676, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0349 - acc: 0.9856 - val_loss: 0.0368 - val_acc: 0.9851\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9859Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0338 - acc: 0.9859 - val_loss: 0.0371 - val_acc: 0.9851\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03881, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 186s 1ms/step - loss: 0.0432 - acc: 0.9836 - val_loss: 0.0388 - val_acc: 0.9845\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9847Epoch 00002: val_loss improved from 0.03881 to 0.03771, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0380 - acc: 0.9847 - val_loss: 0.0377 - val_acc: 0.9848\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "127657/127657 [==============================] - 184s 1ms/step - loss: 0.0367 - acc: 0.9851 - val_loss: 0.0380 - val_acc: 0.9846\n",
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9854Epoch 00004: val_loss improved from 0.03771 to 0.03728, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0355 - acc: 0.9854 - val_loss: 0.0373 - val_acc: 0.9848\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0345 - acc: 0.9857 - val_loss: 0.0388 - val_acc: 0.9842\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0334 - acc: 0.9861 - val_loss: 0.0382 - val_acc: 0.9843\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9835Epoch 00001: val_loss improved from inf to 0.03944, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 186s 1ms/step - loss: 0.0431 - acc: 0.9835 - val_loss: 0.0394 - val_acc: 0.9843\n",
      "Epoch 2/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9846Epoch 00002: val_loss improved from 0.03944 to 0.03818, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 180s 1ms/step - loss: 0.0381 - acc: 0.9846 - val_loss: 0.0382 - val_acc: 0.9852\n",
      "Epoch 3/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9849Epoch 00003: val_loss improved from 0.03818 to 0.03728, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0368 - acc: 0.9849 - val_loss: 0.0373 - val_acc: 0.9852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "127657/127657 [==============================] - 175s 1ms/step - loss: 0.0357 - acc: 0.9853 - val_loss: 0.0373 - val_acc: 0.9853\n",
      "Epoch 5/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 181s 1ms/step - loss: 0.0346 - acc: 0.9856 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 6/6\n",
      "127616/127657 [============================>.] - ETA: 0s - loss: 0.0335 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 185s 1ms/step - loss: 0.0335 - acc: 0.9860 - val_loss: 0.0377 - val_acc: 0.9848\n",
      "-------------------------------\n",
      "0 0.07931138260284377 0.9689605254087522\n",
      "1 0.020891926770841427 0.9909507366626769\n",
      "2 0.040113176926091586 0.9834932412531099\n",
      "3 0.0076026471764150585 0.9973115415708368\n",
      "4 0.055595746264591975 0.9766812265386567\n",
      "5 0.018732496993561963 0.9930125148053218\n",
      "final 0.037041229455724294 0.985068297706559\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=42)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "# 40000,150,lstm + global max_pool\n",
    "# final 0.0407274256871 0.984048897774\n",
    "\n",
    "# 100000,150 lstm + attention, glove embedding\n",
    "# final 0.0404159162853 0.984188856371, pub 9849\n",
    "# 3996, 4093\n",
    "\n",
    "# 100000,150 lstm + attention, use spacial dropout,spacial 0.2, last dropout 0.5, fasttext embedding\n",
    "# 1st epo 4016, 2nd epo 4117, not better compare to glove res\n",
    "\n",
    "# 100000,150,test arch\n",
    "#     x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "#     x = Attention(maxlen)(x)\n",
    "#     x = Dense(6, activation=\"sigmoid\")(x)\n",
    "# 1st epo 4116, not good\n",
    "\n",
    "# 100000,150,test arch\n",
    "#     x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "#     x = Dropout(0.2)(x)\n",
    "#     x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "#     att = Attention(maxlen)(x)\n",
    "#     avg_pool = GlobalAveragePooling1D()(x)\n",
    "#     max_pool = GlobalMaxPooling1D()(x)\n",
    "#     conc = concatenate([att,avg_pool, max_pool])\n",
    "#     x = Dense(256, activation=\"relu\")(conc)\n",
    "#     x = Dense(6, activation=\"sigmoid\")(x)\n",
    "# 1st epo , old LSTM 3945\n",
    "# to save time ,change to CuDNNLSTM\n",
    "# 1st epo , 3928, 4 fold: final 0.0393455938053 0.984445795289\n",
    "# 10 fold: final 0.0391567844913 0.984588887287 PUB 9857\n",
    "\n",
    "# new adj\n",
    "# 5 fold: final 0.037041229455724294 0.985068297706559 PUB 9862"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
      "0  00001cee341fdb12  0.999125  3.895919e-01  0.971504  2.315079e-01  0.932501   \n",
      "1  0000247867823ef7  0.000086  4.912789e-08  0.000006  1.349892e-06  0.000005   \n",
      "2  00013b17ad220c46  0.000011  3.925341e-09  0.000002  2.898996e-07  0.000001   \n",
      "3  00017563c3f7919a  0.000061  1.205650e-07  0.000006  1.818519e-05  0.000012   \n",
      "4  00017695ad8997eb  0.000678  1.680068e-07  0.000024  6.937993e-06  0.000020   \n",
      "\n",
      "   identity_hate  \n",
      "0   5.447136e-01  \n",
      "1   5.466332e-07  \n",
      "2   4.879677e-07  \n",
      "3   1.390482e-06  \n",
      "4   2.878596e-06  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/6\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0428 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03842, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 205s 1ms/step - loss: 0.0428 - acc: 0.9836 - val_loss: 0.0384 - val_acc: 0.9844\n",
      "Epoch 2/6\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9848Epoch 00002: val_loss improved from 0.03842 to 0.03728, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 203s 1ms/step - loss: 0.0380 - acc: 0.9848 - val_loss: 0.0373 - val_acc: 0.9847\n",
      "Epoch 3/6\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9850Epoch 00003: val_loss improved from 0.03728 to 0.03701, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 196s 1ms/step - loss: 0.0368 - acc: 0.9851 - val_loss: 0.0370 - val_acc: 0.9851\n",
      "Epoch 4/6\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9855Epoch 00004: val_loss improved from 0.03701 to 0.03681, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 197s 1ms/step - loss: 0.0358 - acc: 0.9855 - val_loss: 0.0368 - val_acc: 0.9848\n",
      "Epoch 5/6\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 195s 1ms/step - loss: 0.0348 - acc: 0.9858 - val_loss: 0.0376 - val_acc: 0.9846\n",
      "Epoch 6/6\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9859Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 195s 1ms/step - loss: 0.0340 - acc: 0.9859 - val_loss: 0.0373 - val_acc: 0.9846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9838Epoch 00001: val_loss improved from inf to 0.03901, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0425 - acc: 0.9838 - val_loss: 0.0390 - val_acc: 0.9840\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9848Epoch 00002: val_loss improved from 0.03901 to 0.03811, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0378 - acc: 0.9848 - val_loss: 0.0381 - val_acc: 0.9842\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9853Epoch 00003: val_loss improved from 0.03811 to 0.03776, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 194s 1ms/step - loss: 0.0364 - acc: 0.9853 - val_loss: 0.0378 - val_acc: 0.9842\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9856Epoch 00004: val_loss improved from 0.03776 to 0.03748, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0354 - acc: 0.9856 - val_loss: 0.0375 - val_acc: 0.9845\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 188s 1ms/step - loss: 0.0344 - acc: 0.9858 - val_loss: 0.0385 - val_acc: 0.9836\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9862Epoch 00006: val_loss improved from 0.03748 to 0.03725, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0336 - acc: 0.9862 - val_loss: 0.0373 - val_acc: 0.9844\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03665, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0366 - val_acc: 0.9852\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9847Epoch 00002: val_loss improved from 0.03665 to 0.03655, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0380 - acc: 0.9847 - val_loss: 0.0366 - val_acc: 0.9851\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9850Epoch 00003: val_loss improved from 0.03655 to 0.03597, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0368 - acc: 0.9850 - val_loss: 0.0360 - val_acc: 0.9852\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9853Epoch 00004: val_loss improved from 0.03597 to 0.03590, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0358 - acc: 0.9853 - val_loss: 0.0359 - val_acc: 0.9853\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0347 - acc: 0.9857 - val_loss: 0.0362 - val_acc: 0.9854\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0337 - acc: 0.9861 - val_loss: 0.0360 - val_acc: 0.9852\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.04022, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 195s 1ms/step - loss: 0.0424 - acc: 0.9836 - val_loss: 0.0402 - val_acc: 0.9844\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9846Epoch 00002: val_loss improved from 0.04022 to 0.03896, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 195s 1ms/step - loss: 0.0380 - acc: 0.9846 - val_loss: 0.0390 - val_acc: 0.9844\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9851Epoch 00003: val_loss improved from 0.03896 to 0.03762, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 191s 1ms/step - loss: 0.0367 - acc: 0.9851 - val_loss: 0.0376 - val_acc: 0.9853\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0355 - acc: 0.9854 - val_loss: 0.0382 - val_acc: 0.9851\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0347 - acc: 0.9856 - val_loss: 0.0391 - val_acc: 0.9845\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0338 - acc: 0.9860 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03813, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0381 - val_acc: 0.9849\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9848Epoch 00002: val_loss improved from 0.03813 to 0.03794, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0378 - acc: 0.9848 - val_loss: 0.0379 - val_acc: 0.9849\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9851Epoch 00003: val_loss improved from 0.03794 to 0.03636, saving model to weights_base.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0366 - acc: 0.9851 - val_loss: 0.0364 - val_acc: 0.9852\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0354 - acc: 0.9854 - val_loss: 0.0369 - val_acc: 0.9848\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0346 - acc: 0.9856 - val_loss: 0.0375 - val_acc: 0.9851\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0336 - acc: 0.9861 - val_loss: 0.0369 - val_acc: 0.9853\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9837Epoch 00001: val_loss improved from inf to 0.03728, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0427 - acc: 0.9837 - val_loss: 0.0373 - val_acc: 0.9849\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9847Epoch 00002: val_loss improved from 0.03728 to 0.03608, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 187s 1ms/step - loss: 0.0379 - acc: 0.9847 - val_loss: 0.0361 - val_acc: 0.9852\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9850Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 186s 1ms/step - loss: 0.0367 - acc: 0.9850 - val_loss: 0.0361 - val_acc: 0.9854\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9854Epoch 00004: val_loss improved from 0.03608 to 0.03595, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0358 - acc: 0.9854 - val_loss: 0.0360 - val_acc: 0.9851\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0348 - acc: 0.9857 - val_loss: 0.0373 - val_acc: 0.9846\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9861Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 195s 1ms/step - loss: 0.0337 - acc: 0.9861 - val_loss: 0.0366 - val_acc: 0.9850\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03801, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0380 - val_acc: 0.9846\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9846Epoch 00002: val_loss improved from 0.03801 to 0.03677, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0380 - acc: 0.9846 - val_loss: 0.0368 - val_acc: 0.9849\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 195s 1ms/step - loss: 0.0367 - acc: 0.9851 - val_loss: 0.0368 - val_acc: 0.9851\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9854Epoch 00004: val_loss improved from 0.03677 to 0.03645, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0358 - acc: 0.9854 - val_loss: 0.0365 - val_acc: 0.9850\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 195s 1ms/step - loss: 0.0347 - acc: 0.9857 - val_loss: 0.0378 - val_acc: 0.9846\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 196s 1ms/step - loss: 0.0339 - acc: 0.9861 - val_loss: 0.0372 - val_acc: 0.9846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9836Epoch 00001: val_loss improved from inf to 0.03830, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0425 - acc: 0.9836 - val_loss: 0.0383 - val_acc: 0.9845\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9847Epoch 00002: val_loss improved from 0.03830 to 0.03777, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 199s 1ms/step - loss: 0.0378 - acc: 0.9847 - val_loss: 0.0378 - val_acc: 0.9845\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9852Epoch 00003: val_loss improved from 0.03777 to 0.03718, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0364 - acc: 0.9852 - val_loss: 0.0372 - val_acc: 0.9846\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0355 - acc: 0.9855 - val_loss: 0.0378 - val_acc: 0.9846\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9858Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0344 - acc: 0.9858 - val_loss: 0.0381 - val_acc: 0.9844\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 197s 1ms/step - loss: 0.0336 - acc: 0.9860 - val_loss: 0.0378 - val_acc: 0.9843\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9837Epoch 00001: val_loss improved from inf to 0.03748, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 199s 1ms/step - loss: 0.0424 - acc: 0.9837 - val_loss: 0.0375 - val_acc: 0.9853\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9847Epoch 00002: val_loss improved from 0.03748 to 0.03695, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0380 - acc: 0.9847 - val_loss: 0.0370 - val_acc: 0.9852\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0366 - acc: 0.9850Epoch 00003: val_loss improved from 0.03695 to 0.03615, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 199s 1ms/step - loss: 0.0366 - acc: 0.9850 - val_loss: 0.0361 - val_acc: 0.9852\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9854Epoch 00004: val_loss improved from 0.03615 to 0.03585, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 214s 1ms/step - loss: 0.0355 - acc: 0.9854 - val_loss: 0.0358 - val_acc: 0.9857\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0347 - acc: 0.9856Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 198s 1ms/step - loss: 0.0347 - acc: 0.9856 - val_loss: 0.0359 - val_acc: 0.9856\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0337 - acc: 0.9860 - val_loss: 0.0361 - val_acc: 0.9857\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9838Epoch 00001: val_loss improved from inf to 0.03954, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 191s 1ms/step - loss: 0.0424 - acc: 0.9838 - val_loss: 0.0395 - val_acc: 0.9844\n",
      "Epoch 2/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9847Epoch 00002: val_loss improved from 0.03954 to 0.03846, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 188s 1ms/step - loss: 0.0378 - acc: 0.9847 - val_loss: 0.0385 - val_acc: 0.9848\n",
      "Epoch 3/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 189s 1ms/step - loss: 0.0365 - acc: 0.9851 - val_loss: 0.0392 - val_acc: 0.9841\n",
      "Epoch 4/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9853Epoch 00004: val_loss improved from 0.03846 to 0.03811, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 190s 1ms/step - loss: 0.0354 - acc: 0.9853 - val_loss: 0.0381 - val_acc: 0.9847\n",
      "Epoch 5/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9858Epoch 00005: val_loss improved from 0.03811 to 0.03788, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 194s 1ms/step - loss: 0.0344 - acc: 0.9858 - val_loss: 0.0379 - val_acc: 0.9851\n",
      "Epoch 6/6\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0334 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 189s 1ms/step - loss: 0.0334 - acc: 0.9860 - val_loss: 0.0393 - val_acc: 0.9841\n",
      "-------------------------------\n",
      "0 0.07845973429999673 0.9688414561543137\n",
      "1 0.02069356194870345 0.9907376653652606\n",
      "2 0.04001948253566198 0.9833804388015366\n",
      "3 0.007756079238468699 0.9972739407536457\n",
      "4 0.054908620361319854 0.9767689617787694\n",
      "5 0.018521888054325884 0.9932631869199291\n",
      "final 0.03672656107307943 0.9850442749622426\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
      "0  00001cee341fdb12  0.998661  3.786862e-01  0.957138  1.786500e-01  0.912598   \n",
      "1  0000247867823ef7  0.000191  1.732031e-08  0.000011  1.745301e-06  0.000009   \n",
      "2  00013b17ad220c46  0.000019  3.861570e-10  0.000008  1.205197e-07  0.000001   \n",
      "3  00017563c3f7919a  0.000094  1.363162e-08  0.000008  1.649089e-05  0.000011   \n",
      "4  00017695ad8997eb  0.000731  3.728374e-08  0.000039  9.583951e-06  0.000029   \n",
      "\n",
      "   identity_hate  \n",
      "0   4.857816e-01  \n",
      "1   2.278547e-06  \n",
      "2   2.891617e-07  \n",
      "3   2.257141e-06  \n",
      "4   6.690435e-06  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=42)\n",
    "print(train_pred.shape,test_pred.shape) \n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_fasttext_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_fasttext_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')\n",
    "\n",
    "# final 0.03672656107307943 0.9850442749622426 PUB 9863"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
