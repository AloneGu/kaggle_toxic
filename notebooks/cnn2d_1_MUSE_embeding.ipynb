{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D,Conv2D,Reshape\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'nonsense kiss off geek what i said is true i will have your account terminated ',\n",
       "       ' please do not vandalize pages as you did with this edit to w s merwin if you continue to do so you will be blocked from editing ',\n",
       "       ' points of interest i removed the points of interest section you added because it seemed kind of spammy i know you probably did not mean to disobey the rules but generally a point of interest tends to be rather touristy and quite irrelevant to an area culture that just my opinion though if you want to reply just put your reply here and add talkback jamiegraham08 on my talkpage ',\n",
       "       'asking some his nationality is a racial offence wow was not aware of it blocking me has shown your support towards your community thanku for that',\n",
       "       'the reader here is not going by my say so for ethereal vocal style and dark lyrical content the cited sources in the external links are saying those things if you feel the sources are unreliable or i did not represent what they said correctly rewrite or delete it '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 226998\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95851, 150) (226998, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ascorbate 87832\n",
      "tonight 3360\n",
      "alben 73157\n",
      "infrastructure 7701\n",
      "wih 75492\n",
      "vivat 53369\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4871\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 150)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_13 (Embedding)        (None, 150, 300)     12000000    input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 150, 300, 1)  0           embedding_13[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 150, 300, 1)  0           reshape_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 148, 1, 128)  115328      dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 146, 1, 128)  192128      dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 144, 1, 128)  268928      dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling2D) (None, 1, 1, 128)    0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_40 (MaxPooling2D) (None, 1, 1, 128)    0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_41 (MaxPooling2D) (None, 1, 1, 128)    0           conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 1, 1, 384)    0           max_pooling2d_39[0][0]           \n",
      "                                                                 max_pooling2d_40[0][0]           \n",
      "                                                                 max_pooling2d_41[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_9 (Flatten)             (None, 384)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 384)          0           flatten_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 256)          98560       dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 256)          0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 6)            1542        dropout_27[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 12,676,486\n",
      "Trainable params: 676,486\n",
      "Non-trainable params: 12,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.layers import MaxPool2D,concatenate,Flatten\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model(comp=True):\n",
    "    # https://github.com/bhaveshoswal/CNN-text-classification-keras/blob/master/model.py\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Reshape((maxlen,EMBEDDING_DIM,1))(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "   \n",
    "    x1 = Conv2D(128,kernel_size=(3,EMBEDDING_DIM),activation='relu')(x)\n",
    "    x1 = MaxPool2D(pool_size=(maxlen - 3 + 1, 1), strides=(1,1), padding='valid')(x1)\n",
    "    \n",
    "    x2 = Conv2D(128,kernel_size=(5,EMBEDDING_DIM),activation='relu')(x)\n",
    "    x2 = MaxPool2D(pool_size=(maxlen - 5 + 1, 1), strides=(1,1), padding='valid')(x2)\n",
    "    \n",
    "    x3 = Conv2D(128,kernel_size=(7,EMBEDDING_DIM),activation='relu')(x)\n",
    "    x3 = MaxPool2D(pool_size=(maxlen - 7 + 1, 1), strides=(1,1), padding='valid')(x3)\n",
    "    \n",
    "    x = concatenate([x1,x2,x3])\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    if comp:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n",
    "\n",
    "tmp_m = get_cnn_model()\n",
    "tmp_m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_gen(x_data,y_data,batch_size=64):\n",
    "    data_cnt = len(y_data)\n",
    "    curr_idx = 0\n",
    "    while True:\n",
    "        if curr_idx+batch_size>=data_cnt:\n",
    "            start_idx,end_idx = data_cnt-batch_size,data_cnt\n",
    "            curr_idx = 0\n",
    "        else:\n",
    "            start_idx,end_idx = curr_idx,curr_idx+batch_size\n",
    "            curr_idx += batch_size\n",
    "            \n",
    "        curr_x = x_data[start_idx:end_idx]\n",
    "        curr_y = y_data[start_idx:end_idx]\n",
    "        yield curr_x,curr_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0999 - acc: 0.9703Epoch 00001: val_loss improved from inf to 0.05716, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0998 - acc: 0.9704 - val_loss: 0.0572 - val_acc: 0.9799\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0626 - acc: 0.9775Epoch 00002: val_loss improved from 0.05716 to 0.05302, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0627 - acc: 0.9775 - val_loss: 0.0530 - val_acc: 0.9808\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0580 - acc: 0.9790Epoch 00003: val_loss improved from 0.05302 to 0.05176, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0580 - acc: 0.9791 - val_loss: 0.0518 - val_acc: 0.9810\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9801Epoch 00004: val_loss improved from 0.05176 to 0.05146, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0556 - acc: 0.9800 - val_loss: 0.0515 - val_acc: 0.9814\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9802Epoch 00005: val_loss improved from 0.05146 to 0.05123, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0541 - acc: 0.9803 - val_loss: 0.0512 - val_acc: 0.9813\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9815Epoch 00006: val_loss improved from 0.05123 to 0.05027, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0513 - acc: 0.9814 - val_loss: 0.0503 - val_acc: 0.9814\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0513 - acc: 0.9811Epoch 00007: val_loss improved from 0.05027 to 0.04877, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0514 - acc: 0.9811 - val_loss: 0.0488 - val_acc: 0.9823\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0497 - acc: 0.9815Epoch 00008: val_loss did not improve\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0497 - acc: 0.9814 - val_loss: 0.0495 - val_acc: 0.9819\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9818Epoch 00009: val_loss improved from 0.04877 to 0.04831, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0481 - acc: 0.9818 - val_loss: 0.0483 - val_acc: 0.9823\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0473 - acc: 0.9822Epoch 00010: val_loss did not improve\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.0473 - acc: 0.9822 - val_loss: 0.0484 - val_acc: 0.9822\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9830Epoch 00011: val_loss improved from 0.04831 to 0.04746, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0447 - acc: 0.9830 - val_loss: 0.0475 - val_acc: 0.9827\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9827Epoch 00012: val_loss did not improve\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0451 - acc: 0.9827 - val_loss: 0.0475 - val_acc: 0.9827\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9836Epoch 00013: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0438 - acc: 0.9836 - val_loss: 0.0519 - val_acc: 0.9825\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9831Epoch 00014: val_loss improved from 0.04746 to 0.04685, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0430 - acc: 0.9832 - val_loss: 0.0469 - val_acc: 0.9827\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9838Epoch 00015: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.0479 - val_acc: 0.9826\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9849Epoch 00016: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0389 - acc: 0.9849 - val_loss: 0.0485 - val_acc: 0.9828\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9844Epoch 00017: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0400 - acc: 0.9843 - val_loss: 0.0475 - val_acc: 0.9827\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9847Epoch 00018: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0504 - val_acc: 0.9826\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9848Epoch 00019: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0378 - acc: 0.9848 - val_loss: 0.0493 - val_acc: 0.9812\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9848Epoch 00020: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0380 - acc: 0.9848 - val_loss: 0.0478 - val_acc: 0.9824\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.1022 - acc: 0.9703Epoch 00001: val_loss improved from inf to 0.05790, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.1021 - acc: 0.9703 - val_loss: 0.0579 - val_acc: 0.9794\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0634 - acc: 0.9773Epoch 00002: val_loss improved from 0.05790 to 0.05407, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0634 - acc: 0.9773 - val_loss: 0.0541 - val_acc: 0.9803\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0572 - acc: 0.9794Epoch 00003: val_loss improved from 0.05407 to 0.05335, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0572 - acc: 0.9794 - val_loss: 0.0534 - val_acc: 0.9807\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0573 - acc: 0.9792Epoch 00004: val_loss improved from 0.05335 to 0.05084, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0574 - acc: 0.9791 - val_loss: 0.0508 - val_acc: 0.9813\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0547 - acc: 0.9798Epoch 00005: val_loss improved from 0.05084 to 0.05041, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0547 - acc: 0.9798 - val_loss: 0.0504 - val_acc: 0.9816\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9815Epoch 00006: val_loss improved from 0.05041 to 0.04881, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0513 - acc: 0.9815 - val_loss: 0.0488 - val_acc: 0.9817\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9810Epoch 00007: val_loss improved from 0.04881 to 0.04870, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0505 - acc: 0.9810 - val_loss: 0.0487 - val_acc: 0.9821\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9820Epoch 00008: val_loss improved from 0.04870 to 0.04746, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0485 - acc: 0.9819 - val_loss: 0.0475 - val_acc: 0.9824\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0485 - acc: 0.9817Epoch 00009: val_loss improved from 0.04746 to 0.04745, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0485 - acc: 0.9817 - val_loss: 0.0475 - val_acc: 0.9822\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9825Epoch 00010: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0463 - acc: 0.9825 - val_loss: 0.0483 - val_acc: 0.9824\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9825Epoch 00011: val_loss improved from 0.04745 to 0.04740, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0457 - acc: 0.9825 - val_loss: 0.0474 - val_acc: 0.9823\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9826Epoch 00012: val_loss improved from 0.04740 to 0.04678, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0456 - acc: 0.9825 - val_loss: 0.0468 - val_acc: 0.9826\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9835Epoch 00013: val_loss improved from 0.04678 to 0.04647, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 32s 80ms/step - loss: 0.0430 - acc: 0.9836 - val_loss: 0.0465 - val_acc: 0.9829\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9832Epoch 00014: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0435 - acc: 0.9832 - val_loss: 0.0479 - val_acc: 0.9819\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00015: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0418 - acc: 0.9837 - val_loss: 0.0490 - val_acc: 0.9823\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9840Epoch 00016: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0415 - acc: 0.9840 - val_loss: 0.0484 - val_acc: 0.9828\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9841Epoch 00017: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0398 - acc: 0.9841 - val_loss: 0.0473 - val_acc: 0.9827\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9848Epoch 00018: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0388 - acc: 0.9848 - val_loss: 0.0468 - val_acc: 0.9827\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9844Epoch 00019: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0392 - acc: 0.9844 - val_loss: 0.0479 - val_acc: 0.9823\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9851Epoch 00020: val_loss did not improve\n",
      "400/400 [==============================] - 32s 79ms/step - loss: 0.0374 - acc: 0.9851 - val_loss: 0.0475 - val_acc: 0.9826\n",
      "Epoch 1/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.1034 - acc: 0.9698Epoch 00001: val_loss improved from inf to 0.06050, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 82ms/step - loss: 0.1033 - acc: 0.9698 - val_loss: 0.0605 - val_acc: 0.9787\n",
      "Epoch 2/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9785Epoch 00002: val_loss improved from 0.06050 to 0.05552, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.0605 - acc: 0.9785 - val_loss: 0.0555 - val_acc: 0.9799\n",
      "Epoch 3/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9796Epoch 00003: val_loss did not improve\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.0575 - acc: 0.9795 - val_loss: 0.0576 - val_acc: 0.9796\n",
      "Epoch 4/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0554 - acc: 0.9802Epoch 00004: val_loss improved from 0.05552 to 0.05204, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 84ms/step - loss: 0.0554 - acc: 0.9802 - val_loss: 0.0520 - val_acc: 0.9807\n",
      "Epoch 5/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9807Epoch 00005: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0536 - acc: 0.9807 - val_loss: 0.0540 - val_acc: 0.9807\n",
      "Epoch 6/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0517 - acc: 0.9815Epoch 00006: val_loss improved from 0.05204 to 0.05099, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.0516 - acc: 0.9816 - val_loss: 0.0510 - val_acc: 0.9810\n",
      "Epoch 7/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0488 - acc: 0.9818Epoch 00007: val_loss improved from 0.05099 to 0.05028, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.0488 - acc: 0.9818 - val_loss: 0.0503 - val_acc: 0.9813\n",
      "Epoch 8/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0491 - acc: 0.9818Epoch 00008: val_loss improved from 0.05028 to 0.04975, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.0494 - acc: 0.9818 - val_loss: 0.0497 - val_acc: 0.9816\n",
      "Epoch 9/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0479 - acc: 0.9822Epoch 00009: val_loss improved from 0.04975 to 0.04893, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 34s 85ms/step - loss: 0.0479 - acc: 0.9822 - val_loss: 0.0489 - val_acc: 0.9817\n",
      "Epoch 10/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9825Epoch 00010: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0467 - acc: 0.9825 - val_loss: 0.0528 - val_acc: 0.9811\n",
      "Epoch 11/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9826Epoch 00011: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0505 - val_acc: 0.9811\n",
      "Epoch 12/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9834Epoch 00012: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0436 - acc: 0.9834 - val_loss: 0.0491 - val_acc: 0.9818\n",
      "Epoch 13/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9836Epoch 00013: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0439 - acc: 0.9836 - val_loss: 0.0497 - val_acc: 0.9816\n",
      "Epoch 14/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9837Epoch 00014: val_loss improved from 0.04893 to 0.04865, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 34s 84ms/step - loss: 0.0431 - acc: 0.9837 - val_loss: 0.0487 - val_acc: 0.9820\n",
      "Epoch 15/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9839Epoch 00015: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0420 - acc: 0.9840 - val_loss: 0.0523 - val_acc: 0.9815\n",
      "Epoch 16/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9844Epoch 00016: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0420 - acc: 0.9843 - val_loss: 0.0500 - val_acc: 0.9814\n",
      "Epoch 17/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9850Epoch 00017: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0399 - acc: 0.9850 - val_loss: 0.0501 - val_acc: 0.9819\n",
      "Epoch 18/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9847Epoch 00018: val_loss improved from 0.04865 to 0.04865, saving model to weights_base.best.h5\n",
      "400/400 [==============================] - 33s 84ms/step - loss: 0.0398 - acc: 0.9847 - val_loss: 0.0486 - val_acc: 0.9819\n",
      "Epoch 19/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9850Epoch 00019: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0395 - acc: 0.9850 - val_loss: 0.0515 - val_acc: 0.9818\n",
      "Epoch 20/20\n",
      "399/400 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9852Epoch 00020: val_loss did not improve\n",
      "400/400 [==============================] - 33s 83ms/step - loss: 0.0378 - acc: 0.9852 - val_loss: 0.0548 - val_acc: 0.9812\n",
      "-------------------------------\n",
      "0 0.103229998807 0.961909630572\n",
      "1 0.0235950099821 0.990401769413\n",
      "2 0.0538722775577 0.980125402969\n",
      "3 0.0121061027925 0.996901440778\n",
      "4 0.0679522076853 0.972811968576\n",
      "5 0.0231916955782 0.992884789934\n",
      "final 0.0473245487338 0.982505833707\n",
      "all eval None\n",
      "(95851, 6) (226998, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((95851,6)),np.zeros((226998,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        train_gen = data_gen(curr_x,curr_y)\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 20\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit_generator(train_gen, \n",
    "                            steps_per_epoch=400, \n",
    "                            epochs=epochs, \n",
    "                            validation_data=(hold_out_x,hold_out_y), \n",
    "                            callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn2d_muse_1_csv.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn2d_muse_1_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')\n",
    "\n",
    "# pre cnn 4551, 4565, 4717\n",
    "# cnn2d, with data gen\n",
    "# 4685, 4647,4865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.506,  0.   ,  0.026,  0.   ,  0.029,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.012,  0.   ,  0.002,  0.   ,  0.001,  0.001],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.26 ,  0.003,  0.05 ,  0.005,  0.057,  0.009],\n",
       "       [ 0.006,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.026,  0.   ,  0.002,  0.   ,  0.002,  0.   ],\n",
       "       [ 0.004,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.002,  0.   ,  0.001,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
