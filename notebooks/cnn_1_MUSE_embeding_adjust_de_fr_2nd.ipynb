{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    #https://www.kaggle.com/sreeram004/test-lr-with-convai-dataset\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "racistswho 115413\n",
      "eids 61159\n",
      "colonized 16431\n",
      "rivialry 79470\n",
      "maccer 147316\n",
      "vorox 33070\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n",
      "de\n",
      "435713\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# https://github.com/facebookresearch/MUSE\n",
    "word_vec_dict = {}\n",
    "\n",
    "with open('../wiki.multi.fr.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print('fr')\n",
    "with open('../wiki.multi.de.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print('de')\n",
    "with open('../wiki.multi.en.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "        \n",
    "print(len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 4146\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict\n",
    "# import gc\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_cnn_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(256,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 106380 samples, validate on 53191 samples\n",
      "Epoch 1/10\n",
      "106368/106380 [============================>.] - ETA: 0s - loss: 0.0706 - acc: 0.9768Epoch 00001: val_loss improved from inf to 0.05206, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 32s 303us/step - loss: 0.0706 - acc: 0.9768 - val_loss: 0.0521 - val_acc: 0.9807\n",
      "Epoch 2/10\n",
      "106240/106380 [============================>.] - ETA: 0s - loss: 0.0531 - acc: 0.9805Epoch 00002: val_loss improved from 0.05206 to 0.04825, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 30s 281us/step - loss: 0.0531 - acc: 0.9805 - val_loss: 0.0483 - val_acc: 0.9817\n",
      "Epoch 3/10\n",
      "106368/106380 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9816Epoch 00003: val_loss improved from 0.04825 to 0.04680, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 30s 279us/step - loss: 0.0498 - acc: 0.9816 - val_loss: 0.0468 - val_acc: 0.9823\n",
      "Epoch 4/10\n",
      "106240/106380 [============================>.] - ETA: 0s - loss: 0.0472 - acc: 0.9823Epoch 00004: val_loss improved from 0.04680 to 0.04601, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 29s 277us/step - loss: 0.0472 - acc: 0.9823 - val_loss: 0.0460 - val_acc: 0.9825\n",
      "Epoch 5/10\n",
      "106368/106380 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9827Epoch 00005: val_loss improved from 0.04601 to 0.04552, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 30s 280us/step - loss: 0.0453 - acc: 0.9827 - val_loss: 0.0455 - val_acc: 0.9827\n",
      "Epoch 6/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9832Epoch 00006: val_loss did not improve\n",
      "106380/106380 [==============================] - 29s 275us/step - loss: 0.0438 - acc: 0.9832 - val_loss: 0.0455 - val_acc: 0.9827\n",
      "Epoch 7/10\n",
      "106304/106380 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9836Epoch 00007: val_loss did not improve\n",
      "106380/106380 [==============================] - 29s 273us/step - loss: 0.0424 - acc: 0.9836 - val_loss: 0.0459 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "106176/106380 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9840Epoch 00008: val_loss improved from 0.04552 to 0.04455, saving model to weights_base.best.h5\n",
      "106380/106380 [==============================] - 29s 276us/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.0445 - val_acc: 0.9831\n",
      "Epoch 9/10\n",
      "106176/106380 [============================>.] - ETA: 0s - loss: 0.0402 - acc: 0.9841Epoch 00009: val_loss did not improve\n",
      "106380/106380 [==============================] - 29s 272us/step - loss: 0.0402 - acc: 0.9841 - val_loss: 0.0450 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "106240/106380 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845Epoch 00010: val_loss did not improve\n",
      "106380/106380 [==============================] - 29s 269us/step - loss: 0.0393 - acc: 0.9845 - val_loss: 0.0456 - val_acc: 0.9828\n",
      "Train on 106381 samples, validate on 53190 samples\n",
      "Epoch 1/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0720 - acc: 0.9761Epoch 00001: val_loss improved from inf to 0.05132, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 272us/step - loss: 0.0720 - acc: 0.9761 - val_loss: 0.0513 - val_acc: 0.9816\n",
      "Epoch 2/10\n",
      "106240/106381 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9802Epoch 00002: val_loss improved from 0.05132 to 0.04963, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 275us/step - loss: 0.0545 - acc: 0.9802 - val_loss: 0.0496 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9811Epoch 00003: val_loss improved from 0.04963 to 0.04629, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 272us/step - loss: 0.0510 - acc: 0.9811 - val_loss: 0.0463 - val_acc: 0.9825\n",
      "Epoch 4/10\n",
      "106368/106381 [============================>.] - ETA: 0s - loss: 0.0478 - acc: 0.9819Epoch 00004: val_loss improved from 0.04629 to 0.04563, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 276us/step - loss: 0.0478 - acc: 0.9819 - val_loss: 0.0456 - val_acc: 0.9830\n",
      "Epoch 5/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9824Epoch 00005: val_loss improved from 0.04563 to 0.04505, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 272us/step - loss: 0.0461 - acc: 0.9825 - val_loss: 0.0451 - val_acc: 0.9827\n",
      "Epoch 6/10\n",
      "106240/106381 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9828Epoch 00006: val_loss improved from 0.04505 to 0.04448, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 273us/step - loss: 0.0449 - acc: 0.9828 - val_loss: 0.0445 - val_acc: 0.9832\n",
      "Epoch 7/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9832Epoch 00007: val_loss did not improve\n",
      "106381/106381 [==============================] - 29s 275us/step - loss: 0.0432 - acc: 0.9832 - val_loss: 0.0447 - val_acc: 0.9831\n",
      "Epoch 8/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00008: val_loss did not improve\n",
      "106381/106381 [==============================] - 29s 272us/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0447 - val_acc: 0.9829\n",
      "Epoch 9/10\n",
      "106240/106381 [============================>.] - ETA: 0s - loss: 0.0411 - acc: 0.9839Epoch 00009: val_loss improved from 0.04448 to 0.04400, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 273us/step - loss: 0.0411 - acc: 0.9839 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "106240/106381 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9842Epoch 00010: val_loss did not improve\n",
      "106381/106381 [==============================] - 29s 275us/step - loss: 0.0401 - acc: 0.9842 - val_loss: 0.0444 - val_acc: 0.9833\n",
      "Train on 106381 samples, validate on 53190 samples\n",
      "Epoch 1/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9764Epoch 00001: val_loss improved from inf to 0.05128, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 274us/step - loss: 0.0707 - acc: 0.9764 - val_loss: 0.0513 - val_acc: 0.9811\n",
      "Epoch 2/10\n",
      "106368/106381 [============================>.] - ETA: 0s - loss: 0.0529 - acc: 0.9806Epoch 00002: val_loss improved from 0.05128 to 0.04759, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 272us/step - loss: 0.0529 - acc: 0.9806 - val_loss: 0.0476 - val_acc: 0.9822\n",
      "Epoch 3/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0486 - acc: 0.9817Epoch 00003: val_loss did not improve\n",
      "106381/106381 [==============================] - 29s 273us/step - loss: 0.0486 - acc: 0.9817 - val_loss: 0.0484 - val_acc: 0.9818\n",
      "Epoch 4/10\n",
      "106368/106381 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9824Epoch 00004: val_loss improved from 0.04759 to 0.04593, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 272us/step - loss: 0.0467 - acc: 0.9824 - val_loss: 0.0459 - val_acc: 0.9823\n",
      "Epoch 5/10\n",
      "106368/106381 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9829Epoch 00005: val_loss improved from 0.04593 to 0.04495, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 270us/step - loss: 0.0444 - acc: 0.9829 - val_loss: 0.0449 - val_acc: 0.9831\n",
      "Epoch 6/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9834Epoch 00006: val_loss improved from 0.04495 to 0.04469, saving model to weights_base.best.h5\n",
      "106381/106381 [==============================] - 29s 275us/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0447 - val_acc: 0.9831\n",
      "Epoch 7/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9836Epoch 00007: val_loss did not improve\n",
      "106381/106381 [==============================] - 29s 273us/step - loss: 0.0421 - acc: 0.9836 - val_loss: 0.0453 - val_acc: 0.9830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9839Epoch 00008: val_loss did not improve\n",
      "106381/106381 [==============================] - 30s 283us/step - loss: 0.0409 - acc: 0.9839 - val_loss: 0.0470 - val_acc: 0.9827\n",
      "Epoch 9/10\n",
      "106176/106381 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9846Epoch 00009: val_loss did not improve\n",
      "106381/106381 [==============================] - 30s 287us/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0447 - val_acc: 0.9830\n",
      "Epoch 10/10\n",
      "106304/106381 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9847Epoch 00010: val_loss did not improve\n",
      "106381/106381 [==============================] - 30s 282us/step - loss: 0.0386 - acc: 0.9847 - val_loss: 0.0464 - val_acc: 0.9828\n",
      "-------------------------------\n",
      "0 0.0990043847733 0.962869193024\n",
      "1 0.0230936548087 0.990631129717\n",
      "2 0.0509429689113 0.981161990587\n",
      "3 0.00851411888576 0.997261407148\n",
      "4 0.0639181872283 0.974362509479\n",
      "5 0.0210751217056 0.992943579974\n",
      "final 0.0444247393855 0.983204968321\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_cnn_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), callbacks=callbacks_list)\n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "train_pred,test_pred = kf_train()\n",
    "print(train_pred.shape,test_pred.shape)\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/cnn_muse_adj_1_csv_de_fr.gz\", index=False, compression='gzip')\n",
    "import pickle\n",
    "with open('../features/cnn_muse_adj_1_feat_de_fr.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.02 ,  0.   ,  0.   ,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.   ,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.042,  0.   ,  0.004,  0.   ,  0.011,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.692,  0.003,  0.239,  0.002,  0.217,  0.004],\n",
       "       [ 0.007,  0.   ,  0.001,  0.   ,  0.001,  0.   ],\n",
       "       [ 0.003,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ],\n",
       "       [ 0.001,  0.   ,  0.   ,  0.   ,  0.   ,  0.   ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[:10].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
