{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "costs 4195\n",
      "figurines 61515\n",
      "outgunned 55052\n",
      "sai 12096\n",
      "umbridge 38108\n",
      "hatkirby 154106\n",
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 76050\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval')\n",
    "    eval_val(y,train_pred)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9783Epoch 00001: val_loss improved from inf to 0.04784, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 77s 601us/step - loss: 0.0635 - acc: 0.9783 - val_loss: 0.0478 - val_acc: 0.9818\n",
      "Epoch 2/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9823Epoch 00002: val_loss improved from 0.04784 to 0.04367, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 76s 599us/step - loss: 0.0469 - acc: 0.9823 - val_loss: 0.0437 - val_acc: 0.9829\n",
      "Epoch 3/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9831Epoch 00003: val_loss improved from 0.04367 to 0.04256, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 77s 600us/step - loss: 0.0442 - acc: 0.9831 - val_loss: 0.0426 - val_acc: 0.9833\n",
      "Epoch 4/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00004: val_loss improved from 0.04256 to 0.04192, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 77s 601us/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0419 - val_acc: 0.9834\n",
      "Epoch 5/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9841Epoch 00005: val_loss improved from 0.04192 to 0.04121, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 77s 602us/step - loss: 0.0405 - acc: 0.9841 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9845Epoch 00006: val_loss improved from 0.04121 to 0.04100, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 77s 603us/step - loss: 0.0393 - acc: 0.9845 - val_loss: 0.0410 - val_acc: 0.9840\n",
      "Epoch 7/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0379 - acc: 0.9850Epoch 00007: val_loss did not improve\n",
      "127656/127656 [==============================] - 77s 600us/step - loss: 0.0379 - acc: 0.9850 - val_loss: 0.0420 - val_acc: 0.9833\n",
      "Epoch 8/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9853Epoch 00008: val_loss improved from 0.04100 to 0.04092, saving model to weights_base.best.h5\n",
      "127656/127656 [==============================] - 77s 603us/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 9/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9856Epoch 00009: val_loss did not improve\n",
      "127656/127656 [==============================] - 77s 601us/step - loss: 0.0361 - acc: 0.9856 - val_loss: 0.0418 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "127552/127656 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9860Epoch 00010: val_loss did not improve\n",
      "127656/127656 [==============================] - 79s 619us/step - loss: 0.0349 - acc: 0.9860 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0591 - acc: 0.9794Epoch 00001: val_loss improved from inf to 0.04601, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 81s 632us/step - loss: 0.0590 - acc: 0.9794 - val_loss: 0.0460 - val_acc: 0.9825\n",
      "Epoch 2/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9824Epoch 00002: val_loss improved from 0.04601 to 0.04495, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 79s 623us/step - loss: 0.0461 - acc: 0.9824 - val_loss: 0.0450 - val_acc: 0.9833\n",
      "Epoch 3/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9832Epoch 00003: val_loss improved from 0.04495 to 0.04173, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 624us/step - loss: 0.0432 - acc: 0.9832 - val_loss: 0.0417 - val_acc: 0.9837\n",
      "Epoch 4/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9838Epoch 00004: val_loss improved from 0.04173 to 0.04135, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 624us/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "Epoch 5/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0397 - acc: 0.9845Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 619us/step - loss: 0.0397 - acc: 0.9845 - val_loss: 0.0419 - val_acc: 0.9837\n",
      "Epoch 6/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9849Epoch 00006: val_loss improved from 0.04135 to 0.04119, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 626us/step - loss: 0.0383 - acc: 0.9849 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9853Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 620us/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0419 - val_acc: 0.9838\n",
      "Epoch 8/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9856Epoch 00008: val_loss improved from 0.04119 to 0.03991, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 625us/step - loss: 0.0364 - acc: 0.9856 - val_loss: 0.0399 - val_acc: 0.9847\n",
      "Epoch 9/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9857Epoch 00009: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 619us/step - loss: 0.0355 - acc: 0.9857 - val_loss: 0.0409 - val_acc: 0.9844\n",
      "Epoch 10/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9861Epoch 00010: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 621us/step - loss: 0.0344 - acc: 0.9860 - val_loss: 0.0419 - val_acc: 0.9840\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0607 - acc: 0.9791Epoch 00001: val_loss improved from inf to 0.04459, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 628us/step - loss: 0.0607 - acc: 0.9791 - val_loss: 0.0446 - val_acc: 0.9830\n",
      "Epoch 2/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0462 - acc: 0.9825Epoch 00002: val_loss improved from 0.04459 to 0.04289, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 624us/step - loss: 0.0463 - acc: 0.9825 - val_loss: 0.0429 - val_acc: 0.9838\n",
      "Epoch 3/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0427 - acc: 0.9834Epoch 00003: val_loss improved from 0.04289 to 0.04130, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 625us/step - loss: 0.0427 - acc: 0.9834 - val_loss: 0.0413 - val_acc: 0.9843\n",
      "Epoch 4/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0410 - acc: 0.9839Epoch 00004: val_loss improved from 0.04130 to 0.04091, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 625us/step - loss: 0.0410 - acc: 0.9839 - val_loss: 0.0409 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9843Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 622us/step - loss: 0.0396 - acc: 0.9843 - val_loss: 0.0413 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9847Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 79s 618us/step - loss: 0.0383 - acc: 0.9847 - val_loss: 0.0412 - val_acc: 0.9836\n",
      "Epoch 7/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9851Epoch 00007: val_loss improved from 0.04091 to 0.03908, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 80s 625us/step - loss: 0.0371 - acc: 0.9851 - val_loss: 0.0391 - val_acc: 0.9849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9853Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 594us/step - loss: 0.0363 - acc: 0.9853 - val_loss: 0.0392 - val_acc: 0.9848\n",
      "Epoch 9/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9858Epoch 00009: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 597us/step - loss: 0.0352 - acc: 0.9858 - val_loss: 0.0394 - val_acc: 0.9850\n",
      "Epoch 10/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9860Epoch 00010: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 597us/step - loss: 0.0344 - acc: 0.9860 - val_loss: 0.0404 - val_acc: 0.9846\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0618 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.04513, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 78s 612us/step - loss: 0.0618 - acc: 0.9787 - val_loss: 0.0451 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9825Epoch 00002: val_loss improved from 0.04513 to 0.04370, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 77s 604us/step - loss: 0.0460 - acc: 0.9825 - val_loss: 0.0437 - val_acc: 0.9834\n",
      "Epoch 3/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9833Epoch 00003: val_loss improved from 0.04370 to 0.04194, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 77s 604us/step - loss: 0.0430 - acc: 0.9833 - val_loss: 0.0419 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0409 - acc: 0.9840Epoch 00004: val_loss improved from 0.04194 to 0.04183, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 77s 607us/step - loss: 0.0409 - acc: 0.9840 - val_loss: 0.0418 - val_acc: 0.9836\n",
      "Epoch 5/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9846Epoch 00005: val_loss improved from 0.04183 to 0.04109, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 78s 608us/step - loss: 0.0394 - acc: 0.9846 - val_loss: 0.0411 - val_acc: 0.9839\n",
      "Epoch 6/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9851Epoch 00006: val_loss improved from 0.04109 to 0.04089, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 79s 622us/step - loss: 0.0381 - acc: 0.9851 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 7/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854Epoch 00007: val_loss did not improve\n",
      "127657/127657 [==============================] - 78s 608us/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0434 - val_acc: 0.9827\n",
      "Epoch 8/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9859Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 75s 586us/step - loss: 0.0357 - acc: 0.9859 - val_loss: 0.0417 - val_acc: 0.9836\n",
      "Epoch 9/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9860Epoch 00009: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 598us/step - loss: 0.0348 - acc: 0.9860 - val_loss: 0.0436 - val_acc: 0.9828\n",
      "Epoch 10/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9861Epoch 00010: val_loss did not improve\n",
      "127657/127657 [==============================] - 77s 606us/step - loss: 0.0344 - acc: 0.9861 - val_loss: 0.0424 - val_acc: 0.9836\n",
      "Train on 127657 samples, validate on 31914 samples\n",
      "Epoch 1/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0643 - acc: 0.9782Epoch 00001: val_loss improved from inf to 0.04629, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 78s 614us/step - loss: 0.0643 - acc: 0.9782 - val_loss: 0.0463 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0466 - acc: 0.9823Epoch 00002: val_loss improved from 0.04629 to 0.04339, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 77s 607us/step - loss: 0.0467 - acc: 0.9823 - val_loss: 0.0434 - val_acc: 0.9832\n",
      "Epoch 3/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9830Epoch 00003: val_loss improved from 0.04339 to 0.04198, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 78s 608us/step - loss: 0.0438 - acc: 0.9830 - val_loss: 0.0420 - val_acc: 0.9836\n",
      "Epoch 4/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9837Epoch 00004: val_loss improved from 0.04198 to 0.04081, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 77s 603us/step - loss: 0.0417 - acc: 0.9837 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 5/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0401 - acc: 0.9842Epoch 00005: val_loss did not improve\n",
      "127657/127657 [==============================] - 77s 601us/step - loss: 0.0401 - acc: 0.9842 - val_loss: 0.0422 - val_acc: 0.9840\n",
      "Epoch 6/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9846Epoch 00006: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 599us/step - loss: 0.0389 - acc: 0.9846 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 7/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9849Epoch 00007: val_loss improved from 0.04081 to 0.04068, saving model to weights_base.best.h5\n",
      "127657/127657 [==============================] - 77s 604us/step - loss: 0.0378 - acc: 0.9849 - val_loss: 0.0407 - val_acc: 0.9844\n",
      "Epoch 8/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9853Epoch 00008: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 599us/step - loss: 0.0368 - acc: 0.9853 - val_loss: 0.0409 - val_acc: 0.9842\n",
      "Epoch 9/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9855Epoch 00009: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 598us/step - loss: 0.0358 - acc: 0.9855 - val_loss: 0.0415 - val_acc: 0.9845\n",
      "Epoch 10/10\n",
      "127552/127657 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9859Epoch 00010: val_loss did not improve\n",
      "127657/127657 [==============================] - 76s 598us/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0416 - val_acc: 0.9842\n",
      "-------------------------------\n",
      "all eval\n",
      "0 0.08739607786619161 0.9671431525778493\n",
      "1 0.021755021788991635 0.9906561969280132\n",
      "2 0.04484476564551425 0.9827788257264791\n",
      "3 0.008266313915084493 0.9973742095994886\n",
      "4 0.05917957516465637 0.9756660044744973\n",
      "5 0.020365567670911102 0.9928683783394225\n",
      "final 0.040301220341891576 0.9844144612742918\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=5,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "# 40000,150,lstm + global max_pool\n",
    "# final 0.0407274256871 0.984048897774\n",
    "# 100000,150 lstm + attention\n",
    "# final 0.0404159162853 0.984188856371, pub 9849\n",
    "\n",
    "# 180000,150, spacial dr 0.4, fold 5\n",
    "# final 0.040301220341891576 0.9844144612742918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
      "0  00001cee341fdb12  0.998212  3.448951e-01  0.954187  6.623778e-02  0.899314   \n",
      "1  0000247867823ef7  0.000408  2.949290e-06  0.000066  2.324186e-06  0.000036   \n",
      "2  00013b17ad220c46  0.000704  2.330996e-05  0.000082  7.191057e-06  0.000061   \n",
      "3  00017563c3f7919a  0.000022  1.528271e-07  0.000009  2.078268e-07  0.000003   \n",
      "4  00017695ad8997eb  0.001567  1.808700e-05  0.000120  2.457606e-05  0.000081   \n",
      "\n",
      "   identity_hate  \n",
      "0   6.095416e-01  \n",
      "1   9.335724e-06  \n",
      "2   2.464245e-05  \n",
      "3   3.283043e-07  \n",
      "4   2.472824e-05  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_glove_sample_5.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_glove_5_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
