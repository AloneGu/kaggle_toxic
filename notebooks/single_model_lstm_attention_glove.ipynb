{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "cafeterias 128950\n",
      "helixq 52269\n",
      "wilayah 61454\n",
      "sheild 144354\n",
      "banneduser03290610149fz 65722\n",
      "blanshard 123629\n",
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 27136\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 100000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = Attention(maxlen)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 10\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval')\n",
    "    eval_val(y,train_pred)\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.04696, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 636s 5ms/step - loss: 0.0625 - acc: 0.9787 - val_loss: 0.0470 - val_acc: 0.9819\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9826Epoch 00002: val_loss improved from 0.04696 to 0.04282, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 640s 5ms/step - loss: 0.0461 - acc: 0.9826 - val_loss: 0.0428 - val_acc: 0.9831\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9835Epoch 00003: val_loss did not improve\n",
      "119678/119678 [==============================] - 638s 5ms/step - loss: 0.0433 - acc: 0.9835 - val_loss: 0.0428 - val_acc: 0.9831\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9840Epoch 00004: val_loss improved from 0.04282 to 0.04107, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 626s 5ms/step - loss: 0.0413 - acc: 0.9840 - val_loss: 0.0411 - val_acc: 0.9838\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9846Epoch 00005: val_loss did not improve\n",
      "119678/119678 [==============================] - 614s 5ms/step - loss: 0.0398 - acc: 0.9846 - val_loss: 0.0418 - val_acc: 0.9834\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9848Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0385 - acc: 0.9848 - val_loss: 0.0428 - val_acc: 0.9830\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9852Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0413 - val_acc: 0.9838\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9854Epoch 00008: val_loss improved from 0.04107 to 0.03996, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 610s 5ms/step - loss: 0.0368 - acc: 0.9854 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9857Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 606s 5ms/step - loss: 0.0360 - acc: 0.9857 - val_loss: 0.0408 - val_acc: 0.9835\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 606s 5ms/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0412 - val_acc: 0.9836\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9783Epoch 00001: val_loss improved from inf to 0.04623, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 613s 5ms/step - loss: 0.0638 - acc: 0.9783 - val_loss: 0.0462 - val_acc: 0.9825\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0463 - acc: 0.9822Epoch 00002: val_loss improved from 0.04623 to 0.04371, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0463 - acc: 0.9822 - val_loss: 0.0437 - val_acc: 0.9834\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0431 - acc: 0.9834Epoch 00003: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0432 - acc: 0.9834 - val_loss: 0.0441 - val_acc: 0.9830\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9838Epoch 00004: val_loss improved from 0.04371 to 0.04215, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 613s 5ms/step - loss: 0.0414 - acc: 0.9838 - val_loss: 0.0422 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0399 - acc: 0.9843Epoch 00005: val_loss improved from 0.04215 to 0.04146, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 613s 5ms/step - loss: 0.0398 - acc: 0.9843 - val_loss: 0.0415 - val_acc: 0.9843\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9847Epoch 00006: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0387 - acc: 0.9847 - val_loss: 0.0418 - val_acc: 0.9840\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0377 - acc: 0.9849Epoch 00007: val_loss improved from 0.04146 to 0.04093, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 611s 5ms/step - loss: 0.0377 - acc: 0.9849 - val_loss: 0.0409 - val_acc: 0.9843\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9852Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0368 - acc: 0.9852 - val_loss: 0.0419 - val_acc: 0.9839\n",
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9856Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9859Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 611s 5ms/step - loss: 0.0351 - acc: 0.9859 - val_loss: 0.0428 - val_acc: 0.9838\n",
      "Train on 119678 samples, validate on 39893 samples\n",
      "Epoch 1/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0644 - acc: 0.9779Epoch 00001: val_loss improved from inf to 0.04596, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 611s 5ms/step - loss: 0.0644 - acc: 0.9779 - val_loss: 0.0460 - val_acc: 0.9826\n",
      "Epoch 2/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9822Epoch 00002: val_loss improved from 0.04596 to 0.04328, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0470 - acc: 0.9822 - val_loss: 0.0433 - val_acc: 0.9832\n",
      "Epoch 3/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9831Epoch 00003: val_loss improved from 0.04328 to 0.04149, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 610s 5ms/step - loss: 0.0438 - acc: 0.9831 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "Epoch 4/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0413 - acc: 0.9838Epoch 00004: val_loss improved from 0.04149 to 0.04029, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 611s 5ms/step - loss: 0.0413 - acc: 0.9838 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 5/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0400 - acc: 0.9844Epoch 00005: val_loss improved from 0.04029 to 0.04001, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 611s 5ms/step - loss: 0.0400 - acc: 0.9844 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Epoch 6/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9846Epoch 00006: val_loss improved from 0.04001 to 0.03973, saving model to weights_base.best.h5\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0388 - acc: 0.9846 - val_loss: 0.0397 - val_acc: 0.9841\n",
      "Epoch 7/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0378 - acc: 0.9849Epoch 00007: val_loss did not improve\n",
      "119678/119678 [==============================] - 611s 5ms/step - loss: 0.0378 - acc: 0.9849 - val_loss: 0.0409 - val_acc: 0.9834\n",
      "Epoch 8/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9853Epoch 00008: val_loss did not improve\n",
      "119678/119678 [==============================] - 612s 5ms/step - loss: 0.0369 - acc: 0.9853 - val_loss: 0.0398 - val_acc: 0.9840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.9856Epoch 00009: val_loss did not improve\n",
      "119678/119678 [==============================] - 606s 5ms/step - loss: 0.0360 - acc: 0.9856 - val_loss: 0.0397 - val_acc: 0.9842\n",
      "Epoch 10/10\n",
      "119616/119678 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9859Epoch 00010: val_loss did not improve\n",
      "119678/119678 [==============================] - 610s 5ms/step - loss: 0.0350 - acc: 0.9859 - val_loss: 0.0402 - val_acc: 0.9843\n",
      "Train on 119679 samples, validate on 39892 samples\n",
      "Epoch 1/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0636 - acc: 0.9785Epoch 00001: val_loss improved from inf to 0.04806, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 601s 5ms/step - loss: 0.0636 - acc: 0.9785 - val_loss: 0.0481 - val_acc: 0.9818\n",
      "Epoch 2/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9823Epoch 00002: val_loss improved from 0.04806 to 0.04472, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 599s 5ms/step - loss: 0.0467 - acc: 0.9823 - val_loss: 0.0447 - val_acc: 0.9829\n",
      "Epoch 3/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9831Epoch 00003: val_loss improved from 0.04472 to 0.04353, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 600s 5ms/step - loss: 0.0439 - acc: 0.9831 - val_loss: 0.0435 - val_acc: 0.9835\n",
      "Epoch 4/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9836Epoch 00004: val_loss improved from 0.04353 to 0.04230, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 603s 5ms/step - loss: 0.0418 - acc: 0.9836 - val_loss: 0.0423 - val_acc: 0.9839\n",
      "Epoch 5/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9841Epoch 00005: val_loss did not improve\n",
      "119679/119679 [==============================] - 600s 5ms/step - loss: 0.0405 - acc: 0.9841 - val_loss: 0.0426 - val_acc: 0.9835\n",
      "Epoch 6/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9845Epoch 00006: val_loss improved from 0.04230 to 0.04159, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 600s 5ms/step - loss: 0.0390 - acc: 0.9845 - val_loss: 0.0416 - val_acc: 0.9841\n",
      "Epoch 7/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9850Epoch 00007: val_loss improved from 0.04159 to 0.04104, saving model to weights_base.best.h5\n",
      "119679/119679 [==============================] - 602s 5ms/step - loss: 0.0380 - acc: 0.9850 - val_loss: 0.0410 - val_acc: 0.9842\n",
      "Epoch 8/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9852Epoch 00008: val_loss did not improve\n",
      "119679/119679 [==============================] - 603s 5ms/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0424 - val_acc: 0.9839\n",
      "Epoch 9/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9855Epoch 00009: val_loss did not improve\n",
      "119679/119679 [==============================] - 601s 5ms/step - loss: 0.0362 - acc: 0.9855 - val_loss: 0.0419 - val_acc: 0.9841\n",
      "Epoch 10/10\n",
      "119616/119679 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9858Epoch 00010: val_loss did not improve\n",
      "119679/119679 [==============================] - 601s 5ms/step - loss: 0.0353 - acc: 0.9858 - val_loss: 0.0420 - val_acc: 0.9843\n",
      "-------------------------------\n",
      "all eval\n",
      "0 0.087732634717 0.966717009983\n",
      "1 0.0216965094477 0.990662463731\n",
      "2 0.0451697576112 0.982170945849\n",
      "3 0.00850924410112 0.997317808374\n",
      "4 0.0592911478825 0.975271195894\n",
      "5 0.0200962039524 0.992993714397\n",
      "final 0.0404159162853 0.984188856371\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=4,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "# 40000,150,lstm + global max_pool\n",
    "# final 0.0407274256871 0.984048897774\n",
    "# 100000,150 lstm + attention\n",
    "# final 0.0404159162853 0.984188856371, pub 9849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.998945      0.366503  0.966096  0.143002  0.934261   \n",
      "1  0000247867823ef7  0.000949      0.000004  0.000131  0.000001  0.000138   \n",
      "2  00013b17ad220c46  0.001320      0.000009  0.000166  0.000001  0.000168   \n",
      "3  00017563c3f7919a  0.000124      0.000002  0.000026  0.000001  0.000021   \n",
      "4  00017695ad8997eb  0.003229      0.000013  0.000279  0.000009  0.000254   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.541310  \n",
      "1       0.000016  \n",
      "2       0.000029  \n",
      "3       0.000003  \n",
      "4       0.000040  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/lstm_attention_glove_sample_4.gz\", index=False, compression='gzip')\n",
    "with open('../features/lstm_attention_glove_4_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
