{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harrassed 11999\n",
      "murdera 121236\n",
      "deterred 44516\n",
      "eukesh 113417\n",
      "fixed 1203\n",
      "marsupial 48061\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "# print(word_vec_dict['is'])\n",
    "# print(word_vec_dict['are'])\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def del_data_ratio(x,y,ratio=0.8):\n",
    "    pos_index = np.where(y[:,0]==1)[0]\n",
    "    neg_index = np.where(y[:,0]==0)[0]\n",
    "    #print(pos_index)\n",
    "    data_cnt = len(pos_index)\n",
    "    add_cnt = int(data_cnt*ratio)\n",
    "    add_index = pos_index[:add_cnt]\n",
    "    add_x = np.concatenate([x[add_index],x[neg_index]])\n",
    "    add_y = np.concatenate([y[add_index],y[neg_index]])\n",
    "    print(x.shape,add_x.shape,data_cnt)\n",
    "    add_x,add_y = shuffle(add_x,add_y,random_state=6)\n",
    "    return add_x,add_y\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # change pos ratio\n",
    "        new_curr_x,new_curr_y = del_data_ratio(curr_x,curr_y)\n",
    "        new_hold_x,new_hold_y = del_data_ratio(hold_out_x,hold_out_y)\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 256\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(new_curr_x, new_curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(new_hold_x,new_hold_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106380, 150) (104358, 150) 10110\n",
      "(53191, 150) (52154, 150) 5184\n",
      "Train on 104358 samples, validate on 52154 samples\n",
      "Epoch 1/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0724 - acc: 0.9777Epoch 00001: val_loss improved from inf to 0.04586, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 169s 2ms/step - loss: 0.0723 - acc: 0.9777 - val_loss: 0.0459 - val_acc: 0.9828\n",
      "Epoch 2/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0440 - acc: 0.9842Epoch 00002: val_loss improved from 0.04586 to 0.03998, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 169s 2ms/step - loss: 0.0440 - acc: 0.9842 - val_loss: 0.0400 - val_acc: 0.9850\n",
      "Epoch 3/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9850Epoch 00003: val_loss improved from 0.03998 to 0.03844, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 169s 2ms/step - loss: 0.0407 - acc: 0.9850 - val_loss: 0.0384 - val_acc: 0.9852\n",
      "Epoch 4/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9854Epoch 00004: val_loss did not improve\n",
      "104358/104358 [==============================] - 170s 2ms/step - loss: 0.0391 - acc: 0.9854 - val_loss: 0.0397 - val_acc: 0.9848\n",
      "Epoch 5/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0376 - acc: 0.9858Epoch 00005: val_loss improved from 0.03844 to 0.03789, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 173s 2ms/step - loss: 0.0376 - acc: 0.9858 - val_loss: 0.0379 - val_acc: 0.9855\n",
      "Epoch 6/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.9861Epoch 00006: val_loss improved from 0.03789 to 0.03700, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 173s 2ms/step - loss: 0.0367 - acc: 0.9862 - val_loss: 0.0370 - val_acc: 0.9858\n",
      "Epoch 7/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9864Epoch 00007: val_loss improved from 0.03700 to 0.03699, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 173s 2ms/step - loss: 0.0357 - acc: 0.9864 - val_loss: 0.0370 - val_acc: 0.9856\n",
      "Epoch 8/8\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0346 - acc: 0.9867Epoch 00008: val_loss improved from 0.03699 to 0.03577, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 171s 2ms/step - loss: 0.0346 - acc: 0.9867 - val_loss: 0.0358 - val_acc: 0.9861\n",
      "(106381, 150) (104330, 150) 10252\n",
      "(53190, 150) (52181, 150) 5042\n",
      "Train on 104330 samples, validate on 52181 samples\n",
      "Epoch 1/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0737 - acc: 0.9773Epoch 00001: val_loss improved from inf to 0.04380, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 171s 2ms/step - loss: 0.0737 - acc: 0.9773 - val_loss: 0.0438 - val_acc: 0.9838\n",
      "Epoch 2/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9842Epoch 00002: val_loss improved from 0.04380 to 0.03995, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 170s 2ms/step - loss: 0.0432 - acc: 0.9842 - val_loss: 0.0400 - val_acc: 0.9855\n",
      "Epoch 3/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.9848Epoch 00003: val_loss improved from 0.03995 to 0.03754, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 171s 2ms/step - loss: 0.0408 - acc: 0.9848 - val_loss: 0.0375 - val_acc: 0.9858\n",
      "Epoch 4/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9853Epoch 00004: val_loss improved from 0.03754 to 0.03692, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 171s 2ms/step - loss: 0.0389 - acc: 0.9853 - val_loss: 0.0369 - val_acc: 0.9861\n",
      "Epoch 5/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9856Epoch 00005: val_loss improved from 0.03692 to 0.03628, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 170s 2ms/step - loss: 0.0372 - acc: 0.9856 - val_loss: 0.0363 - val_acc: 0.9861\n",
      "Epoch 6/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "104330/104330 [==============================] - 170s 2ms/step - loss: 0.0362 - acc: 0.9860 - val_loss: 0.0364 - val_acc: 0.9860\n",
      "Epoch 7/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9864Epoch 00007: val_loss improved from 0.03628 to 0.03580, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 170s 2ms/step - loss: 0.0353 - acc: 0.9864 - val_loss: 0.0358 - val_acc: 0.9860\n",
      "Epoch 8/8\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9866Epoch 00008: val_loss did not improve\n",
      "104330/104330 [==============================] - 171s 2ms/step - loss: 0.0344 - acc: 0.9866 - val_loss: 0.0360 - val_acc: 0.9860\n",
      "(106381, 150) (104335, 150) 10226\n",
      "(53190, 150) (52176, 150) 5068\n",
      "Train on 104335 samples, validate on 52176 samples\n",
      "Epoch 1/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0712 - acc: 0.9788Epoch 00001: val_loss improved from inf to 0.04245, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 172s 2ms/step - loss: 0.0711 - acc: 0.9788 - val_loss: 0.0425 - val_acc: 0.9844\n",
      "Epoch 2/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0430 - acc: 0.9843Epoch 00002: val_loss improved from 0.04245 to 0.03904, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 170s 2ms/step - loss: 0.0430 - acc: 0.9843 - val_loss: 0.0390 - val_acc: 0.9856\n",
      "Epoch 3/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0403 - acc: 0.9850Epoch 00003: val_loss improved from 0.03904 to 0.03757, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 170s 2ms/step - loss: 0.0403 - acc: 0.9850 - val_loss: 0.0376 - val_acc: 0.9858\n",
      "Epoch 4/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9854Epoch 00004: val_loss improved from 0.03757 to 0.03702, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 171s 2ms/step - loss: 0.0383 - acc: 0.9854 - val_loss: 0.0370 - val_acc: 0.9858\n",
      "Epoch 5/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9858Epoch 00005: val_loss improved from 0.03702 to 0.03629, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 172s 2ms/step - loss: 0.0369 - acc: 0.9858 - val_loss: 0.0363 - val_acc: 0.9862\n",
      "Epoch 6/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9862Epoch 00006: val_loss did not improve\n",
      "104335/104335 [==============================] - 171s 2ms/step - loss: 0.0357 - acc: 0.9862 - val_loss: 0.0363 - val_acc: 0.9859\n",
      "Epoch 7/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.9865Epoch 00007: val_loss improved from 0.03629 to 0.03589, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 171s 2ms/step - loss: 0.0349 - acc: 0.9865 - val_loss: 0.0359 - val_acc: 0.9862\n",
      "Epoch 8/8\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9868Epoch 00008: val_loss did not improve\n",
      "104335/104335 [==============================] - 174s 2ms/step - loss: 0.0339 - acc: 0.9868 - val_loss: 0.0360 - val_acc: 0.9861\n",
      "-------------------------------\n",
      "0 0.091411956156 0.965469916213\n",
      "1 0.0222525665333 0.990198720319\n",
      "2 0.0460770775553 0.982177212651\n",
      "3 0.00871690534485 0.997236339936\n",
      "4 0.0605489057318 0.974613181593\n",
      "5 0.0200660179552 0.992962380382\n",
      "final 0.0415122382127 0.983776291849\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "===================================\n",
      "(119678, 150) (117406, 150) 11359\n",
      "(39893, 150) (39106, 150) 3935\n",
      "Train on 117406 samples, validate on 39106 samples\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0698 - acc: 0.9787Epoch 00001: val_loss improved from inf to 0.04375, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 176s 1ms/step - loss: 0.0697 - acc: 0.9787 - val_loss: 0.0437 - val_acc: 0.9838\n",
      "Epoch 2/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0433 - acc: 0.9843Epoch 00002: val_loss improved from 0.04375 to 0.04017, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 174s 1ms/step - loss: 0.0433 - acc: 0.9843 - val_loss: 0.0402 - val_acc: 0.9848\n",
      "Epoch 3/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9850Epoch 00003: val_loss improved from 0.04017 to 0.03788, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 175s 1ms/step - loss: 0.0407 - acc: 0.9850 - val_loss: 0.0379 - val_acc: 0.9855\n",
      "Epoch 4/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9856Epoch 00004: val_loss did not improve\n",
      "117406/117406 [==============================] - 175s 1ms/step - loss: 0.0386 - acc: 0.9856 - val_loss: 0.0379 - val_acc: 0.9852\n",
      "Epoch 5/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9857Epoch 00005: val_loss improved from 0.03788 to 0.03754, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 175s 1ms/step - loss: 0.0375 - acc: 0.9857 - val_loss: 0.0375 - val_acc: 0.9855\n",
      "Epoch 6/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0365 - acc: 0.9862Epoch 00006: val_loss improved from 0.03754 to 0.03604, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 175s 1ms/step - loss: 0.0365 - acc: 0.9862 - val_loss: 0.0360 - val_acc: 0.9859\n",
      "Epoch 7/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "117406/117406 [==============================] - 178s 2ms/step - loss: 0.0353 - acc: 0.9864 - val_loss: 0.0366 - val_acc: 0.9856\n",
      "Epoch 8/8\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9867Epoch 00008: val_loss did not improve\n",
      "117406/117406 [==============================] - 180s 2ms/step - loss: 0.0344 - acc: 0.9867 - val_loss: 0.0364 - val_acc: 0.9858\n",
      "(119678, 150) (117381, 150) 11482\n",
      "(39893, 150) (39130, 150) 3812\n",
      "Train on 117381 samples, validate on 39130 samples\n",
      "Epoch 1/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9785Epoch 00001: val_loss improved from inf to 0.04300, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 182s 2ms/step - loss: 0.0709 - acc: 0.9786 - val_loss: 0.0430 - val_acc: 0.9841\n",
      "Epoch 2/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9844Epoch 00002: val_loss improved from 0.04300 to 0.03954, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 181s 2ms/step - loss: 0.0426 - acc: 0.9844 - val_loss: 0.0395 - val_acc: 0.9849\n",
      "Epoch 3/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0398 - acc: 0.9852Epoch 00003: val_loss improved from 0.03954 to 0.03782, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 180s 2ms/step - loss: 0.0397 - acc: 0.9852 - val_loss: 0.0378 - val_acc: 0.9858\n",
      "Epoch 4/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0381 - acc: 0.9855Epoch 00004: val_loss improved from 0.03782 to 0.03698, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 181s 2ms/step - loss: 0.0381 - acc: 0.9855 - val_loss: 0.0370 - val_acc: 0.9861\n",
      "Epoch 5/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9860Epoch 00005: val_loss improved from 0.03698 to 0.03646, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 182s 2ms/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.0365 - val_acc: 0.9861\n",
      "Epoch 6/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9860Epoch 00006: val_loss improved from 0.03646 to 0.03590, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 182s 2ms/step - loss: 0.0362 - acc: 0.9860 - val_loss: 0.0359 - val_acc: 0.9863\n",
      "Epoch 7/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0350 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "117381/117381 [==============================] - 181s 2ms/step - loss: 0.0350 - acc: 0.9864 - val_loss: 0.0360 - val_acc: 0.9862\n",
      "Epoch 8/8\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9868Epoch 00008: val_loss did not improve\n",
      "117381/117381 [==============================] - 181s 2ms/step - loss: 0.0341 - acc: 0.9868 - val_loss: 0.0364 - val_acc: 0.9861\n",
      "(119678, 150) (117363, 150) 11575\n",
      "(39893, 150) (39149, 150) 3719\n",
      "Train on 117363 samples, validate on 39149 samples\n",
      "Epoch 1/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0699 - acc: 0.9788Epoch 00001: val_loss improved from inf to 0.04170, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 183s 2ms/step - loss: 0.0698 - acc: 0.9788 - val_loss: 0.0417 - val_acc: 0.9849\n",
      "Epoch 2/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9840Epoch 00002: val_loss did not improve\n",
      "117363/117363 [==============================] - 182s 2ms/step - loss: 0.0441 - acc: 0.9840 - val_loss: 0.0438 - val_acc: 0.9836\n",
      "Epoch 3/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0407 - acc: 0.9848Epoch 00003: val_loss improved from 0.04170 to 0.03676, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 182s 2ms/step - loss: 0.0407 - acc: 0.9848 - val_loss: 0.0368 - val_acc: 0.9859\n",
      "Epoch 4/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9852Epoch 00004: val_loss improved from 0.03676 to 0.03636, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 180s 2ms/step - loss: 0.0389 - acc: 0.9852 - val_loss: 0.0364 - val_acc: 0.9863\n",
      "Epoch 5/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9856Epoch 00005: val_loss improved from 0.03636 to 0.03511, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 181s 2ms/step - loss: 0.0380 - acc: 0.9856 - val_loss: 0.0351 - val_acc: 0.9866\n",
      "Epoch 6/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9860Epoch 00006: val_loss did not improve\n",
      "117363/117363 [==============================] - 181s 2ms/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.0357 - val_acc: 0.9861\n",
      "Epoch 7/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "117363/117363 [==============================] - 182s 2ms/step - loss: 0.0359 - acc: 0.9864 - val_loss: 0.0354 - val_acc: 0.9864\n",
      "Epoch 8/8\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9865Epoch 00008: val_loss improved from 0.03511 to 0.03451, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 181s 2ms/step - loss: 0.0348 - acc: 0.9865 - val_loss: 0.0345 - val_acc: 0.9866\n",
      "(119679, 150) (117385, 150) 11466\n",
      "(39892, 150) (39126, 150) 3828\n",
      "Train on 117385 samples, validate on 39126 samples\n",
      "Epoch 1/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0732 - acc: 0.9770Epoch 00001: val_loss improved from inf to 0.04280, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 184s 2ms/step - loss: 0.0732 - acc: 0.9771 - val_loss: 0.0428 - val_acc: 0.9844\n",
      "Epoch 2/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9842Epoch 00002: val_loss improved from 0.04280 to 0.03988, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 182s 2ms/step - loss: 0.0436 - acc: 0.9842 - val_loss: 0.0399 - val_acc: 0.9852\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0404 - acc: 0.9850Epoch 00003: val_loss improved from 0.03988 to 0.03907, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 175s 1ms/step - loss: 0.0404 - acc: 0.9850 - val_loss: 0.0391 - val_acc: 0.9853\n",
      "Epoch 4/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0386 - acc: 0.9855Epoch 00004: val_loss improved from 0.03907 to 0.03801, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 174s 1ms/step - loss: 0.0386 - acc: 0.9855 - val_loss: 0.0380 - val_acc: 0.9858\n",
      "Epoch 5/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9859Epoch 00005: val_loss improved from 0.03801 to 0.03712, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 174s 1ms/step - loss: 0.0371 - acc: 0.9859 - val_loss: 0.0371 - val_acc: 0.9863\n",
      "Epoch 6/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9863Epoch 00006: val_loss improved from 0.03712 to 0.03662, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 181s 2ms/step - loss: 0.0359 - acc: 0.9863 - val_loss: 0.0366 - val_acc: 0.9862\n",
      "Epoch 7/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9864Epoch 00007: val_loss did not improve\n",
      "117385/117385 [==============================] - 177s 2ms/step - loss: 0.0352 - acc: 0.9864 - val_loss: 0.0369 - val_acc: 0.9860\n",
      "Epoch 8/8\n",
      "117248/117385 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9867Epoch 00008: val_loss did not improve\n",
      "117385/117385 [==============================] - 179s 2ms/step - loss: 0.0341 - acc: 0.9867 - val_loss: 0.0385 - val_acc: 0.9852\n",
      "-------------------------------\n",
      "0 0.0907050780469 0.965626586284\n",
      "1 0.0217618013115 0.990649930125\n",
      "2 0.0457471416469 0.982139611834\n",
      "3 0.00955393922061 0.997085936668\n",
      "4 0.0599717407068 0.974870120511\n",
      "5 0.0200502960364 0.992993714397\n",
      "final 0.0412983328282 0.983894316636\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "for f in [3,4]:\n",
    "    train_pred,test_pred = kf_train(fold_cnt=f,rnd=f)\n",
    "    print(train_pred.shape,test_pred.shape)    \n",
    "    sample_submission[list_classes] = test_pred\n",
    "    sample_submission.to_csv(\"../results/lstm1_glove_sample_{}.gz\".format(f), index=False, compression='gzip')\n",
    "    with open('../features/lstm1_glove_sample_feat_{}.pkl'.format(f),'wb') as fout:\n",
    "        pickle.dump([train_pred,test_pred],fout)\n",
    "    sample_submission.head()\n",
    "    print('===================================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
