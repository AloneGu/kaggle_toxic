{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n"
     ]
    }
   ],
   "source": [
    "# ref: https://www.kaggle.com/jacklinggu/lstm-with-glove-embedding-public-lb-score-0-049\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 40000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 'explanation why the edits made under my username hardcore metallica fan were reverted they were not vandalisms just closure on some gas after i voted at new york dolls fac and please do not remove the template from the talk page since i am retired now 89 205 38 27',\n",
       "       'd aww ! he matches this background colour i am seemingly stuck with thanks talk 21 51 january 11 2016 utc ',\n",
       "       'hey man i am really not trying to edit war it just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info ',\n",
       "       ' more i cannot make any real suggestions on improvement - i wondered if the section statistics should be later on or a subsection of types of accidents - i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no - one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it listed in the relevant form eg wikipedia good article nominations transport ',\n",
       "       'you sir are my hero any chance you remember what page that on '], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test len 153164\n"
     ]
    }
   ],
   "source": [
    "print('test len',len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 150) (153164, 150)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suberb 132478\n",
      "featural 133420\n",
      "sendic 98228\n",
      "rdh 66152\n",
      "semperfly 159653\n",
      "optimizer 38896\n"
     ]
    }
   ],
   "source": [
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2196007\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 3388\n"
     ]
    }
   ],
   "source": [
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../glove.840B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        v_list = line.strip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Conv1D(256,\n",
    "             3,\n",
    "             padding='valid',\n",
    "             activation='relu',\n",
    "             strides=1)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def del_data_ratio(x,y,ratio=0.8):\n",
    "    pos_index = np.where(y[:,0]==1)[0]\n",
    "    neg_index = np.where(y[:,0]==0)[0]\n",
    "    #print(pos_index)\n",
    "    data_cnt = len(pos_index)\n",
    "    add_cnt = int(data_cnt*ratio)\n",
    "    add_index = pos_index[:add_cnt]\n",
    "    add_x = np.concatenate([x[add_index],x[neg_index]])\n",
    "    add_y = np.concatenate([y[add_index],y[neg_index]])\n",
    "    print(x.shape,add_x.shape,data_cnt)\n",
    "    add_x,add_y = shuffle(add_x,add_y,random_state=666)\n",
    "    return add_x,add_y\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # change pos ratio\n",
    "        new_curr_x,new_curr_y = del_data_ratio(curr_x,curr_y)\n",
    "        new_hold_x,new_hold_y = del_data_ratio(hold_out_x,hold_out_y)\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 6\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(new_curr_x, new_curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(new_hold_x,new_hold_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model = load_model(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106380, 150) (104358, 150) 10110\n",
      "(53191, 150) (52154, 150) 5184\n",
      "Train on 104358 samples, validate on 52154 samples\n",
      "Epoch 1/6\n",
      "104320/104358 [============================>.] - ETA: 0s - loss: 0.0522 - acc: 0.9819Epoch 00001: val_loss improved from inf to 0.04140, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 29s 277us/step - loss: 0.0522 - acc: 0.9819 - val_loss: 0.0414 - val_acc: 0.9843\n",
      "Epoch 2/6\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9845Epoch 00002: val_loss improved from 0.04140 to 0.03934, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 29s 278us/step - loss: 0.0423 - acc: 0.9845 - val_loss: 0.0393 - val_acc: 0.9850\n",
      "Epoch 3/6\n",
      "104192/104358 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9851Epoch 00003: val_loss improved from 0.03934 to 0.03765, saving model to weights_base.best.h5\n",
      "104358/104358 [==============================] - 29s 275us/step - loss: 0.0393 - acc: 0.9852 - val_loss: 0.0377 - val_acc: 0.9853\n",
      "Epoch 4/6\n",
      "104128/104358 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9858Epoch 00004: val_loss did not improve\n",
      "104358/104358 [==============================] - 29s 275us/step - loss: 0.0370 - acc: 0.9857 - val_loss: 0.0387 - val_acc: 0.9848\n",
      "Epoch 5/6\n",
      "104256/104358 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "104358/104358 [==============================] - 28s 273us/step - loss: 0.0354 - acc: 0.9860 - val_loss: 0.0380 - val_acc: 0.9856\n",
      "Epoch 6/6\n",
      "104320/104358 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9867Epoch 00006: val_loss did not improve\n",
      "104358/104358 [==============================] - 29s 278us/step - loss: 0.0338 - acc: 0.9867 - val_loss: 0.0395 - val_acc: 0.9847\n",
      "(106381, 150) (104330, 150) 10252\n",
      "(53190, 150) (52181, 150) 5042\n",
      "Train on 104330 samples, validate on 52181 samples\n",
      "Epoch 1/6\n",
      "104320/104330 [============================>.] - ETA: 0s - loss: 0.0512 - acc: 0.9821Epoch 00001: val_loss improved from inf to 0.04048, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 29s 279us/step - loss: 0.0512 - acc: 0.9821 - val_loss: 0.0405 - val_acc: 0.9847\n",
      "Epoch 2/6\n",
      "104320/104330 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9844Epoch 00002: val_loss did not improve\n",
      "104330/104330 [==============================] - 29s 276us/step - loss: 0.0416 - acc: 0.9844 - val_loss: 0.0411 - val_acc: 0.9849\n",
      "Epoch 3/6\n",
      "104320/104330 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9852Epoch 00003: val_loss improved from 0.04048 to 0.03805, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 28s 272us/step - loss: 0.0390 - acc: 0.9852 - val_loss: 0.0380 - val_acc: 0.9857\n",
      "Epoch 4/6\n",
      "104192/104330 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9858Epoch 00004: val_loss improved from 0.03805 to 0.03792, saving model to weights_base.best.h5\n",
      "104330/104330 [==============================] - 29s 276us/step - loss: 0.0370 - acc: 0.9858 - val_loss: 0.0379 - val_acc: 0.9857\n",
      "Epoch 5/6\n",
      "104320/104330 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00005: val_loss did not improve\n",
      "104330/104330 [==============================] - 28s 272us/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0386 - val_acc: 0.9857\n",
      "Epoch 6/6\n",
      "104128/104330 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "104330/104330 [==============================] - 29s 275us/step - loss: 0.0336 - acc: 0.9866 - val_loss: 0.0397 - val_acc: 0.9858\n",
      "(106381, 150) (104335, 150) 10226\n",
      "(53190, 150) (52176, 150) 5068\n",
      "Train on 104335 samples, validate on 52176 samples\n",
      "Epoch 1/6\n",
      "104128/104335 [============================>.] - ETA: 0s - loss: 0.0528 - acc: 0.9815Epoch 00001: val_loss improved from inf to 0.04092, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 29s 278us/step - loss: 0.0528 - acc: 0.9815 - val_loss: 0.0409 - val_acc: 0.9844\n",
      "Epoch 2/6\n",
      "104256/104335 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9842Epoch 00002: val_loss improved from 0.04092 to 0.03903, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 27s 263us/step - loss: 0.0419 - acc: 0.9842 - val_loss: 0.0390 - val_acc: 0.9850\n",
      "Epoch 3/6\n",
      "104256/104335 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9850Epoch 00003: val_loss improved from 0.03903 to 0.03873, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 31s 293us/step - loss: 0.0391 - acc: 0.9850 - val_loss: 0.0387 - val_acc: 0.9854\n",
      "Epoch 4/6\n",
      "104192/104335 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9856Epoch 00004: val_loss improved from 0.03873 to 0.03860, saving model to weights_base.best.h5\n",
      "104335/104335 [==============================] - 31s 300us/step - loss: 0.0371 - acc: 0.9856 - val_loss: 0.0386 - val_acc: 0.9854\n",
      "Epoch 5/6\n",
      "104320/104335 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9862Epoch 00005: val_loss did not improve\n",
      "104335/104335 [==============================] - 32s 303us/step - loss: 0.0348 - acc: 0.9862 - val_loss: 0.0388 - val_acc: 0.9856\n",
      "Epoch 6/6\n",
      "104320/104335 [============================>.] - ETA: 0s - loss: 0.0333 - acc: 0.9867Epoch 00006: val_loss did not improve\n",
      "104335/104335 [==============================] - 32s 302us/step - loss: 0.0333 - acc: 0.9867 - val_loss: 0.0393 - val_acc: 0.9855\n",
      "-------------------------------\n",
      "0 0.0991174802855 0.962374115597\n",
      "1 0.0232051452439 0.990349123588\n",
      "2 0.0489866649293 0.981174524193\n",
      "3 0.00924286554839 0.997098470273\n",
      "4 0.0634274868055 0.973704495178\n",
      "5 0.0213235108248 0.992761842691\n",
      "final 0.0442171922729 0.982910428587\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "===================================\n",
      "(119678, 150) (117406, 150) 11359\n",
      "(39893, 150) (39106, 150) 3935\n",
      "Train on 117406 samples, validate on 39106 samples\n",
      "Epoch 1/6\n",
      "117376/117406 [============================>.] - ETA: 0s - loss: 0.0520 - acc: 0.9820Epoch 00001: val_loss improved from inf to 0.04312, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 35s 294us/step - loss: 0.0520 - acc: 0.9820 - val_loss: 0.0431 - val_acc: 0.9837\n",
      "Epoch 2/6\n",
      "117184/117406 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9845Epoch 00002: val_loss improved from 0.04312 to 0.03851, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 34s 291us/step - loss: 0.0414 - acc: 0.9845 - val_loss: 0.0385 - val_acc: 0.9851\n",
      "Epoch 3/6\n",
      "117312/117406 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9852Epoch 00003: val_loss improved from 0.03851 to 0.03845, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 34s 289us/step - loss: 0.0390 - acc: 0.9852 - val_loss: 0.0385 - val_acc: 0.9849\n",
      "Epoch 4/6\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9858Epoch 00004: val_loss did not improve\n",
      "117406/117406 [==============================] - 34s 290us/step - loss: 0.0372 - acc: 0.9858 - val_loss: 0.0396 - val_acc: 0.9846\n",
      "Epoch 5/6\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0354 - acc: 0.9861Epoch 00005: val_loss improved from 0.03845 to 0.03808, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 34s 288us/step - loss: 0.0354 - acc: 0.9861 - val_loss: 0.0381 - val_acc: 0.9856\n",
      "Epoch 6/6\n",
      "117248/117406 [============================>.] - ETA: 0s - loss: 0.0341 - acc: 0.9865Epoch 00006: val_loss improved from 0.03808 to 0.03759, saving model to weights_base.best.h5\n",
      "117406/117406 [==============================] - 34s 292us/step - loss: 0.0341 - acc: 0.9865 - val_loss: 0.0376 - val_acc: 0.9856\n",
      "(119678, 150) (117381, 150) 11482\n",
      "(39893, 150) (39130, 150) 3812\n",
      "Train on 117381 samples, validate on 39130 samples\n",
      "Epoch 1/6\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0514 - acc: 0.9820Epoch 00001: val_loss improved from inf to 0.04106, saving model to weights_base.best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117381/117381 [==============================] - 34s 292us/step - loss: 0.0514 - acc: 0.9821 - val_loss: 0.0411 - val_acc: 0.9847\n",
      "Epoch 2/6\n",
      "117376/117381 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9845Epoch 00002: val_loss improved from 0.04106 to 0.03863, saving model to weights_base.best.h5\n",
      "117381/117381 [==============================] - 34s 291us/step - loss: 0.0417 - acc: 0.9845 - val_loss: 0.0386 - val_acc: 0.9851\n",
      "Epoch 3/6\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "117381/117381 [==============================] - 34s 291us/step - loss: 0.0390 - acc: 0.9851 - val_loss: 0.0389 - val_acc: 0.9854\n",
      "Epoch 4/6\n",
      "117376/117381 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9856Epoch 00004: val_loss did not improve\n",
      "117381/117381 [==============================] - 34s 289us/step - loss: 0.0371 - acc: 0.9856 - val_loss: 0.0394 - val_acc: 0.9850\n",
      "Epoch 5/6\n",
      "117312/117381 [============================>.] - ETA: 0s - loss: 0.0353 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "117381/117381 [==============================] - 34s 290us/step - loss: 0.0353 - acc: 0.9860 - val_loss: 0.0390 - val_acc: 0.9849\n",
      "Epoch 6/6\n",
      "117248/117381 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "117381/117381 [==============================] - 34s 291us/step - loss: 0.0339 - acc: 0.9864 - val_loss: 0.0395 - val_acc: 0.9850\n",
      "(119678, 150) (117363, 150) 11575\n",
      "(39893, 150) (39149, 150) 3719\n",
      "Train on 117363 samples, validate on 39149 samples\n",
      "Epoch 1/6\n",
      "117248/117363 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9819Epoch 00001: val_loss improved from inf to 0.03947, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 35s 299us/step - loss: 0.0515 - acc: 0.9819 - val_loss: 0.0395 - val_acc: 0.9853\n",
      "Epoch 2/6\n",
      "117184/117363 [============================>.] - ETA: 0s - loss: 0.0420 - acc: 0.9843Epoch 00002: val_loss improved from 0.03947 to 0.03773, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 31s 266us/step - loss: 0.0421 - acc: 0.9843 - val_loss: 0.0377 - val_acc: 0.9859\n",
      "Epoch 3/6\n",
      "117184/117363 [============================>.] - ETA: 0s - loss: 0.0394 - acc: 0.9849Epoch 00003: val_loss improved from 0.03773 to 0.03770, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 30s 252us/step - loss: 0.0394 - acc: 0.9849 - val_loss: 0.0377 - val_acc: 0.9854\n",
      "Epoch 4/6\n",
      "117184/117363 [============================>.] - ETA: 0s - loss: 0.0374 - acc: 0.9853Epoch 00004: val_loss improved from 0.03770 to 0.03671, saving model to weights_base.best.h5\n",
      "117363/117363 [==============================] - 31s 262us/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0367 - val_acc: 0.9862\n",
      "Epoch 5/6\n",
      "117184/117363 [============================>.] - ETA: 0s - loss: 0.0358 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "117363/117363 [==============================] - 31s 260us/step - loss: 0.0358 - acc: 0.9860 - val_loss: 0.0368 - val_acc: 0.9865\n",
      "Epoch 6/6\n",
      "117312/117363 [============================>.] - ETA: 0s - loss: 0.0342 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "117363/117363 [==============================] - 31s 261us/step - loss: 0.0342 - acc: 0.9864 - val_loss: 0.0384 - val_acc: 0.9862\n",
      "(119679, 150) (117385, 150) 11466\n",
      "(39892, 150) (39126, 150) 3828\n",
      "Train on 117385 samples, validate on 39126 samples\n",
      "Epoch 1/6\n",
      "117312/117385 [============================>.] - ETA: 0s - loss: 0.0515 - acc: 0.9817Epoch 00001: val_loss improved from inf to 0.04051, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 32s 269us/step - loss: 0.0515 - acc: 0.9817 - val_loss: 0.0405 - val_acc: 0.9850\n",
      "Epoch 2/6\n",
      "117184/117385 [============================>.] - ETA: 0s - loss: 0.0414 - acc: 0.9846Epoch 00002: val_loss improved from 0.04051 to 0.03983, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 30s 259us/step - loss: 0.0414 - acc: 0.9846 - val_loss: 0.0398 - val_acc: 0.9851\n",
      "Epoch 3/6\n",
      "117376/117385 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9851Epoch 00003: val_loss did not improve\n",
      "117385/117385 [==============================] - 31s 262us/step - loss: 0.0388 - acc: 0.9851 - val_loss: 0.0407 - val_acc: 0.9847\n",
      "Epoch 4/6\n",
      "117312/117385 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9858Epoch 00004: val_loss improved from 0.03983 to 0.03873, saving model to weights_base.best.h5\n",
      "117385/117385 [==============================] - 31s 261us/step - loss: 0.0370 - acc: 0.9858 - val_loss: 0.0387 - val_acc: 0.9855\n",
      "Epoch 5/6\n",
      "117312/117385 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "117385/117385 [==============================] - 31s 260us/step - loss: 0.0355 - acc: 0.9860 - val_loss: 0.0390 - val_acc: 0.9856\n",
      "Epoch 6/6\n",
      "117376/117385 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "117385/117385 [==============================] - 30s 259us/step - loss: 0.0337 - acc: 0.9865 - val_loss: 0.0394 - val_acc: 0.9854\n",
      "-------------------------------\n",
      "0 0.0975802460034 0.962762657375\n",
      "1 0.0233861828757 0.990449392433\n",
      "2 0.0486953069926 0.981625733999\n",
      "3 0.00875647208743 0.997186205514\n",
      "4 0.0626847284235 0.973867432052\n",
      "5 0.0209781041642 0.992686641056\n",
      "final 0.0436801734245 0.983096343738\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "for f in [3,4]:\n",
    "    train_pred,test_pred = kf_train(fold_cnt=f)\n",
    "    print(train_pred.shape,test_pred.shape)    \n",
    "    sample_submission[list_classes] = test_pred\n",
    "    sample_submission.to_csv(\"../results/cnn1_glove_sample_{}.gz\".format(f), index=False, compression='gzip')\n",
    "    with open('../features/cnn1_glove_sample_feat_{}.pkl'.format(f),'wb') as fout:\n",
    "        pickle.dump([train_pred,test_pred],fout)\n",
    "    sample_submission.head()\n",
    "    print('===================================')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
