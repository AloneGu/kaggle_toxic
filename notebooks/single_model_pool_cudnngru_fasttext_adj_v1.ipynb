{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 6)\n",
      "(159571, 150) (153164, 150)\n",
      "yarne 91537\n",
      "athonite 55455\n",
      "gbagbo 123757\n",
      "calender 27493\n",
      "collaborateur 36307\n",
      "fd8baaaaebaj 143551\n",
      "2000000\n",
      "Preparing embedding matrix\n",
      "Null word embeddings: 79399\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Dense, Embedding, Input, GRU\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout,GlobalAveragePooling1D,Conv1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from models_def import Attention\n",
    "\n",
    "max_features = 180000\n",
    "maxlen = 150\n",
    "\n",
    "def clean_text( text ):\n",
    "    text = text.lower().split()\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+\\-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    #\n",
    "    return text\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").apply(clean_text).values\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "print(X_train.shape,X_test.shape)\n",
    "\n",
    "# check word_index\n",
    "tmp_cnt = 0\n",
    "for k in tokenizer.word_index:\n",
    "    print(k,tokenizer.word_index[k])\n",
    "    tmp_cnt += 1\n",
    "    if tmp_cnt >5:\n",
    "        break\n",
    "word_idx = tokenizer.word_index\n",
    "\n",
    "# read word2vec\n",
    "# \n",
    "word_vec_dict = {}\n",
    "with open('../crawl-300d-2M.vec') as f:\n",
    "    first_line_flag = True\n",
    "    for line in f:\n",
    "        if first_line_flag:\n",
    "            first_line_flag= False\n",
    "            continue\n",
    "        v_list = line.rstrip().split(' ')\n",
    "        k = str(v_list[0])\n",
    "        v = np.array([float(x) for x in v_list[1:]])\n",
    "        word_vec_dict[k] = v\n",
    "print(len(word_vec_dict))\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "nb_words = min(max_features,len(word_idx))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word,i in word_idx.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    else:\n",
    "        if word in word_vec_dict:\n",
    "            embedding_matrix[i] = word_vec_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "del word_vec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def model done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss,accuracy_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU,CuDNNGRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "\n",
    "def eval_val(y,train_x):\n",
    "    res = 0\n",
    "    acc_res = 0\n",
    "    for i in range(6):\n",
    "        curr_loss = log_loss(y[:,i],train_x[:,i])\n",
    "        acc = accuracy_score(y[:,i],train_x[:,i].round())\n",
    "        print(i,curr_loss,acc)\n",
    "        res += curr_loss\n",
    "        acc_res += acc\n",
    "    print('final',res/6, acc_res/6)\n",
    "\n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNGRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    att = Attention(maxlen)(x)\n",
    "    conc = concatenate([att,avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "print('def model done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def kf_train(fold_cnt=3,rnd=1):\n",
    "    kf = KFold(n_splits=fold_cnt, shuffle=False, random_state=233*rnd)\n",
    "    train_pred, test_pred = np.zeros((159571,6)),np.zeros((153164,6))\n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        # x,y\n",
    "        curr_x,curr_y = X_train[train_index],y[train_index]\n",
    "        hold_out_x,hold_out_y = X_train[test_index],y[test_index]\n",
    "        \n",
    "        # model\n",
    "        model = get_model()\n",
    "        batch_size = 64\n",
    "        epochs = 8\n",
    "        file_path=\"weights_base.best.h5\"\n",
    "        checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "        callbacks_list = [checkpoint] \n",
    "        \n",
    "        # train and pred\n",
    "        model.fit(curr_x, curr_y, \n",
    "                  batch_size=batch_size, epochs=epochs, \n",
    "                  validation_data=(hold_out_x,hold_out_y), \n",
    "                  callbacks=callbacks_list)\n",
    "        \n",
    "        model.load_weights(file_path)\n",
    "        y_test = model.predict(X_test)\n",
    "        test_pred += y_test\n",
    "        hold_out_pred = model.predict(hold_out_x)\n",
    "        train_pred[test_index] = hold_out_pred\n",
    "        \n",
    "        # clear\n",
    "        del model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "    test_pred = test_pred / fold_cnt\n",
    "    print('-------------------------------')\n",
    "    print('all eval',eval_val(y,train_pred))\n",
    "    return train_pred, test_pred\n",
    "\n",
    "\n",
    "print('def done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0560 - acc: 0.9801Epoch 00001: val_loss improved from inf to 0.04180, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 120s 836us/step - loss: 0.0560 - acc: 0.9801 - val_loss: 0.0418 - val_acc: 0.9837\n",
      "Epoch 2/8\n",
      "143488/143613 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9837Epoch 00002: val_loss improved from 0.04180 to 0.04049, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 114s 796us/step - loss: 0.0418 - acc: 0.9837 - val_loss: 0.0405 - val_acc: 0.9844\n",
      "Epoch 3/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9845Epoch 00003: val_loss improved from 0.04049 to 0.03901, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 117s 812us/step - loss: 0.0392 - acc: 0.9845 - val_loss: 0.0390 - val_acc: 0.9848\n",
      "Epoch 4/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9855Epoch 00004: val_loss improved from 0.03901 to 0.03865, saving model to weights_base.best.h5\n",
      "143613/143613 [==============================] - 117s 813us/step - loss: 0.0371 - acc: 0.9855 - val_loss: 0.0386 - val_acc: 0.9850\n",
      "Epoch 5/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143613/143613 [==============================] - 120s 837us/step - loss: 0.0355 - acc: 0.9860 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 6/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143613/143613 [==============================] - 118s 818us/step - loss: 0.0337 - acc: 0.9866 - val_loss: 0.0392 - val_acc: 0.9846\n",
      "Epoch 7/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9872Epoch 00007: val_loss did not improve\n",
      "143613/143613 [==============================] - 118s 819us/step - loss: 0.0323 - acc: 0.9872 - val_loss: 0.0398 - val_acc: 0.9846\n",
      "Epoch 8/8\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143613/143613 [==============================] - 118s 820us/step - loss: 0.0309 - acc: 0.9877 - val_loss: 0.0404 - val_acc: 0.9843\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0546 - acc: 0.9804Epoch 00001: val_loss improved from inf to 0.04295, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 115s 800us/step - loss: 0.0546 - acc: 0.9804 - val_loss: 0.0430 - val_acc: 0.9831\n",
      "Epoch 2/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00002: val_loss improved from 0.04295 to 0.04104, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 111s 774us/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0410 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9847Epoch 00003: val_loss improved from 0.04104 to 0.03996, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 113s 787us/step - loss: 0.0391 - acc: 0.9847 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9855Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 833us/step - loss: 0.0372 - acc: 0.9855 - val_loss: 0.0424 - val_acc: 0.9829\n",
      "Epoch 5/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 114s 793us/step - loss: 0.0355 - acc: 0.9859 - val_loss: 0.0401 - val_acc: 0.9840\n",
      "Epoch 6/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9867Epoch 00006: val_loss improved from 0.03996 to 0.03960, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 110s 768us/step - loss: 0.0339 - acc: 0.9867 - val_loss: 0.0396 - val_acc: 0.9842\n",
      "Epoch 7/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9872Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 110s 767us/step - loss: 0.0325 - acc: 0.9872 - val_loss: 0.0400 - val_acc: 0.9840\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 109s 760us/step - loss: 0.0310 - acc: 0.9877 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0565 - acc: 0.9800Epoch 00001: val_loss improved from inf to 0.04337, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 844us/step - loss: 0.0564 - acc: 0.9800 - val_loss: 0.0434 - val_acc: 0.9829\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0421 - acc: 0.9837Epoch 00002: val_loss improved from 0.04337 to 0.03963, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 124s 866us/step - loss: 0.0421 - acc: 0.9837 - val_loss: 0.0396 - val_acc: 0.9842\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.9845Epoch 00003: val_loss improved from 0.03963 to 0.03880, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 125s 870us/step - loss: 0.0396 - acc: 0.9845 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0375 - acc: 0.9852Epoch 00004: val_loss improved from 0.03880 to 0.03861, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 122s 850us/step - loss: 0.0375 - acc: 0.9852 - val_loss: 0.0386 - val_acc: 0.9846\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9857Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 838us/step - loss: 0.0361 - acc: 0.9857 - val_loss: 0.0388 - val_acc: 0.9846\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0344 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 122s 847us/step - loss: 0.0344 - acc: 0.9864 - val_loss: 0.0387 - val_acc: 0.9847\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9869Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 123s 859us/step - loss: 0.0330 - acc: 0.9869 - val_loss: 0.0397 - val_acc: 0.9842\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 126s 877us/step - loss: 0.0314 - acc: 0.9876 - val_loss: 0.0400 - val_acc: 0.9841\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0545 - acc: 0.9807Epoch 00001: val_loss improved from inf to 0.04366, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 846us/step - loss: 0.0546 - acc: 0.9807 - val_loss: 0.0437 - val_acc: 0.9831\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0416 - acc: 0.9839Epoch 00002: val_loss improved from 0.04366 to 0.04093, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 120s 837us/step - loss: 0.0416 - acc: 0.9839 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9847Epoch 00003: val_loss improved from 0.04093 to 0.04075, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 845us/step - loss: 0.0389 - acc: 0.9847 - val_loss: 0.0407 - val_acc: 0.9840\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9854Epoch 00004: val_loss improved from 0.04075 to 0.04045, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 842us/step - loss: 0.0371 - acc: 0.9854 - val_loss: 0.0405 - val_acc: 0.9839\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9861Epoch 00005: val_loss improved from 0.04045 to 0.03986, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 843us/step - loss: 0.0351 - acc: 0.9861 - val_loss: 0.0399 - val_acc: 0.9844\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0337 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 839us/step - loss: 0.0337 - acc: 0.9865 - val_loss: 0.0406 - val_acc: 0.9840\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0320 - acc: 0.9872Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 840us/step - loss: 0.0320 - acc: 0.9872 - val_loss: 0.0407 - val_acc: 0.9843\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9878Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 842us/step - loss: 0.0307 - acc: 0.9878 - val_loss: 0.0414 - val_acc: 0.9837\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0552 - acc: 0.9800Epoch 00001: val_loss improved from inf to 0.04214, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 844us/step - loss: 0.0552 - acc: 0.9801 - val_loss: 0.0421 - val_acc: 0.9841\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00002: val_loss improved from 0.04214 to 0.04062, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 122s 848us/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0406 - val_acc: 0.9842\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9846Epoch 00003: val_loss improved from 0.04062 to 0.03902, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 122s 847us/step - loss: 0.0392 - acc: 0.9846 - val_loss: 0.0390 - val_acc: 0.9849\n",
      "Epoch 4/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9852Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 838us/step - loss: 0.0372 - acc: 0.9852 - val_loss: 0.0394 - val_acc: 0.9849\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9859Epoch 00005: val_loss improved from 0.03902 to 0.03845, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 122s 851us/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0384 - val_acc: 0.9851\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 840us/step - loss: 0.0340 - acc: 0.9864 - val_loss: 0.0391 - val_acc: 0.9846\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0325 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 112s 779us/step - loss: 0.0325 - acc: 0.9871 - val_loss: 0.0397 - val_acc: 0.9846\n",
      "Epoch 8/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0311 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 113s 784us/step - loss: 0.0311 - acc: 0.9875 - val_loss: 0.0397 - val_acc: 0.9851\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0548 - acc: 0.9806Epoch 00001: val_loss improved from inf to 0.04144, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 116s 807us/step - loss: 0.0548 - acc: 0.9806 - val_loss: 0.0414 - val_acc: 0.9838\n",
      "Epoch 2/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00002: val_loss improved from 0.04144 to 0.03966, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 113s 788us/step - loss: 0.0417 - acc: 0.9838 - val_loss: 0.0397 - val_acc: 0.9848\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9846Epoch 00003: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 842us/step - loss: 0.0391 - acc: 0.9846 - val_loss: 0.0399 - val_acc: 0.9842\n",
      "Epoch 4/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9853Epoch 00004: val_loss improved from 0.03966 to 0.03804, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 115s 798us/step - loss: 0.0371 - acc: 0.9853 - val_loss: 0.0380 - val_acc: 0.9851\n",
      "Epoch 5/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 109s 762us/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0381 - val_acc: 0.9849\n",
      "Epoch 6/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 110s 764us/step - loss: 0.0338 - acc: 0.9865 - val_loss: 0.0383 - val_acc: 0.9851\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0323 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 112s 783us/step - loss: 0.0323 - acc: 0.9871 - val_loss: 0.0384 - val_acc: 0.9849\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0310 - acc: 0.9875Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 113s 785us/step - loss: 0.0310 - acc: 0.9875 - val_loss: 0.0390 - val_acc: 0.9846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9801Epoch 00001: val_loss improved from inf to 0.04180, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 118s 824us/step - loss: 0.0553 - acc: 0.9801 - val_loss: 0.0418 - val_acc: 0.9836\n",
      "Epoch 2/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0417 - acc: 0.9838Epoch 00002: val_loss improved from 0.04180 to 0.03960, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 112s 782us/step - loss: 0.0418 - acc: 0.9838 - val_loss: 0.0396 - val_acc: 0.9844\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0393 - acc: 0.9846Epoch 00003: val_loss improved from 0.03960 to 0.03863, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 116s 810us/step - loss: 0.0393 - acc: 0.9846 - val_loss: 0.0386 - val_acc: 0.9847\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9853Epoch 00004: val_loss improved from 0.03863 to 0.03843, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 112s 782us/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0384 - val_acc: 0.9847\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9858Epoch 00005: val_loss improved from 0.03843 to 0.03816, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 111s 772us/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0382 - val_acc: 0.9848\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9864Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 110s 763us/step - loss: 0.0340 - acc: 0.9863 - val_loss: 0.0391 - val_acc: 0.9844\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 114s 793us/step - loss: 0.0324 - acc: 0.9871 - val_loss: 0.0406 - val_acc: 0.9837\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9876Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 111s 774us/step - loss: 0.0309 - acc: 0.9876 - val_loss: 0.0398 - val_acc: 0.9845\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0541 - acc: 0.9807Epoch 00001: val_loss improved from inf to 0.04273, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 111s 771us/step - loss: 0.0541 - acc: 0.9807 - val_loss: 0.0427 - val_acc: 0.9832\n",
      "Epoch 2/8\n",
      "143488/143614 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9839Epoch 00002: val_loss improved from 0.04273 to 0.04075, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 113s 786us/step - loss: 0.0415 - acc: 0.9839 - val_loss: 0.0408 - val_acc: 0.9840\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9848Epoch 00003: val_loss improved from 0.04075 to 0.04041, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 113s 785us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9854Epoch 00004: val_loss improved from 0.04041 to 0.03991, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 841us/step - loss: 0.0369 - acc: 0.9854 - val_loss: 0.0399 - val_acc: 0.9842\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9860Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 839us/step - loss: 0.0355 - acc: 0.9860 - val_loss: 0.0405 - val_acc: 0.9842\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0340 - acc: 0.9866Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 839us/step - loss: 0.0340 - acc: 0.9866 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 843us/step - loss: 0.0324 - acc: 0.9871 - val_loss: 0.0408 - val_acc: 0.9838\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0309 - acc: 0.9877Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 832us/step - loss: 0.0309 - acc: 0.9877 - val_loss: 0.0420 - val_acc: 0.9835\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0555 - acc: 0.9801Epoch 00001: val_loss improved from inf to 0.04167, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 841us/step - loss: 0.0555 - acc: 0.9801 - val_loss: 0.0417 - val_acc: 0.9841\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0419 - acc: 0.9837Epoch 00002: val_loss improved from 0.04167 to 0.03904, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 123s 857us/step - loss: 0.0419 - acc: 0.9837 - val_loss: 0.0390 - val_acc: 0.9852\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0392 - acc: 0.9847Epoch 00003: val_loss improved from 0.03904 to 0.03781, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 112s 777us/step - loss: 0.0392 - acc: 0.9847 - val_loss: 0.0378 - val_acc: 0.9853\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0373 - acc: 0.9853Epoch 00004: val_loss did not improve\n",
      "143614/143614 [==============================] - 126s 879us/step - loss: 0.0373 - acc: 0.9853 - val_loss: 0.0403 - val_acc: 0.9841\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9859Epoch 00005: val_loss did not improve\n",
      "143614/143614 [==============================] - 130s 906us/step - loss: 0.0356 - acc: 0.9859 - val_loss: 0.0386 - val_acc: 0.9852\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9865Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 126s 877us/step - loss: 0.0339 - acc: 0.9865 - val_loss: 0.0380 - val_acc: 0.9850\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0324 - acc: 0.9870Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 122s 852us/step - loss: 0.0324 - acc: 0.9870 - val_loss: 0.0394 - val_acc: 0.9846\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9878Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 835us/step - loss: 0.0308 - acc: 0.9878 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0542 - acc: 0.9807Epoch 00001: val_loss improved from inf to 0.04340, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 124s 865us/step - loss: 0.0542 - acc: 0.9807 - val_loss: 0.0434 - val_acc: 0.9832\n",
      "Epoch 2/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9840Epoch 00002: val_loss improved from 0.04340 to 0.04171, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 122s 848us/step - loss: 0.0415 - acc: 0.9840 - val_loss: 0.0417 - val_acc: 0.9838\n",
      "Epoch 3/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9848Epoch 00003: val_loss improved from 0.04171 to 0.04123, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 841us/step - loss: 0.0389 - acc: 0.9848 - val_loss: 0.0412 - val_acc: 0.9838\n",
      "Epoch 4/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9855Epoch 00004: val_loss improved from 0.04123 to 0.04096, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 121s 845us/step - loss: 0.0369 - acc: 0.9855 - val_loss: 0.0410 - val_acc: 0.9839\n",
      "Epoch 5/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0352 - acc: 0.9860Epoch 00005: val_loss improved from 0.04096 to 0.04025, saving model to weights_base.best.h5\n",
      "143614/143614 [==============================] - 122s 848us/step - loss: 0.0352 - acc: 0.9860 - val_loss: 0.0403 - val_acc: 0.9842\n",
      "Epoch 6/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0336 - acc: 0.9867Epoch 00006: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 842us/step - loss: 0.0336 - acc: 0.9867 - val_loss: 0.0427 - val_acc: 0.9833\n",
      "Epoch 7/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0321 - acc: 0.9871Epoch 00007: val_loss did not improve\n",
      "143614/143614 [==============================] - 121s 840us/step - loss: 0.0321 - acc: 0.9871 - val_loss: 0.0408 - val_acc: 0.9841\n",
      "Epoch 8/8\n",
      "143552/143614 [============================>.] - ETA: 0s - loss: 0.0306 - acc: 0.9879Epoch 00008: val_loss did not improve\n",
      "143614/143614 [==============================] - 120s 838us/step - loss: 0.0306 - acc: 0.9879 - val_loss: 0.0415 - val_acc: 0.9840\n",
      "-------------------------------\n",
      "0 0.0849441778872 0.967631963201\n",
      "1 0.0218750219557 0.990562194885\n",
      "2 0.0426177367264 0.983186167913\n",
      "3 0.00776414598335 0.997286474359\n",
      "4 0.0575360927879 0.976330285578\n",
      "5 0.0188731687683 0.993169184877\n",
      "final 0.0389350573515 0.984694378469\n",
      "all eval None\n",
      "(159571, 6) (153164, 6)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "sample_submission = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "\n",
    "train_pred,test_pred = kf_train(fold_cnt=10,rnd=4)\n",
    "print(train_pred.shape,test_pred.shape)    \n",
    "\n",
    "\n",
    "# 40000, 150\n",
    "# final 0.040348153796 0.984148122153, pub 9849\n",
    "# 180000,150, concat add attention, 10 fold\n",
    "# final 0.0389350573515 0.984694378469ï¼Œ PUB 9861"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
      "0  00001cee341fdb12  0.997501      0.550919  0.979196  0.171820  0.934243   \n",
      "1  0000247867823ef7  0.000349      0.000013  0.000144  0.000014  0.000171   \n",
      "2  00013b17ad220c46  0.000716      0.000049  0.000303  0.000060  0.000304   \n",
      "3  00017563c3f7919a  0.000130      0.000008  0.000089  0.000076  0.000095   \n",
      "4  00017695ad8997eb  0.003955      0.000048  0.000337  0.000099  0.000421   \n",
      "\n",
      "   identity_hate  \n",
      "0       0.575691  \n",
      "1       0.000014  \n",
      "2       0.000120  \n",
      "3       0.000007  \n",
      "4       0.000027  \n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "sample_submission[list_classes] = test_pred\n",
    "sample_submission.to_csv(\"../results/pool_gru1_fasttext_adj1_sample_10.gz\", index=False, compression='gzip')\n",
    "with open('../features/pool_gru_fasttext_adj1_10_feat.pkl','wb') as fout:\n",
    "    pickle.dump([train_pred,test_pred],fout)\n",
    "print(sample_submission.head())\n",
    "print('===================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
